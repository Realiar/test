\long\def\HideThis#1{}
%\def\SEPCLASSLIB{../../../../sepclasslib}
\def\CAKEDIR{.}
\def\RMS{{{\sc rms}}} 
\def\RMS{{\rm RMS}}

\title{Moveout, velocity, and stacking}
\author{Jon Claerbout}
\maketitle
\label{paper:vela}

%{\today. \em  This chapter is owned by JFC.}

In this chapter we handle data as though the earth had no dipping reflectors.
The earth model is one of stratified layers
with velocity a (generally increasing) function of depth.
We consider reflections from layers,
which we process by normal moveout correction (NMO).
The NMO operation is an interesting example
of many general principles of linear operators and numerical analysis.
Finally, using NMO, we estimate the earth's velocity with depth
and we stack some data,
getting a picture of an earth with dipping layers.
This irony, that techniques developed for a stratified earth
can give reasonable images of non-stratified reflectors,
is one of the ``lucky breaks'' of seismic processing.
We will explore the limitations of this phenomenon
in the chapter on dip-moveout.

\par
First, a few words about informal language.
The inverse to velocity arises more frequently
in seismology than the velocity itself.
This inverse is called the ``slowness.''
In common speech, however, the word ``velocity'' is a catch-all,
so what is called a ``velocity analysis''
might actually be a plane of slowness versus time.

\section{INTERPOLATION AS A MATRIX}
Here we see how general principles of linear operators
are exemplified by linear interpolation.
Because the subject matter is so simple and intuitive,
it is ideal to exemplify abstract mathematical concepts
that apply to all linear operators.
\par
Let an integer $k$ range along a survey line,
and let data values $x_k$ be packed into a vector $\bold x$.
(Each data point $x_k$ could also be a seismogram.)
Next we resample the data more densely,
say from 4 to 6 points.
For illustration, I follow a crude
\bx{nearest-neighbor interpolation}
\sx{interpolation, nearest-neighbor}
scheme by sprinkling ones along the diagonal of a rectangular matrix
that is
\begin{equation}
\bold y \eq  \bold B \, \bold x
\label{eqn:interp1}
\end{equation}
where
\begin{equation}
 \left[ 
  \begin{array}{c}
   y_1 \\ 
   y_2 \\
   y_3 \\
   y_4 \\
   y_5 \\
   y_6
  \end{array}
 \right] 
\eq
 \left[ 
  \begin{array}{cccc}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1
  \end{array}
 \right] \;
 \left[ 
  \begin{array}{c}
   x_1 \\ 
   x_2 \\
   x_3 \\
   x_4
  \end{array}
 \right] 
\label{eqn:interp2}
\end{equation}
The interpolated data is simply
$\bold y = (x_1, x_2,x_2,x_3, x_4,x_4)$.
The matrix multiplication~(\ref{eqn:interp2})
would not be done in practice.
Instead there would be a loop running over the space of the
outputs $\bold y$ that picked up values from the input.

\subsection{Looping over input space}
The obvious way to program a deformation
is to take each point from the {\em  input} space
and find where it goes on the output space.
Naturally, many points could land in the same place,
and then only the last would be seen.
Alternately, we could first erase the output space,
then add in points, and finally divide by the number of points
that ended up in each place.
The biggest aggravation is that some places could end up with no points.
This happens where the transformation \bx{stretch}es.
There we need to decide whether to interpolate the missing points,
or simply low-pass filter the output.

\subsection{Looping over output space}
The alternate method that is usually preferable
to looping over input space
is that
our program have a loop over the space of the {\em  outputs,}
and that each output find its input.
The matrix multiply of (\ref{eqn:interp2})
can be interpreted this way.
Where the transformation \bx{shrink}s is a small problem.
In that area
many points in the input space are ignored,
where perhaps they should somehow be averaged with their neighbors.
This is not a serious problem unless we are contemplating
iterative transformations back and forth between the spaces.

\par
We will now address interesting questions
about
the reversibility of these deformation transforms.

\subsection{Formal inversion}
We have thought of equation~(\ref{eqn:interp1})
as a formula for finding $\bold y$ from $\bold x$.
Now consider the opposite problem, finding $\bold x$ from $\bold y$.
Begin by multiplying equation (\ref{eqn:interp2})
by the \bx{transpose matrix}
to define a new quantity $\tilde\bold x$:
\begin{equation}
 \left[ 
  \begin{array}{c}
   \tilde x_1 \\ 
   \tilde x_2 \\
   \tilde x_3 \\
   \tilde x_4
  \end{array}
 \right] 
\eq
 \left[ 
  \begin{array}{ccccccc}
  1 & 0 & 0 & 0 & 0 & 0\\
  0 & 1 & 1 & 0 & 0 & 0\\
  0 & 0 & 0 & 1 & 0 & 0\\
  0 & 0 & 0 & 0 & 1 & 1
  \end{array}
 \right] \;
 \left[ 
  \begin{array}{c}
   y_1 \\ 
   y_2 \\
   y_3 \\
   y_4 \\
   y_5 \\
   y_6
  \end{array}
 \right] 
\end{equation}
$\tilde \bold x$ is not the same as $\bold x$,
but these two vectors have the same dimensionality
and in many applications
it may happen that
$\tilde \bold x$ is a good approximation to $\bold x$.
In general, 
$\tilde \bold x$ may be called an ``image'' of $\bold x$.
Finding the image is the first step of finding $\bold x$ itself.
Formally, the problem is
\begin{equation}
 \bold y  \eq \bold B  \, \bold x
						\label{eqn:formalproblem}
\end{equation}
And the formal solution to the problem is
\begin{equation}
\bold x \eq ({\bf B'\, B})^{-1} \, {\bf B'} \, \bold y
							\label{eqn:interpinv}
\end{equation}
Formally, we verify this solution by substituting
(\ref{eqn:formalproblem}) into
(\ref{eqn:interpinv}).
\begin{equation}
\bold x \eq ( {\bf B' \, B} )^{-1} \, ({\bf B'} \, \bold B) \,\bold x
		   \eq \bold I \, \bold x  \eq \bold x
\end{equation}
In applications,
the possible nonexistence of an inverse for the matrix $( {\bf B' \, B} )$
is always a topic for discussion.
For now we simply examine this matrix for the interpolation problem.
We see that it is diagonal:
\begin{equation}
\bold B' \, \bold B
\eq
 \left[ 
  \begin{array}{ccccccc}
  1 & 0 & 0 & 0 & 0 & 0\\
  0 & 1 & 1 & 0 & 0 & 0\\
  0 & 0 & 0 & 1 & 0 & 0\\
  0 & 0 & 0 & 0 & 1 & 1
  \end{array}
 \right] \;
 \left[ 
  \begin{array}{cccc}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1
  \end{array}
 \right] \;
 \eq
 \left[ 
  \begin{array}{cccc}
  1 & 0 & 0 & 0 \\
  0 & 2 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 2
  \end{array}
 \right] \;
\end{equation}
So, ${\bf \tilde x}_1 =   \bold x_1$; but
    ${\bf \tilde x}_2 = 2 \bold x_2$.
To recover the original data,
we need to divide ${\bf \tilde x}$ by the diagonal matrix $\bold B'\,\bold B$.
Thus, matrix inversion is easy here.

\par
Equation
(\ref{eqn:interpinv})
has an illustrious reputation, which
arises in the context of ``least squares.''
\bxbx{Least squares}{least squares}
is a general method for solving sets of equations
that have more equations than unknowns.

\par
Recovering $\bold x$ from $\bold y$ using equation
(\ref{eqn:interpinv})
presumes the existence of the inverse of
$\bold B'\,\bold B$.
As you might expect, this matrix is nonsingular when ${\bf B}$
{\em  stretches} the data,
because then a few data values are distributed
among a greater number of locations.
Where the transformation {\em  squeezes} the data,
$\bold B'\,\bold B$
must become singular,
since returning
uniquely to the uncompressed condition is impossible.
%chapter \LS\ explains how to handle singular matrices,
%enabling us to decompress losing only the high frequencies.

\par
We can now understand why an adjoint operator is often an
approximate inverse.
This equivalency happens in proportion to the nearness of the matrix
$\bold B'\,\bold B$
to an identity matrix.
The interpolation example we have just examined is one in which
$\bold B'\,\bold B$
differs from an identity matrix merely by a scaling.

\section{THE NORMAL MOVEOUT MAPPING}
\inputdir{stretch}
Recall the traveltime equation~(\ref{eqn:hyper}).
\begin{eqnarray}
v^2 \, t^2
&=&
z^2 \ +\  x^2
\\
t^2
&=&
\tau^2 \ +\ { x^2   \over  v^2 } 
\end{eqnarray}
where $\tau$ is traveltime depth.
This equation gives either time from a surface source
to a receiver at depth $\tau$,
or it gives time to a surface receiver
from an image source at depth $\tau$.

\par
A seismic \bx{trace} is a signal $d(t)$
recorded at some constant $x$.
We can convert the trace
to a ``vertical propagation'' signal
$m(\tau)=d(t)$
by stretching $t$ to $\tau$.
This process is called
``\bx{normal moveout} correction'' (NMO).
Typically we have many traces at different $x$ distances
each of which theoretically produces the same
hypothetical zero-offset trace.
Figure~\ref{fig:stretch} shows a marine shot profile
before and after NMO correction at the water velocity.
You can notice that the wave packet reflected from the ocean bottom
is approximately a constant width on the raw data.
After NMO, however,
this waveform broadens considerably---a phenomenon known
as ``NMO stretch."

\sideplot{stretch}{width=3.0in}{
  Marine data moved out with water velocity.
  Input on the left, output on the right.
%  Press button for \bx{movie} sweeping through
%  velocity (actually through slowness squared).
}

\par
The  NMO  transformation ${\bf N}$ is representable as a square matrix.
The matrix  ${\bf N}$  is a $(\tau,t)$-plane containing all zeros
except an interpolation operator centered along the hyperbola.
The dots in the matrix below are zeros.
The input signal $d_t$ is put into the vector $\bold d$.
The output vector $\bold m$---i.e.,~the NMO'ed signal---is simply
$(d_6,d_6,d_6, d_7,d_7, d_8,d_8, d_9, d_{10}, 0)$.
In real life examples such as Figure~\ref{fig:stretch}
the subscript goes up to about one thousand instead of
merely to ten.

\begin{equation}
{\bf m\eq Nd} \eq
 \left[ 
  \begin{array}{c}
   m_1 \\ 
   m_2 \\
   m_3 \\
   m_4 \\
   m_5 \\
   m_6 \\
   m_7 \\
   m_8 \\
   m_9 \\
   m_{10}
  \end{array}
 \right] 
\eq
 \left[ 
  \begin{array}{cccccccccc}
   .&.&.&.&.&1&.&.&.&. \\
   .&.&.&.&.&1&.&.&.&. \\
   .&.&.&.&.&1&.&.&.&. \\
   .&.&.&.&.&.&1&.&.&. \\
   .&.&.&.&.&.&1&.&.&. \\
   .&.&.&.&.&.&.&1&.&. \\
   .&.&.&.&.&.&.&1&.&. \\
   .&.&.&.&.&.&.&.&1&. \\
   .&.&.&.&.&.&.&.&.&1 \\
   .&.&.&.&.&.&.&.&.&.
  \end{array}
 \right] \;
 \left[ 
  \begin{array}{c}
   d_1 \\ 
   d_2 \\
   d_3 \\
   d_4 \\
   d_5 \\
   d_6 \\
   d_7 \\
   d_8 \\
   d_9 \\
   d_{10}
  \end{array}
 \right] 
 \label{eqn:NMOarray}
\end{equation}

You can think of the matrix as having a horizontal $t$-axis and
a vertical $\tau$-axis.
The 1's in the matrix are arranged on the hyperbola
$t^2=\tau^2+x_0^2/v^2$.
The transpose matrix defining some ${\bf \tilde d}$
from $\bold m$ gives synthetic data ${\bf \tilde d}$ from the zero-offset
(or stack) model $\bold m$, namely,

\begin{equation}
{\bf \tilde d \eq N' m } \eq
 \left[ 
  \begin{array}{c}
   \tilde d_1 \\ 
   \tilde d_2 \\
   \tilde d_3 \\
   \tilde d_4 \\
   \tilde d_5 \\
   \tilde d_6 \\
   \tilde d_7 \\
   \tilde d_8 \\
   \tilde d_9 \\
   \tilde d_{10}
  \end{array}
 \right] 
\eq
 \left[ 
  \begin{array}{cccccccccc}
   .&.&.&.&.&.&.&.&.&. \\
   .&.&.&.&.&.&.&.&.&. \\
   .&.&.&.&.&.&.&.&.&. \\
   .&.&.&.&.&.&.&.&.&. \\
   .&.&.&.&.&.&.&.&.&. \\
   1&1&1&.&.&.&.&.&.&. \\
   .&.&.&1&1&.&.&.&.&. \\
   .&.&.&.&.&1&1&.&.&. \\
   .&.&.&.&.&.&.&1&.&. \\
   .&.&.&.&.&.&.&.&1&.
  \end{array}
 \right] \;
 \left[ 
  \begin{array}{c}
   m_1 \\ 
   m_2 \\
   m_3 \\
   m_4 \\
   m_5 \\
   m_6 \\
   m_7 \\
   m_8 \\
   m_9 \\
   m_{10}
  \end{array}
 \right] 
\label{eqn:NMOTarray}
\end{equation}

A program for \bx{nearest-neighbor normal moveout} as defined by
\sx{normal moveout, nearest neighbor}
equations~(\ref{eqn:NMOarray}) and (\ref{eqn:NMOTarray})
is {\tt nmo0()}.
Because of the limited alphabet of programming languages,
I used the keystroke {\tt z} to denote $\tau$.%
\opdex{nmo0}{normal moveout}{52}{61}{user/gee}
A program is a ``pull'' program if the loop creating the output
covers each location in the output and gathers the input from wherever
it may be.
A program is a ``push'' program if it takes each input and
pushes it to wherever it belongs.
Thus this NMO program is a ``pull'' program
for doing the model building (data processing),
and it is a ``push'' program for the data building.
You could write a program that worked the other way around,
namely, a loop over $t$ with $z$ found
by calculation $z=\sqrt{t^2/v^2-x^2}$.
What is annoying is that if you want a push program going
both ways, those two ways cannot be adjoint to one another.

\par
Normal moveout is a linear operation.
This means that data can be decomposed into any two parts,
early and late,
high frequency and low,
smooth and rough,
steep and shallow dip, etc.;
and whether the two parts
are NMO'ed either separately or together, the result is the same.
The reason normal moveout is a linear operation
is that we have shown it is effectively a matrix multiply operation
and that operation fulfills ${\bf N(d_1+d_2) = Nd_1+Nd_2}$.

\section{COMMON-MIDPOINT STACKING}
\inputdir{stack}
Typically, many receivers record every shot, and there are many shots
over the reflectors of interest.
It is common practice to define 
the midpoint $y =(x_s+x_g)/2$ and then to sort the
seismic traces into ``\bx{common-midpoint} gathers''.
%\todo {
%	The reason for this is shown in Figure~\FIG{midpoint},
%	which shows that the traces in a common-midpoint
%	gather all correspond to reflection from same place on the reflector.
%	This would not be true on a common-shot gather,
%	nor would it be true if the earth model were not stratified
%	(as we shall see in a later chapter).
%	\activesideplot{midpoint}{width=2.2in}{}{
%		Raypaths for a common-midpoint gather.  Various shot/receiver
%		combinations that have the same shot-receiver midpoint
%		will correspond to reflection from the same point on
%		the reflector, regardless of the offset $2h$.
%		NOTE:  Need to create this figure (JLB)!!
%		}
%	}
After sorting,
each trace on a common-midpoint gather can be transformed by NMO
into an equivalent zero-offset trace
and the traces in the gather can all be added together.
This is often called ``common-depth-point (\bx{CDP}) stacking'' 
or, more correctly, ``\bx{common-midpoint stack}ing''.

\par
The adjoint to this operation is to begin from
a model that is identical to the zero-offset trace
and spray this trace to all offsets.
There is no ``official'' definition
of which operator of an operator pair is the operator itself
and which is the adjoint.
On the one hand, I like to think of the modeling operation itself
as {\em  the} operator.
On the other hand,
the industry machinery keeps churning away at many processes
that have well-known names,
so people often think of one of them as {\em  the} operator.
Industrial data-processing operators are typically
\bx{adjoint}s
to modeling operators.

\par
Figure~\ref{fig:stack} illustrates the operator pair, consisting
of spraying out a zero-offset trace (the model) to all offsets
and the adjoint of the spraying,
which is \bx{stack}ing.
The moveout and stack operations are in subroutine {\tt stack0()}.
\opdex{stack0}{NMO stack}{53}{56}{user/gee}%
Let $\bold S'$ denote NMO, and let the stack be
defined by invoking {\tt stack0()} with the {\tt adj=1} argument.
Then $\bold S$ is the modeling operation
defined by invoking {\tt stack0()} with the {\tt adj=0} argument.
Figure~\ref{fig:stack} illustrates both.%
\sideplot{stack}{width=3in}{
  Top is a model trace $\bold m$.
  Center shows the spraying to synthetic traces, $\bold S' \bold m$.
  Bottom is the stack of the synthetic data, ${\bf S S' m}$.
}
Notice the roughness on the waveforms caused by
different numbers of points landing in one place.
Notice also the increase of \bx{AVO}
(\bx{amplitude} versus \bx{offset})
as the waveform gets compressed into a smaller space.
Finally, notice that the stack is a little rough,
but the energy is all in the desired time window.

\par
We notice a contradiction of aspirations.
On the one hand,
an operator has smooth outputs if it
``loops over output space''
and finds its input where ever it may.
On the other hand,
it is nice to have modeling and processing
be exact adjoints of each other.
Unfortunately,
we cannot have both.
If you loop over the output space of an operator,
then the adjoint operator has a loop over input space
and a consequent roughness of its output.

\subsection{Crossing traveltime curves}
\inputdir{strat}
\sx{crossing traveltime curves}
\sx{traveltime curves, crossing}
\par
Since velocity increases with depth,
at wide enough offset a deep enough path
will arrive sooner than a shallow path.
In other words,
traveltime curves for shallow events
must cut across the curves of deeper events.
Where traveltime curves cross,
NMO is no longer a one-to-one transformation.
To see what happens to the \bx{stack}ing process
I prepared Figures~\ref{fig:nmoalfa0}-\ref{fig:nmoalfa0.5}
using a typical marine recording geometry
(although for clarity I used
larger $(\Delta t,\Delta x)$) and we will use
a typical Texas gulf coast average velocity,
$v(z)=1.5+\alpha z$ where $\alpha=.5$.

First we repeat the calculation of Figure~\ref{fig:stack}
with constant velocity $\alpha=0$ and more reflectors.
We see in Figure~\ref{fig:nmoalfa0} that the \bx{stack} reconstructs
the model except for two details:
(1) the \bx{amplitude} diminishes with time, and 
(2) the early waveforms have become rounded.

\sideplot{nmoalfa0}{width=3in}{
  Synthetic CMP gather for constant velocity earth and reconstruction.
}

Then we repeat the calculation
with the Gulf coast typical velocity gradient $\alpha=1/2$.
The polarity reversal on the first arrival of the wide offset trace
in Figure~\ref{fig:nmoalfa0.5}
is evidence that in practice traveltime curves do cross.
(As was plainly evident in Figures
\ref{fig:wzl.34},
\ref{fig:wzl.20} and
\ref{fig:wzl.32}
crossing traveltime curves are even more significant elsewhere in the world.)
Comparing Figure~\ref{fig:nmoalfa0} to Figure~\ref{fig:nmoalfa1}
we see that an effect of the velocity gradient
is to degrade the \bx{stack}'s reconstruction of the model.
Velocity gradient has ruined the waveform on the shallowest event,
at about 400ms.
If the plot were made on a finer mesh
with higher frequencies,
we could expect ruined waveforms a little deeper too.

\sideplot{nmoalfa1}{width=3in}{
  Synthetic CMP gather 
  for velocity linearly increasing with depth
  (typical of Gulf of Mexico)
  and reconstruction.
}

\begin{comment}
\par
Our NMO and stack subroutines can be used for modeling or for data processing.
In designing these programs we gave no thought to signal \bx{amplitude}s
(although results showed an interesting \bx{AVO} effect in Figure~\ref{fig:stack}.)
We could redesign the programs so that the modeling operator has the
most realistic \bx{amplitude} that we can devise.
Alternately, we could design the \bx{amplitude}s
to get the best approximation to
$\bold S'\bold S\approx \bold I$
which should result in ``{\tt Stack}''
being a good approximation to ``{\tt Model}.''
I experimented with various weighting functions until
I came up with
subroutines \texttt{nmo1()} \vpageref{lst:nmo1}
and {\tt stack1()} (like \texttt{stack0()} \vpageref{lst:stack0})
%\sx{subroutine!{\tt stack1}, weighted NMO stack}
which embodies the \bx{weighting function} $(\tau/t)(1/\sqrt{t})$
and which produces the result in Figure~\ref{fig:nmo1alfa.5}.

\activesideplot{nmo1alfa.5}{width=3in}{ER}{
	Synthetic CMP gather 
	for velocity linearly increasing with depth
	and reconstruction with weighting functions
	in subroutine {\tt nmo1()}.
	Lots of adjustable parameters here.
	}
The result in Figure~\ref{fig:nmo1alfa.5} is very pleasing.
Not only is the \bx{amplitude} as a function of time better preserved,
more importantly, the shallow wavelets are less smeared
and have recovered their rectangular shape.
The reason the reconstruction is much better is
the cosine weighting implicit in $\tau/t$.
It has muted away much of the energy in the shallow asymptote.
I think this energy near the asymptote is harmful
because the waveform stretch is so large there.
Perhaps a similar good result could be found by experimenting
with muting programs such as \texttt{mutter()} \vpageref{lst:mutter}.
However, subroutine \texttt{nmo1()} \vpageref{lst:nmo1} differs from {\tt mutter()}
in two significant respects:
(1) {\tt nmo1()} is based on a theoretical concept
whereas {\tt mutter()} requires observational parameters
and (2) {\tt mutter()} applies a weighting in
the coordinates of the $(t,x)$ input space,
while {\tt nmo1()} does that
but also includes the coordinate $\tau$ of the the output space.
With {\tt nmo1()} events from different $\tau$ depths see different mutes
which is good where a shallow event asymptote
crosses a deeper event far from its own asymptote.
In practice the problem of crossing traveltime curves is severe,
as evidenced by
Figures \ref{fig:wzl.34}-\ref{fig:wzl.32}
and both weighting during NMO and muting should be used.

%\progdex{nmo1}{weighted NMO}
\opdex{nmo1}{weighted normal moveout}{52}{62}{user/gee}

It is important to realize that the most accurate possible
physical \bx{amplitude}s are not necessarily those for which
$\bold S'\bold S\approx \bold I$.
Physically accurate amplitudes involve many theoretical
issues not covered here.
It is easy to include some effects
(spherical divergence based on velocity depth variation)
and harder to include others
(surface ghosts and arrays).
We omit detailed modeling here
because it is the topic of so many other studies.
\end{comment}

\subsection{Ideal weighting functions for stacking}
The difference between \bx{stack}ing as defined by
\texttt{nmo0()} \vpageref{lst:nmo0} and by
\texttt{nmo1()} \vpageref{lst:nmo1}
is in the weighting function $(\tau/t)(1/\sqrt{t})$.
This weight made a big difference in the resolution of the stacks
but I cannot explain
whether this weighting function is the best possible one,
or what systematic procedure
leads to the best weighting function in general.
To understand this better, 
notice that $(\tau/t)(1/\sqrt{t})$
can be factored into two weights, $\tau$ and $t^{-3/2}$.
One weight could be applied before NMO and the other after.
That would also be more efficient than weighting inside NMO,
as does {\tt nmo1()}.
Additionally, it is likely that
these weighting functions should take into account
data truncation at the cable's end.
Stacking is the most important operator in seismology.
Perhaps some objective measure of quality can be defined
and arbitrary powers of $t$, $x$, and $\tau$
can be adjusted until the optimum stack is defined.
Likewise, we should consider weighting functions in the spectral domain.
As the weights $\tau$ and $t^{-3/2}$
tend to cancel one another,
perhaps we should filter with opposing filters
before and after \bx{NMO} and stack.

\subsection{Gulf of Mexico stack and AGC}
\inputdir{vscan}
\par
Next we create a ``CDP stack'' of our the Gulf of Mexico data set.
Recall the moved out common-midpoint (CMP) gather
Figure~\ref{fig:nmogath}.
At each midpoint there is one of these CMP gathers.
Each gather is summed over its offset axis.
Figure~\ref{fig:wgstack} shows the result of stacking over offset,
at each midpoint.
The result is an image of a cross section of the earth.
\plot{wgstack}{width=6.00in,height=3.6in}{
  Stack done with a given velocity profile for all midpoints.
}

In Figure~\ref{fig:wgstack} the early signals are too weak to see.
This results from the small number of traces
at early times because of the mute function.
(Notice missing information at wide offset and early time
on Figure~\ref{fig:nmogath}.)
To make the stack properly, we should divide by the number of nonzero traces.
The fact that the mute function is tapered rather than cut off abruptly
complicates the decision of what is a nonzero trace.
In general we might like to apply a weighting function of offset.
How then should the stack be weighted with time to preserve
something like the proper signal strength?
A solution is to make constant synthetic data (zero frequency).
Stacking this synthetic data gives a weight that can be used
as a divisor when stacking field data.
I prepared code for such weighted stacking,
but it cluttered the NMO and stack program and required
two additional new subroutines,
so I chose to leave the clutter in the electronic book
and not to display it here.
Instead, I chose to solve the signal strength problem
by an old standby method, Automatic Gain Control (AGC).
A divisor for the data is found by smoothing the absolute
values of the data over a moving window.
To make Figure~\ref{fig:agcstack} I made the divisor by smoothing
in triangle shaped windows about a half second long.
%To do this, I used subroutine \texttt{triangle()} \vpageref{lst:triangle}.

\plot{agcstack}{width=6.00in,height=3.6in}{
  Stack of Figure~\protect\ref{fig:wgstack} after AGC.
}

%\subsection{Jon's stack weighting function fantasy}
%The code is sprinkled around in vela/Trueamp.
%If the figure looks OK,
%my cleanup will consist of:
%
%Merge and include nmow() and stack2(), renaming them suitably.
%
%Include waiter().
%
%\activeplot{stackw}{width=6.00in,height=3.6in}{}{
%	Stack with weighting function.
%	}

\section{VELOCITY SPECTRA}
\sx{velocity spectrum}
An important transformation in exploration geophysics
takes data as a function of shot-receiver offset
and transforms it to data as a function of apparent velocity.
Data is summed along hyperbolas of many velocities.
This important industrial process is adjoint to another that may
be easier to grasp:  data is synthesized by a superposition
of many hyperbolas.
The hyperbolas have various asymptotes (velocities) and various
tops (apexes).
Pseudocode for these transformations is
\par\noindent

\vbox{\begin{tabbing}
indent \= char \= char \= char \= char \= lotsofcharshereIhope \= \kill
\>do $v$ \{		\\
\>do $\tau$ \{ \\
\>do $x$  \{\\
\>	\>	\>$t = \sqrt{ \tau^2 + x^2/v^2 }$ \\
\>	\>	\>if hyperbola superposition \\
\>	\>	\>\>\> data$(t,x)$ = data$(t,x)$ + vspace$(\tau,v)$ \\
\>	\>	\>else if velocity analysis \\
\>	\>	\>\>\> vspace$(\tau,v)$ = vspace$(\tau,v)$ + data$(t,x)$\\
\>	\>	\}\}\}
\end{tabbing}}
\par\noindent
This pseudocode transforms one plane to another using the equation
$t^2 = \tau^2 +x^2/v^2$.  This equation relates four variables,
the two coordinates of the data space $(t,x)$
and the two of the model space $(\tau,v)$.
Suppose a model space is all zeros except for an impulse at $(\tau_0, v_0)$.
The code copies this inpulse to data space everywhere where
$t^2 = \tau_0^2 + x^2/v_0^2$.   In other words, the impulse
in velocity space is copied to a hyperbola in data space.
In the opposite case an impulse at a point in data space $(t_0,x_0)$
is copied to model space everywhere that satisfies the equation
$t_0^2 = \tau^2 + x_0^2/v^2$.  
Changing from velocity space to
slowness space this equation
$t_0^2 = \tau^2 + x_0^2 s^2$
has a name.   In $(\tau, s)$-space it is an ellipse
(which reduces to a circle when $x_0^2=1$.

%XX
\par
Look carefully in the model spaces of
Figure~\ref{fig:velvel} and
Figure~\ref{fig:mutvel}.
Can you detect any ellipses?
For each ellipse,
does it come from a large $x_0$ or a small one?
Can you identify the point $(t_0,x_0)$ causing the ellipse?



\par
We can ask the question, if we transform data to velocity space,
and then return to data space,
will we get the original data?
Likewise we could begin from the velocity space,
synthesize some data, and return to velocity space.
Would we come back to where we started?
The answer is yes, in some degree.
Mathematically, the question amounts to this:
Given the operator $\bold A$, is $\bold A'\bold A$ approximately
an identity operator, i.e.~is $\bold A$ nearly a unitary operator?
It happens that $\bold A'\bold A$ defined by the pseudocode above
is rather far from an identity transformation,
but we can bring it much closer
by including some simple scaling factors.
It would be a lengthy digression here to derive all these weighting factors
but let us briefly see the motivation for them.
One weight arises because waves lose \bx{amplitude} as they spread out.
Another weight arises because some angle-dependent effects should be taken
into account.  A third weight arises because in creating a velocity space,
the near offsets are less important than the wide offsets
and we do not even need the zero-offset data.
A fourth weight is a frequency dependent one
which is explained in chapter~\ref{paper:ft1}.
Basically, the summations in the velocity transformation are like integrations,
thus they tend to boost low frequencies.
This could be compensated by scaling
in the frequency domain
with frequency as $\sqrt{-i\omega}$.
 with subroutine \texttt{halfint()} \vpageref{lst:halfint}.
% To remove reference to halfdifa if Section 6.5.2 is removed - Biondo 4/96

\par
The weighting issue will be examined in more detail later.
Meanwhile, we can see nice quality examples
from very simple programs
if we include the weights
in the physical domain, $w= \sqrt{1/t}\; \sqrt{x/v}\; \tau /t $.
(Typographical note:  Do not confuse
the weight $w$ (double you) with omega $\omega$.)
To avoid the coding clutter of the frequency domain weighting
$\sqrt{-i\omega}$ I omit that,
thus getting smoother results than theoretically preferable.
Figure~\ref{fig:velvel} illustrates this smoothing by starting
from points in velocity space, transforming to offset,
and then back and forth again.

\plot{velvel}{width=5.50in}{
  Iteration between spaces.
  Left are model spaces.
  Right are data spaces.
  Right derived from left.
  Lower model space derived from upper data space.
}

\par
There is one final complication relating to weighting.
The most symmetrical approach is to put
$w$ into both $\bold A$ and $\bold A'$.
%This is what subroutine \texttt{velsimp()} \vpageref{lst:velsimp} does.
Thus, because of the weighting by $\sqrt{x}$,
the synthetic data in Figure~\ref{fig:velvel} is
nonphysical.
An alternate view is to {\em  define} $\bold A$
(by the pseudo code above, or by some modeling theory)
and then for reverse transformation
use $w^2\bold A'$.

%\progdex{velsimp}{velocity spectra}

\par
An example %of applying subroutine \texttt{velsimp()} \vpageref{lst:velsimp}
%to field data 
is shown in Figure~\ref{fig:mutvel}.

\plot{mutvel}{width=6.00in,height=3.6in}{
  Transformation of data as a function of offset (left)
  to data as a function of slowness (velocity scans)
  on the right
  using subroutine {\tt velsimp()}.
}

\subsection{Velocity picking}
\sx{velocity!picking}
For many kinds of data analysis,
we need to know the velocity of the earth as a function of depth.
To derive such information
we begin from Figure~\ref{fig:mutvel}
and draw a line through the maxima.
In practice this is often a tedious manual process,
and it needs to be done everywhere we go.
There is no universally accepted way to automate
this procedure, but we will consider one
that is simple enough that it can be fully described here,
and which works well enough for these demonstrations.
(I plan to do a better job later.)

\par
Theoretically we can define the velocity or slowness
as a function of traveltime depth by the moment function.
Take the absolute value of the data scans and smooth
them a little on the time axis to make something like an unnormalized
probability function, say $p(\tau ,s)>0$.
Then the slowness $s(\tau )$ could be defined by the moment function, i.e.,
\begin{equation}
s(\tau ) \eq { \sum_s \ s\  p(\tau ,s) \over \sum_s \ p(\tau ,s) }
\label{eqn:mommy}
\end{equation}
The problem with defining slowness $s(\tau )$ by the moment is that it is 
strongly influenced by noises away from the peaks,
particularly water velocity noises.
Thus, better results can be obtained if the sums in equation~(\ref{eqn:mommy})
are limited to a range about the likely solution.
To begin with, we can take the likely solution to be defined
by universal or regional experience.
It is sensible to begin from a one-parameter equation
for velocity increasing with depth where the form of the equation
allows a ray tracing solution
such as equation~(\ref{eqn:Vrms}).
Experience with Gulf of Mexico data shows that
$\alpha\approx 1/2\  {\rm sec}^{-1}$ is reasonable there
for equation~(\ref{eqn:Vrms}).
%and that is the smooth curve
%in Figure~\ref{fig:slowfit}.

\par
Experience with moments,
equation~(\ref{eqn:mommy}),
shows they are reasonable when
the desired result is near the guessed center of the range.
Otherwise, the moment is biased towards the initial guess.
This bias can be reduced in stages.
At each stage we shrink the width of the zone used to compute the moment.
%This procedure is used in subroutine \texttt{slowfit()} \vpageref{lst:slowfit}
%which
%after smoothing to be described,
%gives the oscillatory curve you see in Figure~\ref{fig:slowfit}.

%\progdex{slowfit}{velocity est.}

A more customary way to view velocity space
is to square the velocity scans
and normalize them by the sum of the squares of the signals.
This has the advantage that the remaining information
represents velocity spectra
and removes variation due to seismic \bx{amplitude}s.
Since in practice, reliability seems somehow proportional to \bx{amplitude}
the disadvantage of normalization
is that reliability becomes more veiled.

\begin{comment}
\par
An appealing visualization of velocity is shown in the right side
of Figure~\ref{fig:slowfit}.
This was prepared from the absolute value of left side,
followed by filtering spatially with an antisymmetric
leaky integral function.
(See PVI page 57).
An example is shown on the right side of Figure~\ref{fig:slowfit}.

%\plot{slowfit}{width=6.00in,height=3.6in}{
  Left is the slowness scans.
  Right is the slowness scans after absolute value,
  smoothing a little in time,
  and antisymmetric leaky integration over slowness.
  Overlaying both is the line of slowness picks.
}
\end{comment}

\sideplot{fit}{width=3.00in,height=3.6in}{
  Slowness scans.
%  Right is the slowness scans after absolute value,
%  smoothing a little in time,
%  and antisymmetric leaky integration over slowness.
  Overlaying is the line of slowness picks.
}

%
% The plot below is not stable enough to include.
%
%\aKtivesideplot{vstack}{width=3.00in}{vscan}{
%	Stacking velocity.
%	Repeating the analysis of Figure~\protect\FIG{slowfit}
%	for hundreds of midpoint $y$-values gives this slowness
%	model, $s(t,y)$.
%	The velocity obviously varies with depth.
%	The validity of lateral variation is doubtful here,
%	though we will examine it more carefully later.
%	}

\subsection{Stabilizing RMS velocity}
\par
\sx{velocity!RMS}
\sx{velocity!interval}
With velocity analysis, we estimate the \RMS\ velocity.
Later we will need both the \RMS\ velocity and the \bx{interval velocity}.
(The word ``interval'' designates an interval between two reflectors.)
Recall from chapter~\ref{paper:wvs} equation~(\ref{eqn:vrmshyp})
$$
t^2 \eq \tau^2 + \frac{4h^2}{V^2(\tau)}  \nonumber
$$
\par
%Routine~\texttt{vint2rms()} \vpageref{lst:vint2rms}
%converts from interval velocity to \RMS\ velocity
%and vice versa.
%\progdex{vint2rms}{interval to/from RMS vel}
The forward conversion follows 
in straightforward steps: square, integrate, square root.
The inverse conversion, like an adjoint,
retraces the steps of the forward transform
but it does the inverse at every stage.
There is however,
a messy problem with nearly all field data
that must be handled along the inverse route.
The problem is that the observed \RMS\ velocity function
is generally a rough function,
and it is generally unreliable over a significant portion of its range.
To make matters worse,
deriving an \bx{interval velocity} begins as does a derivative,
roughening the function further.
We soon find ourselves taking square roots of negative numbers,
which requires judgement to proceed.
\begin{comment}
The technique used in \texttt{vint2rms()} \vpageref{lst:vint2rms}
is to average the squared interval velocity
in ever expanding neighborhoods until there are no longer
any negative squared interval velocities.
As long as we are restricting $v^2$ from being negative,
it is easy to restrict it to be above some allowable velocity,
say {\tt vminallow}.
Figures~\ref{fig:rufsmo} and~\ref{fig:vrmsint}
were derived from the velocity scans in Figure~\ref{fig:slowfit}. %
\activeplot{rufsmo}{width=5.00in,height=2.5in}{ER}{
	Left is the raw \RMS\ velocity.
	Right is a superposition of \RMS\ velocities,
	the raw one,
	and one constrained to have realistic interval velocities.
	}%
Figure~\ref{fig:rufsmo} shows the \RMS\ velocity before and after
a trip backward and forward through routine~\texttt{vint2rms()} \vpageref{lst:vint2rms}.
The interval velocity associated with the smoothed velocity
is in figure~\ref{fig:vrmsint}.
	

\activesideplot{vrmsint}{width=2.50in,height=2.5in}{ER}{
	Interval velocity associated with the smoothed
	\RMS\ velocity of Figure~\protect\ref{fig:rufsmo}.
	Pushbutton allows experimentation with {\tt vminallow}.
	}
\end{comment}
\plot{wgvel1}{width=\textwidth}{Left is a superposition of \RMS\
  velocities, the raw one, and one constrained to have realistic
  interval velocities. Right is the nterval velocity.}












































































