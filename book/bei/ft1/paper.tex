\def\CAKEDIR{.}

\title{Waves and Fourier sums}
\author{Jon Claerbout}
\maketitle
\label{paper:ft1}
%\long\def\HideThis#1{}

%{\em \today . This chapter is owned by JFC.}

\todo{jarring jump from recommending odd FFT, then using even}

\sx{Fourier transform!discrete}
\par
An important concept in wave imaging is the extrapolation
of a wavefield from one depth $z$ to another.
Fourier transforms are an essential basic tool.
There are many books and chapters of books on the
{\em  theory}
of Fourier transformation.
The first half of this chapter is
an introduction to {\em  practice} with Fourier sums.
It assumes you already know something of the theory
and takes you through the theory rather quickly
emphasizing practice by examining examples,
and by performing two-dimensional
Fourier transformation of data and interpreting the result.
For a somewhat more theoretical background,
I suggest my previous book PVI at
http://sepwww.stanford.edu/sep/prof/.


\par
The second half of this chapter
uses Fourier transformation
to explain the Hankel waveform we observed
in chapter~\ref{paper:vela} and chapter~\ref{paper:krch}.
Interestingly,
it is the Fourier transform of $\sqrt{-i\omega}$,
which is half the derivative operator.

\section{FOURIER TRANSFORM}
We first examine the two ways to visualize polynomial multiplication.
The two ways lead us to the most basic principle of Fourier analysis
that
\par
\boxit{ A product in the Fourier domain
	is a convolution in the physical domain}
\par\noindent
Look what happens to the coefficients when we multiply polynomials.
\begin{eqnarray}
X(Z)\, B(Z) &\eq & Y(Z) \label{eqn:1-1-5} \\
(x_0 + x_1 Z + x_2 Z^2 + \cdots )\, (b_0 + b_1 Z + b_2 Z^2) &\eq &
y_0 + y_1 Z + y_2 Z^2 + \cdots  \label{eqn:1-1-6}
\end{eqnarray}
Identifying coefficients of successive powers of $Z$, we get
\begin{eqnarray}
y_0 &\eq & x_0 b_0 \nonumber \\
y_1 &\eq & x_1 b_0  + x_0 b_1 \nonumber \\
y_2 &\eq & x_2 b_0 + x_1 b_1 + x_0 b_2 \label{eqn:1-1-7} \\
y_3 &\eq & x_3 b_0 + x_2 b_1 + x_1 b_2 \nonumber \\
y_4 &\eq & x_4 b_0 + x_3 b_1 + x_2 b_2 \nonumber \\
    &\eq & \cdots\cdots\cdots\cdots\cdots\cdots  \nonumber
\end{eqnarray}
In matrix form this looks like
\begin{equation}
\left[ 
\begin{array}{c}
  y_0 \\ 
  y_1 \\ 
  y_2 \\ 
  y_3 \\ 
  y_4 \\ 
  y_5 \\ 
  y_6
  \end{array} \right] 
\eq
\left[ 
\begin{array}{ccc}
  x_0 & 0   & 0    \\
  x_1 & x_0 & 0    \\
  x_2 & x_1 & x_0  \\
  x_3 & x_2 & x_1  \\
  x_4 & x_3 & x_2  \\
  0   & x_4 & x_3  \\
  0   & 0   & x_4
  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  b_0 \\ 
  b_1 \\ 
  b_2 \end{array} \right]
\label{eqn:contran2}
\end{equation}
The following equation, called the
``convolution equation,''
carries the spirit of the group shown in (\ref{eqn:1-1-7})
\begin{equation}
y_k \eq  \sum_{i = 0} x_{k - i} b_i
\label{eqn:conv}
\end{equation}

\par
The second way to visualize polynomial multiplication is simpler.
Above we did not think of $Z$ as a numerical value.
Instead we thought of it as ``a unit delay operator''.
Now we think of the product $X(Z) B(Z) = Y(Z)$ numerically.
For all possible numerical values of $Z$,
each value $Y$ is determined
from the product of the two numbers $X$ and $B$.
Instead of considering all possible numerical values
we limit ourselves to all values of unit magnitude
$Z=e^{i\omega}$ for all real values of $\omega$.
This is Fourier analysis, a topic we consider next.

\subsection{FT as an invertible matrix}
\par
A \bx{Fourier sum} may be written
\begin{equation}
B(\omega) \eq  \sum_t \ b_t \ e^{i\omega t}
	  \eq  \sum_t \ b_t \ Z^t
\end{equation}
where the complex value $Z$
is related to the real frequency
$\omega$ by $Z=e^{i\omega}$.
This Fourier sum is a way of building
a continuous function of $\omega$
from discrete signal values $b_t$ in the time domain.
Here we specify both time and frequency domains by a set of points.
Begin with an example of a signal
that is nonzero at four successive instants,
$( b_0, b_1, b_2, b_3)$.
The transform is
\begin{equation}
B(\omega) \eq  b_0 + b_1 Z + b_2 Z^2 + b_3 Z^3
\end{equation}
The evaluation of this polynomial can be organized as a matrix times a vector,
such as
\begin{equation}
  \left[ \begin{array}{c}
   B_0 \\
   B_1 \\
   B_2 \\
   B_3  \end{array} \right]
\eq   \left[ \begin{array}{cccc}
   1 & 1 & 1 & 1 \\
   1 & W & W^2 & W^3 \\
   1  & W^2 & W^4 & W^6 \\
   1  & W^3 & W^6 & W^9  \end{array} \right]  \;
  \left[ \begin{array}{c}
   b_0 \\
   b_1 \\
   b_2 \\
   b_3  \end{array} \right]   
\label{eqn:3-3}
\end{equation}
Observe that the top row of the matrix evaluates the polynomial at $Z=1$,
a point where also
$\omega=0$.
The second row evaluates $B_1=B(Z=W=e^{i\omega_0})$,
where $\omega_0$ is some base frequency.
The third row evaluates the Fourier transform for $2\omega_0$,
and the bottom row for $3\omega_0$.
The matrix could have more than four rows for more frequencies
and more columns for more time points.
I have made the matrix square in order to show you next
how we can find the inverse matrix.
The size of the matrix in (\ref{eqn:3-3}) is $N=4$.
If we choose the base frequency $\omega_0$ and hence $W$ correctly,
the inverse matrix will be
\begin{equation}
  \left[ \begin{array}{c}
   b_0 \\
   b_1 \\
   b_2 \\
   b_3  \end{array} \right]
\eq   1/N \; 
\left[ \begin{array}{cccc}
   1 & 1 & 1 & 1 \\
   1 & 1/W & 1/W^2 & 1/W^3 \\
   1  & 1/W^2 & 1/W^4 & 1/W^6 \\
   1  & 1/W^3 & 1/W^6 & 1/W^9  \end{array} \right]  \;
  \left[ \begin{array}{c}
   B_0 \\
   B_1 \\
   B_2 \\
   B_3 \end{array} \right]   
\label{eqn:3-5}
\end{equation}
Multiplying the matrix of
(\ref{eqn:3-5}) with that of 
(\ref{eqn:3-3}),
we first see that the diagonals are +1 as desired.
To have the off diagonals vanish,
we need various sums,
such as $1+W  +W^2+W^3$
and     $1+W^2+W^4+W^6$, to vanish.
Every element ($W^6$, for example,
or $1/W^9$) is a unit vector in the complex plane.
In order for the sums of the unit vectors to vanish,
we must ensure that the vectors pull symmetrically away from the origin.
A uniform distribution of directions meets this requirement.
In other words, $W$ should be the $N$-th root of unity, i.e.,
\begin{equation}
W \eq
\sqrt[N]{1} \eq
e^{2\pi i/N} 
\label{eqn:3-4}
\end{equation}
\par
The lowest frequency is zero, corresponding to the top row of
(\ref{eqn:3-3}).
The next-to-the-lowest frequency we find by setting $W$ in
(\ref{eqn:3-4}) to $Z=e^{i\omega_0}$.
So $\omega_0=2\pi /N$; and
for (\ref{eqn:3-5}) to be inverse to (\ref{eqn:3-3}),
the frequencies required are
\begin{equation}
\omega_k \eq { (0, 1, 2, \ldots , N- 1) \, 2\pi \over N} 
\label{eqn:3-2}
\end{equation}



\subsection{The Nyquist frequency}
\inputdir{matrix}
The highest frequency
in equation~(\ref{eqn:3-2}),
$\omega=2\pi (N-1)/N$,
is almost $2\pi$.
This frequency is twice as high as the Nyquist frequency $\omega=\pi$.
The \bx{Nyquist frequency}
is normally thought of as the ``highest possible'' frequency,
because $e^{i\pi t}$, for integer $t$,
plots as $(\cdots ,1,-1,1,-1,1,-1,\cdots)$.
The double Nyquist frequency function,
$e^{i2\pi t}$, for integer $t$,
plots as $(\cdots ,1,1,1,1,1,\cdots)$.
So this frequency above the highest frequency is really zero frequency!
We need to recall that $B(\omega)=B(\omega -2\pi )$.
Thus, all the frequencies near the upper end of the range equation~(\ref{eqn:3-2})
are really small negative frequencies.
Negative frequencies on the interval $(-\pi,0)$
were moved to interval $(\pi,2\pi)$
by the matrix form of Fourier summation.

\par
A picture of the Fourier transform matrix is shown in Figure \ref{fig:matrix}.
Notice the Nyquist frequency is the center row
and center column of each matrix.
\plot{matrix}{width=6in, height=6in}{
  Two different graphical means of showing the
  real and imaginary parts of the Fourier transform
  matrix of size $32\times 32$.
}
%\newslide


\subsection{Laying out a mesh}
\par
In theoretical work and in programs,
the unit delay operator definition $Z=e^{i\omega \Delta t}$
is often simplified to $\Delta t = 1$,
leaving us with $ Z = e^{i\omega} $.
How do we know whether $\omega$
is given in radians per second or radians per sample?
We may not invoke a cosine or an exponential unless
the argument has no physical dimensions.
So where we see $\omega$ without $\Delta t$,
we know it is in units of radians per sample.
\par
In practical work,
frequency is typically given in cycles/sec or \bx{Hertz}, $f$,
rather than radians, $\omega$
(where $\omega = 2\pi f$).
Here we will now switch to $f$.
We will design a computer \bx{mesh} on a physical object
(such as a waveform or a function of space).
\label{'mesh'}
We often take the mesh to begin at $t=0$,
and continue till the end $t_{\rm max}$ of the object,
so the time range $t_{\rm range} = t_{\rm max}$.
Then we decide how many points we want to use.
This will be the $N$ used in the discrete Fourier-transform program.
Dividing the range by the number gives a mesh interval $\Delta t$.
\par
Now let us see what this choice implies in the frequency domain.
We customarily take the maximum frequency to be the Nyquist,
either $f_{\rm max} = .5 /\Delta t$ Hz or
$\omega_{\rm max} = \pi /\Delta t$ radians/sec.
The frequency range $f_{\rm range}$ goes from $-.5/\Delta t$ to $.5/\Delta t$.
In summary:
\begin{itemize}
\item $\Delta t \eq t_{\rm range} / N \quad$ is time \bx{resolution}.
\item $f_{\rm range} \eq 1/\Delta t \eq N /t_{\rm range} \quad$
	is frequency range.
\item $\Delta f \eq f_{\rm range}/N \eq 1/t_{\rm range} \quad$
	is frequency \bx{resolution}.
\end{itemize}
In principle, we can always increase $N$ to refine the calculation.
Notice that increasing $N$ sharpens the time resolution
(makes $\Delta t$ smaller)
but does not sharpen the frequency resolution
$\Delta f$, which remains fixed.
Increasing $N$ increases the frequency
{\em  range,}
but not the frequency
{\em  resolution.}
\par
What if we want to increase the frequency resolution?
Then we need to choose $t_{\rm range}$ larger than required to
cover our object of interest.
Thus we either record data over a larger range,
or we assert that such measurements would be zero.
Three equations summarize the facts:
\begin{eqnarray}
\Delta t \ f_{\rm range} &=& 1			\\
\Delta f \ t_{\rm range} &=& 1			\\
\Delta f \ \Delta t        &=& {1 \over N}
\label{eqn:dtdf}
\end{eqnarray}
\par
\boxit{Increasing {\em  range} in the time domain increases
	{\em  resolution} in the frequency domain and vice versa.
	Increasing \bx{resolution} in one domain
	does not increase \bx{resolution} in the other.
	}

\section{INVERTIBLE SLOW FT PROGRAM}

\par
Typically, signals are real valued.
But the programs in this chapter are for complex-valued signals.
In order to use these programs,
copy the real-valued signal into a complex array,
where the signal goes into the real part of the complex numbers;
the imaginary parts are then automatically set to zero.

\par
There is no universally correct choice
of \bx{scale factor} in Fourier transform:
choice of scale is a matter of convenience.
Equations~(\ref{eqn:3-3}) and (\ref{eqn:3-5}) mimic the $Z$-transform,
so their scaling factors are
convenient for the convolution theorem---that
a product in the frequency domain is a convolution in the time domain.
Obviously, the scaling factors of
equations~(\ref{eqn:3-3}) and (\ref{eqn:3-5})
will need to be interchanged for the
complementary theorem 
that a convolution in the frequency domain
is a product in the time domain.
I like to use a scale factor that keeps the sums of squares
the same in the time domain as in the frequency domain.
Since I almost never need the scale factor,
it simplifies life to omit it from the subroutine argument list.
\begin{comment}
When a scaling program is desired,
we can use a simple one like \texttt{scale()} \vpageref{lst:scale}.
Complex-valued data can be scaled with {\tt scale()}
merely by doubling the value of {\tt n}.

\progdex{scale}{scale an array}

\subsection{The simple FT code}
Subroutine \texttt{simpleft()} \vpageref{lst:simpleft} exhibits features
found in many physics and engineering programs.
For example, the time-domain signal (which is denoted ``{\tt tt()}"),
has {\tt nt} values subscripted, from {\tt tt(1)} to {\tt tt(nt)}.
The first value of this signal {\tt tt(1)} is located
in real physical time at {\tt t0}.
The time interval between values is {\tt dt}.
The value of {\tt tt(it)} is at time {\tt t0+(it-1)*dt}.
We do not use ``{\tt if}'' as a pointer on the frequency axis
because {\tt if} is a keyword in most programming languages.
Instead, we count along the frequency axis with a variable named {\tt ie}.%
\progdex{simpleft}{slow FT}
%\newslide
The total frequency band is
$2\pi$ radians per sample unit
or $1/\Delta t$ Hz.
Dividing the total interval by the number of points {\tt nf} gives $\Delta f$.
We could choose the frequencies to run from 0 to $2\pi$ radians/sample.
That would work well for many applications,
but it would be a nuisance for applications such as differentiation
in the frequency domain, which require multiplication by $-i\omega$
including the \bxbx{negative frequencies}{negative frequency}
as well as the positive.
So it seems more natural to begin at the most negative frequency
and step forward to the most positive frequency.
\end{comment}

\section{CORRELATION AND SPECTRA}
The spectrum of a signal is a positive function of frequency
that says how much of each tone is present.
The Fourier transform of a spectrum yields an interesting function
called an ``\bx{autocorrelation},''
which measures the similarity of a signal to itself shifted.

\subsection{Spectra in terms of Z-transforms}
Let us look at spectra in terms of $Z$-transforms.
Let a \bx{spectrum} be denoted $S(\omega)$, where
\begin{equation}
S(\omega) \eq |B(\omega)|^2 \eq \overline{B(\omega)}B(\omega)
\label{eqn:1-5-3}
\end{equation}
Expressing this in terms of a three-point $Z$-transform, we have
\begin{eqnarray}
S(\omega) & = & (\bar{b}_0+\bar{b}_1 e^{-i\omega} + 
		\bar{b}_2 e^{-i2\omega})
		(b_0 + b_1e^{i\omega} +b_2 e^{i2\omega})
							\\
S(Z) & = & \left(\bar{b}_0 +\frac{\bar{b}_1}{Z} +
		\frac{\bar{b}_2}{Z^2} \right)
		(b_0 + b_1Z + b_2Z^2 )			\\
S(Z) & = & \overline{B} \left(\frac{1}{Z}\right) B(Z)
\end{eqnarray}
It is interesting to multiply out
the polynomial $\bar{B}(1/Z)$ with $B(Z)$ in order
to examine the coefficients of $S(Z)$:
\begin{eqnarray}
S(Z) &=& \frac{\bar{b}_2b_0}{Z^2}  + 
	\frac{(\bar{b}_1b_0 + \bar{b}_2b_1)}{Z} +
	(\bar{b}_0b_0 + \bar{b}_1b_1 + \bar{b}_2 b_2)
    + (\bar{b}_0 b_1 + \bar{b}_1b_2)Z + \bar{b}_0 b_2 Z^2 \nonumber \\
S(Z) &=& \frac{s_{-2}}{Z^2} + \frac{s_{-1}}{Z} + s_0 + s_1Z + s_2 Z^2
\label{eqn:1-5-8}
\end{eqnarray}
The coefficient $s_k$ of $Z^k$ is given by
\begin{equation}
s_k \eq \sum_{i} \bar{b}_i b_{i+k}
\label{eqn:1-5-9}
\end{equation}
Equation (\ref{eqn:1-5-9}) is the
\bx{autocorrelation} formula.
The autocorrelation
value $s_k$ at lag $10$ is $s_{10}$.
It is a measure of the similarity of $b_i$
with itself shifted $10$ units in time.
In the most
frequently occurring case, $b_i$ is real; 
then, by inspection of (\ref{eqn:1-5-9}),
we see that the autocorrelation coefficients are real,
and $s_k=s_{-k}$.
\par
Specializing to a real time series gives
\begin{eqnarray}
S(Z) & = & s_0 + s_1\left(Z+\frac{1}{Z}     \right) +
		 s_2\left(Z^2 +\frac{1}{Z^2}\right) 		\\
S(Z(\omega )) & = & s_0 + s_1(e^{i\omega} + e^{-i\omega}) +
		s_2(e^{i2\omega} + e^{-i2\omega})		\\
S(\omega ) & = & s_0 + 2s_1\cos \omega + 2s_2 \cos 2\omega	\\
S(\omega ) & = & \sum_{k} s_k \cos k\omega
						\label{eqn:1-5-13} \\
S(\omega ) & = & \mbox{cosine transform of }\;\; s_k
\label{eqn:1-5-14}
\end{eqnarray}
This proves a classic theorem that for real-valued signals
can be simply stated as follows:
\par
\boxit{ For any real signal, the cosine transform of the \bx{autocorrelation}
	equals the magnitude squared of the Fourier transform.
	}

\subsection{Two ways to compute a spectrum}
There are two computationally distinct methods 
by which we can
compute a spectrum: (1) compute all the $s_k$ coefficients
from (\ref{eqn:1-5-9}) and
then form the cosine sum (\ref{eqn:1-5-13}) for each $\omega$; and
alternately, (2) evaluate $B(Z)$ for some value of Z on the unit circle,
and multiply the resulting number by its complex conjugate.
Repeat for many values of $Z$ on the unit circle.
When there are more than about twenty lags,
method (2) is cheaper, because
the fast Fourier transform (coming up soon) can be used.

\subsection{Common signals}
\inputdir{autocor}
Figure~\ref{fig:autocor} shows some common signals and their
\bx{autocorrelation}s.
Figure~\ref{fig:spectra} shows the cosine transforms of
the autocorrelations.
Cosine transform takes us from time to frequency and it also takes
us from frequency to time.
Thus, transform pairs in Figure~\ref{fig:spectra}
are sometimes more comprehensible
if you interchange time and frequency.
The various signals are given names in the figures,
and a description of each follows:
\plot{autocor}{width=6in}{
  Common signals and one side of their autocorrelations.
}
% \newslide
\plot{spectra}{width=6in}{
  Autocorrelations and their cosine transforms,
  i.e.,~the (energy) spectra of the common signals.
}
%\newslide
\begin{description}
\item [cos]
The theoretical spectrum of a sinusoid is an impulse,
but the sinusoid was truncated (multiplied by a rectangle function).
The autocorrelation is a sinusoid under a triangle,
and its spectrum is a broadened impulse
(which can be shown to be a narrow sinc-squared function).
\item [sinc]
The \bx{sinc} function is $\sin(\omega_0 t)/(\omega_0 t)$.
Its autocorrelation is another sinc function, and its spectrum
is a rectangle function.
Here the rectangle is  corrupted slightly by
``\bx{Gibbs sidelobes},''
which result from the time truncation of the original sinc.
\item [wide box]
A wide
\bx{rectangle function}
has a wide triangle function for
an autocorrelation and a narrow sinc-squared spectrum.
\item [narrow box]
A narrow rectangle has a wide sinc-squared spectrum.
\item [twin] Two pulses.
\item [2 boxes]
Two separated narrow boxes have the spectrum of one of them,
but this spectrum is modulated (multiplied) by a sinusoidal function
of frequency, where the modulation frequency measures the
time separation of the narrow boxes.
(An oscillation seen in the frequency domain
is sometimes called a ``\bx{quefrency}.'')
\item [comb]
Fine-toothed-\bx{comb}
functions are like rectangle functions with a lower Nyquist frequency.
Coarse-toothed-comb functions have a spectrum which is a fine-toothed comb.
\item [exponential]
The autocorrelation of a transient \bx{exponential} function
is a \bx{double-sided exponential} function.
\sx{exponential ! double-sided}
The spectrum (energy) is a Cauchy function, $1/(\omega^2+\omega_0^2)$.
The curious thing about the
\bx{Cauchy function}
is that the amplitude spectrum
diminishes inversely with frequency to the {\em  first} power;
hence, over an infinite frequency axis, the function has infinite integral.
The sharp edge at the onset of the transient exponential
has much high-frequency energy.
\item [Gauss]
The autocorrelation of a \bx{Gaussian}
function is another Gaussian,
and the spectrum is also a Gaussian.
\item [random]
\bxbx{Random}{random}
numbers have an autocorrelation that is an impulse
surrounded by some short grass.
The spectrum is positive random numbers.
\item [smoothed random]
Smoothed random numbers are much the same as random numbers, 
but their spectral bandwidth is limited.
\end{description}



\section{SETTING UP THE FAST FOURIER TRANSFORM }
Typically we Fourier transform seismograms about a thousand points long.
Under these conditions another Fourier summation method
works about a hundred times faster than those already given.
Unfortunately, the faster Fourier transform program
is not so transparently clear as the programs given earlier.
Also, it is slightly less flexible.
The speedup is so overwhelming, however,
that the fast program is always used in routine work.
\par
Flexibility may be lost because the basic fast program
works with complex-valued signals,
so we ordinarily convert our real signals to complex ones
(by adding a zero imaginary part).
More flexibility is lost because typical fast FT programs require
the data length to be an integral power of 2.
Thus geophysical datasets often
have zeros appended (a process called ``\bx{zero pad}ding")
until the data length is a power of 2.
From time to time I notice clumsy computer code written to deduce
a number that is a power of 2 and is larger than the
length of a dataset.
An answer is found by rounding up the logarithm to base 2.
%The more obvious and the quicker way to get the desired value,
%however, is
%with the simple Fortran function {\tt pad2()}.%
%\progdex{pad2}{round up to power of two}
\par
How fast is the fast Fourier transform method?
The answer depends on the size of the data.
The matrix times vector operation in~(\ref{eqn:3-3})
requires $N^2$ multiplications and additions.
That determines the speed of the slow transform.
For the fast method the number of adds and multiplies
is proportional to $N \,\log_2 N$.
Since $2^{10}=1024$, the speed ratio is typically 1024/10 or about 100.
In reality, the fast method is not quite that fast,
depending on certain details of overhead and implementation.
\par
\begin{comment}
Below is {\tt ftu()}, a version of the \bx{fast Fourier transform} program.
\sx{Fourier transform!fast}
There are many versions of the program---I have chosen
this one for its simplicity.
Considering the complexity of the task,
it is remarkable that no auxiliary memory vectors are required;
indeed, the output vector lies on top of the input vector.
To run this program, your first step might be to copy
your real-valued signal into a complex-valued array.
Then append enough zeros to fill in the remaining space.%
\progdex{ftu}{unitary FT}
\par
The following two lines serve to Fourier transform
a vector of 1024 complex-valued points,
and then to \bx{inverse Fourier transform} them back to the original data:
\sx{Fourier transform!inverse}
\begin{verbatim}
    call ftu(  1., 1024, cx)
    call ftu( -1., 1024, cx)
 \end{verbatim}
\par
\end{comment}
A reference given at the end of this chapter
contains many %other 
versions of the FFT program.
One version transforms real-valued signals to complex-valued
frequency functions in the interval $0 \le \omega < \pi$.
Others that do not transform data on top of itself
may be faster with specialized computer architectures.

\begin{comment}
\subsection{Shifted spectrum}
Subroutine \texttt{simpleft()} \vpageref{lst:simpleft}
sets things up in a convenient manner:
The frequency range runs from minus Nyquist
up to (but not including) plus Nyquist.
Thus there is no problem with the many (but not all)
user programs that have trouble with aliased frequencies.
Subroutine \texttt{ftu()} \vpageref{lst:ftu}, however has a frequency range
from zero to double the Nyquist.
Let us therefore define a friendlier ``front end'' to {\tt ftu()}
which looks more like {\tt simpleft()}.

\par
Recall that a time shift of $t_0$ can be implemented in 
the Fourier domain by multiplication by
$e^{-i\omega t_0}$.
Likewise, in the Fourier domain,
the frequency interval, 
% used by subroutine~\texttt{ftu()} \vpageref{lst:ftu},
namely, $ 0 \le \omega < 2\pi$,
can be shifted to the friendlier interval
$ -\pi \le \omega < \pi$
by a weighting function in the time domain.
That weighting function is $e^{-i\omega_0 t}$
where $\omega_0$ happens to be the Nyquist frequency,
i.e.~alternate points on the time axis are to be multiplied by $-1$.

A subroutine for this purpose is {\tt fth()}.
\progdex{fth}{FT, Hale style}
\newslide
\par
\noindent
To Fourier transform a 1024-point complex vector {\tt cx(1024)}
and then inverse transform it, we would write
\begin{verbatim}
call fth( 0, 1., 1, 1024, cx)
call fth( 1, 1., 1, 1024, cx)
\end{verbatim}
\noindent
You might wonder about the apparent redundancy of using both
the argument {\tt adj} and the argument {\tt sign}.
Having two arguments instead of one allows
us to define the {\em  forward} transform for a {\em  time} axis
with the opposite sign as
             the      forward  transform for a {\em  space} axis.
\par
The subroutine {\tt fth()} is somewhat cluttered by
the inclusion of a
frequently needed practical feature---namely,
the facility to extract vectors from a matrix,
transform the vectors, and then restore them into the matrix.
\end{comment}

\section{SETTING UP 2-D FT}
\begin{comment}
The program {\tt fth()} is set up so that the vectors transformed
can be either rows or columns of a two-dimensional array.
In any computer language there is a way to extract
a vector (column or row) from a matrix.
In some languages the vector can be processed directly without extraction.
To see how this works in \bx{Fortran},
recall a matrix allocated as
{\tt (n1,n2)}
can be subscripted as a matrix
{\tt (i1,i2)}
or as a long vector
{\tt (i1 + n1*(i2-1),1)},
and {\tt call sub(x(i1,i2))} passes the subroutine
a pointer to the {\tt (i1,i2)} element.
To transform an entire axis, the subroutines
{\tt ft1axis()} and
{\tt ft2axis()} are given.
For a two-dimensional FT,
we simply call both
{\tt ft1axis()} and
{\tt ft2axis()} in either order.%
\progdex{ft1axis}{FT 1-axis}
\progdex{ft2axis}{FT 2-axis}
\end{comment}
\par
%I confess that there are faster ways to do things
%than those I have shown you above.
%When we are doing many FTs, for example,
%the overhead calculations done the first time
%should be saved for use on subsequent FTs,
%as in the subroutine {\tt rocca()} included in IEI.
%Further, manufacturers of computers for heavy numerical use
%generally design special FT codes for their architecture.
%Although the basic fast FT used here
%ingeniously stores its output on top of its input,
%that feature is not compatible with vectorizing architectures.

\subsection{Basics of two-dimensional Fourier transform}
\inputdir{plane4}
\par
Let us review some basic facts about
\bxbx{two-dimensional Fourier transform}
    {Fourier transform!two-dimensional}.
A two-dimensional function is represented in a computer as numerical 
values in a matrix, whereas
a one-dimensional Fourier transform in a computer
is an operation on a vector.
A 2-D Fourier transform can be computed by a sequence
of 1-D Fourier transforms.
We can first transform each column vector of the matrix and then
each row vector of the matrix.
Alternately, we can first do the rows and later do the columns.
This is diagrammed as follows:
$$\def\normalbaselines{\baselineskip =\normalbaselineskip
                       \lineskip10pt \lineskiplimit10pt}
  \matrix{
   p(t,\ x) & \smash{\mathop{\longleftrightarrow}}
     & P(t, \ k_x)\cr
   \lineskip10pt
   \Big \updownarrow &               &\Big\updownarrow\cr
   \lineskip10pt
   P(\omega, \ x)& \smash{\mathop{\longleftrightarrow}}
     & P(\omega,\ k_x)\cr
  }
$$
\par
The diagram has the
notational problem that we cannot maintain
the usual convention of using a lower-case letter for the domain
of physical space and an upper-case letter for the Fourier domain,
because that convention cannot include
the mixed objects  $ P(t ,  k_x )$  and  $ P ( \omega , x )$.
Rather than invent some new notation, it seems best to let the reader
rely on the context:
the arguments of the function must help name the function.
\par
An example of
\bxbx{two-dimensional Fourier transform}
    {Fourier transform!two-dimensional}s
on typical deep-ocean
data is shown in Figure~\ref{fig:plane4}.%
\plot{plane4}{width=6in,height=6in}{
  A deep-marine dataset  $p(t , x)$  from Alaska
  (U.S. Geological Survey)
  and the {\em  real} part of various
  Fourier transforms of it.
  Because of the long traveltime through the water,
  the time axis does not begin at  $t=0$.
}
%\newslide
In the deep ocean, sediments are fine-grained and deposit slowly in
flat, regular, horizontal beds.
The lack of permeable rocks such as sandstone severely reduces
the potential for petroleum production from the deep ocean.
The fine-grained shales overlay irregular, igneous, \bx{basement rock}s.
In the plot of  $P(t ,  k_x )$,  the lateral continuity of the
sediments is shown by the strong spectrum at low  $k_x$.
The igneous rocks show a  $k_x$  spectrum
extending to such large  $k_x$  that the deep data may be somewhat
\bxbx{spatially aliased}{spatial alias}
(sampled too coarsely).
The plot of  $P( \omega , x)$  shows that the data contains no
low-frequency energy.
% At large  $\omega$  the energy is not dropping off
% as fast as one might like, which indicates temporal frequency aliasing.
% This aliasing is also apparent in the plot of  $p(t , x)$,
% showing up as the steplike shape of the sea-floor arrival.
The dip of the sea floor shows up in  $( \omega , k_x )$-space as
the energy crossing the origin at an angle.
\par
Altogether, the
\bxbx{two-dimensional Fourier transform}
    {Fourier transform!two-dimensional}
of a collection of seismograms
involves only twice as much computation as the one-dimensional
Fourier transform of each seismogram.
This is lucky.
Let us write some equations to establish that the asserted procedure
does indeed do a 2-D Fourier transform.
Say first that any function of  $x$  and  $t$  may
be expressed as a superposition of sinusoidal functions:
\begin{equation}
p(t , x) \ \ =\ \  \int \int \  e^{ -i \omega t + i k_x x } \ 
P( \omega , k_x ) \  d \omega \  d k_x 
\label{eqn:2.9}
\end{equation}
The double integration can be nested to show
that the temporal transforms are done first (inside):
\begin{eqnarray*}
p(t , x) \ \ &=&\ \  \int \  e^{ i \,k_x x } \  \left[ \int \ 
e^{ -i \omega t } \  P ( \omega , k_x ) \  d \omega \right]\,dk_x
\\
 \ \ &=&\ \ 
\int \  e^{ i \,k_x x } \  P(t ,  k_x ) \  dk_x
\end{eqnarray*}
The quantity in brackets is a Fourier transform over  $\omega$  done
for each and every  $ k_x $.
Alternately, the nesting could be done with the $ k_x $-integral
on the inside.
That would imply rows first instead of columns (or vice versa).
It is the separability of  $\exp (-i \omega t \,+\, i\, k_x x ) $  into a
product of exponentials that makes the computation easy and cheap.
\subsection{Signs in Fourier transforms}
\par
\label{'sign convention'}
In Fourier transforming  $t$-, $x$-, and $z$-coordinates,
we must choose
a sign convention for each coordinate.
Of the two alternative \bx{sign convention}s,
electrical engineers have chosen one and physicists another.
While both have good reasons for their choices,
our circumstances more closely resemble those of physicists,
so we will use their convention.
For the {\em  inverse} Fourier transform, our choice is
\begin{equation}
p(t,x,z) \eq \int \int \int \ e^{ -i \omega t \,+\, ik_x x \,+\, ik_z z}
\ P ( \omega , k_x , k_z ) \ d  \omega \,  dk_x \, dk_z
\label{eqn:6.1}
\end{equation}
For the {\em  forward} Fourier transform,
the space variables carry a {\em  negative} sign, and
time carries a {\em  positive} sign.
\par
Let us see the reasons why electrical engineers have made
the opposite choice,
and why we go with the physicists.
Essentially, engineers transform only the time axis,
whereas physicists transform both time and space axes.
Both are simplifying their lives by their choice
of sign convention,
but physicists complicate their time axis
in order to simplify
their many space axes.
The engineering choice minimizes the number of minus signs
associated with the time axis, because for engineers,
$d/dt$ is associated with $i\omega$ instead of,
as is the case for us and for physicists,
with $-i\omega$.
We confirm this with equation~(\ref{eqn:6.1}).
Physicists and geophysicists deal with many more independent variables
than time.
Besides the obvious three space axes
are their mutual combinations, such as midpoint and offset.
\par
You might ask,
why not make {\em  all} the signs positive in equation~(\ref{eqn:6.1})?
The reason is that in that case
waves would not move in a positive direction along the space axes.
This would be especially unnatural when the space axis was a radius.
Atoms, like geophysical sources,
always radiate from a point to infinity,
not the other way around.
Thus, in equation (\ref{eqn:6.1}) the sign of the spatial frequencies
must be opposite that of the temporal frequency.
\par
The only good reason I know to choose the
engineering convention
is that we might compute with an
array processor built and microcoded by engineers.
Conflict of sign convention is not a problem
for the programs that transform complex-valued 
time functions to complex-valued frequency functions,
because there the sign convention is under the user's control.
But sign conflict does make a difference when we use any program
that converts real-time functions to complex frequency functions.
The way to live in both worlds is to imagine
that the frequencies produced by such a program
do not range from  $0$  to  $+ \pi$  as
the program description says, but from  $0$  to  $- \pi$.
Alternately, we could always take the complex conjugate of the transform,
which would swap the sign of the $\omega$-axis.
\subsection{Simple examples of 2-D FT}
\inputdir{ft2d}
\par
An example of a 
\bxbx{two-dimensional Fourier transform}
    {Fourier transform!two-dimensional}
of a pulse is shown in Figure~\ref{fig:ft2dofpulse}.%
\plot{ft2dofpulse}{width=6in}{
  A broadened pulse (left) and the real part of its FT (right).
}
%\newslide
Notice the location of the pulse.
It is closer to the time axis than the space axis.
This will affect the real part of the FT in a certain way
(see exercises).
Notice the broadening of the pulse.
It was an impulse smoothed over time (vertically) by convolution
with (1,1) and over space (horizontally) with (1,4,6,4,1).
This will affect the real part of the FT in another way.
\par
Another example of a two-dimensional Fourier transform
is given in Figure~\ref{fig:airwave}.
This example simulates an impulsive air wave originating at a point
on the $x$-axis.
We see a wave propagating in each direction
from the location of the source of the wave.
In Fourier space there are also two lines, one for each wave.
Notice that there are other lines which do not go through the origin;
these lines are called ``\bx{spatial alias}es.''
Each actually goes through the origin
of another square plane that is not shown,
but which we can imagine alongside the one shown.
These other planes are periodic replicas of the one shown.%
\plot{airwave}{width=6in}{
  A simulated air wave (left) and the amplitude of its FT (right).
}
%\newslide

%\subsection{Dot-product test of real to complex transforms}
%My previous book, PVI,
%makes the error of mixing up real number pairs with complex numbers.
%The difference became a roadblock
%when Dave Nichols and I were attempting to write a universal program in C++
%for performing dot product tests.
%(In Fortran one generally writes a different test program for each operator).
%\par
%Defining a complex number as a real number pair
%works as you expect for addition, subtraction, and length $\overline{z} z$,
%but products of complex numbers do not behave like
%those of real number pairs.
%To illustrate this, let us represent the operator $\Re$ that
%takes the real part of a complex number by a simple
%$1\times 2$ matrix multiplication
%and then examine the dot-product test for this matrix.
%\begin{equation}
%\left[
%   \begin{array}{c}
%     \tilde r
%     \end{array} \right]
%\eq 
%\left[
%   \begin{array}{cc}
%     1 & 0
%     \end{array} \right]
%\left[
%   \begin{array}{c}
%     x \\
%     y \end{array} \right]
%\end{equation}
%The adjoint operation is
%\begin{equation}
%\left[
%   \begin{array}{c}
%     \tilde x \\
%     \tilde y \end{array} \right]
%\eq
%\left[
%   \begin{array}{c}
%     1 \\
%     0 \end{array} \right]
%\ 
%\left[
%   \begin{array}{c}
%      r
%     \end{array} \right]
%\end{equation}
%
%The dot product test assuming that complex numbers are
%handled as real number pairs is
%\begin{equation}
%\tilde r r \eq
%\left[
%   \begin{array}{cc}
%     \tilde x & \tilde y 
%     \end{array} \right]
%\left[
%   \begin{array}{c}
%     x \\
%     y \end{array} \right]
%\eq
%\tilde x x + \tilde y y
%\EQNLABEL{dotpair}
%\end{equation}
%
%%The dot-product test with complex numbers is
%%\begin{equation}
%%\overline{\tilde z} z \eq
%%(\tilde x - i\tilde y)(x+iy)
%%\eq
%%\tilde x x +
%%\tilde y y + i(
%%\tilde x y -
%%\tilde y x  )
%%\EQNLABEL{dotcomplex}
%%\end{equation}
%
%It seems there are two ways
%to administer the dot-product test
%to a program that inputs real values and outputs complex ones.
%The first is to express inputs and outputs as number pairs
%and interpret the test as equation~\EQN{dotpair}.
%The second is to
%%use equation~\EQN{dotcomplex}
%express all inputs and outputs as complex numbers.
%Then the dot-product test compares two complex numbers, say
%\begin{equation}
%\tilde z_1' z_1 \eq
%\tilde z_2' z_2
%\end{equation}
%Here you have two tests,
%one on the real part
%and one on the imaginary part.
%
%
%%Let $x$ denote a vector of reals and $z$ denote a vector of complex values.
%%Suppose one is derived from the other by what appears to be
%%a linear operator $A$.
%%The question is whether the operator $A$
%%will/can pass the dot product test,
%%and the answer is surprising.
%%Given a random real vector $x$ and a random complex vector $z$,
%%we define two new vectors, $\tilde x$ and $\tilde z$.
%%\begin{eqnarray}
%%\tilde z &=& A x	\\
%%\tilde x &=& A' z
%%\end{eqnarray}
%%where $A'$ is the adjoint of $A$.
%%The dot-product question is whether 
%%\begin{equation}
%%\tilde x'x \eq \tilde z' z \quad ?
%%\end{equation}
%%The answer is no.
%%The left side is necessarily real
%%whereas the right side is necessarily complex.
%%Since $z$ and $\tilde z$ come from different sets of random numbers,
%%there is no reason for the imaginary part $\Im \tilde z' z$ to vanish.
%%The paradox above arises because
%%you cannot make a linear operator $A'$
%%whose function is to truncate an imaginary part of a complex number,
%%or a linear operator $A$ whose function is to append a zero imaginary part
%%to a real number to make a complex number.
%
%\par
%Now for the second surprise.
%Consider real to complex Fourier transform,
%the operator that takes real valued signals and
%converts them to complex valued frequency functions.
%The real part of the frequency function is an even function of frequency,
%and the imaginary part of the frequency function is odd.
%Thus $\Im \tilde z' z$ is a product of an even and an odd function,
%so it is odd.
%The integral of an odd function vanishes,
%and it is this integral that is the imaginary part of the dot product.
%Thus, the real to complex FT operator passes the dot product test.

\begin{exer}
\item
Most time functions are real.
Their imaginary part is zero.
Show that this means that  $F( \omega , k )$  can
be determined from  $F( - \omega , - k )$.
\item
What would change in Figure~\ref{fig:ft2dofpulse}
if the pulse were moved
(a) earlier on the $t$-axis,  and
(b) further on the $x$-axis?
What would change in Figure~\ref{fig:ft2dofpulse}
if instead
the time axis were smoothed with (1,4,6,4,1)
and the space axis with (1,1)?
\item
What would Figure~\ref{fig:airwave}
look like on an earth with half the earth velocity?
\item
Numerically (or theoretically)
compute the two-dimensional spectrum
of a plane wave [$\delta (t-px)$], where
the plane wave has a randomly fluctuating amplitude:
say, rand$(x)$ is a random number between $\pm 1$,
and the randomly modulated plane wave is
[$(1 \ +\ .2\,{\rm rand}(x)) \, \delta (t-px)$].
\item
Explain the horizontal ``layering'' in Figure~\ref{fig:plane4}
in the plot of  $P( \omega  , x)$.
What determines the ``layer'' separation?
What determines the ``layer'' slope?
\end{exer}


\subsection{Magic with 2-D Fourier transforms}
\inputdir{brad}
We have struggled through some technical details
to learn how to perform a 2-D Fourier transformation.
An immediate reward next is a few "magical" results on data.

\par
In this book waves go down into the earth; they reflect;
they come back up; and then they disappear.
In reality after they come back up they reflect
from the earth surface and go back down for another episode.
Such waves, called multiple reflections,
in real life are in some places negligible
while in other places they overwhelm.
Some places these multiply reflected waves can be suppressed
because their RMS velocity tends to be slower
because they spend more time in shallower regions.
In other places this is not so.
We can always think of making an earth model,
using it to predict the multiply reflected waveforms,
and subtracting the multiples from the data.
But a serious pitfall is that we would need to have the earth model
in order to find the earth model.

\par
Fortunately, a little Fourier transform magic goes a long
way towards solving the problem.
Take a shot profile $d(t,x)$.
Fourier transform it to
$D(\omega, k_x)$.
For every $\omega$ and $k_x$, square this value $D(\omega, k_x)^2$.
Inverse Fourier transform.
In
Figure~\ref{fig:brad1}
we inspect the result.
For the squared part the $x$-axis is reversed
to facilitate comparison at zero offset.
A great many reflections on the raw data (right) carry over into the
predicted multiples (left).
If not, they are almost certainly primary reflections.
This data shows more multiples than primaries.

\plot{brad1}{width=6in,height=8.5in}{
  Data (right) with its FT squared (left).
}

\par
Why does this work?  Why does squaring the Fourier Transform of the raw
data give us this good looking estimate of the multiple reflections?
Recall $Z$-transforms $Z=e^{i\omega\Delta t}$.
A $Z$-transform is really a Fourier transform.
Take a signal that is an impulse of amplitude r at time $t=100\Delta t$.
Its $Z$-transform is $r Z^{100}$.
The square of this $Z$-transform is $r^2 Z^{200}$,
just what we expect of a multiple reflection ---
squared amplitude and twice the travel time.
That explains vertically propagating waves.
When a ray has a horizontal component,
an additional copy of the ray doubles the horizontal distance traveled.
Remember what squaring a Fourier transformation does -- a convolution.
Here the convolution is over both $t$ and $x$.
Every bit of the echo upon reaching the earth surface
turns around and pretends it is a new little shot.
Mathematically, every point in the upcoming wave $d(t,x)$ launches a replica
of $d(t,x)$ shifted in both time and space -- an autoconvolution.

\par
In reality, multiple reflections offer a considerable number
of challenges that I'm not mentioning.
The point here is just that FT is a good
tool to have.

\subsection{Passive seismology}

\par
Signals go on and on, practically forever.
Sometimes we like to limit our attention to something more limited such
as their spectrum, or equivalently, their autocorrelation.
We can compute the autocorrelation in the Fourier domain.
We multiply the FT times its complex conjugate
$D(\omega, k_x) \overline{D(\omega, k_x)}$.
Transforming back to the physical domain we see Figure~\ref{fig:brad2}.
We expect a giant burst at zero offset (upper right corner).
We do not see it because it is "clipped",
i.e. plot values above some threshhold are plotted at that threshhold.
I could scale the plot to see the zero-offset  burst,
but then the interesting signals shown here would be too weak to be seen.
\par
\plot{brad2}{width=6in,height=8.5in}{
  The 2-D autocorrelation of a shot profile resembles itself.
}


\par
Figure~\ref{fig:brad2} shows us that the 2-D autocorrelation
of a shot profile shares a lot in common with the shot profile itself.
This is interesting news.
If we had a better understanding of this
we might find some productive applications.
We might find a situation where we do not have (or do not want)
the data itself but we do wish to build an earth model.
For example, suppose we have permanently emplaced geophones.
The earth is constantly excited by seismic noise.
Some of it is man made;  some results from earthquakes
elsewhere in the world; most probably results from natural
sources such as ocean waves, wind in trees, etc.
Recall every bit of acoustic energy that arrives at the surface from below
becomes a little bit of a source for a second reflection seismic experiment.
So, by autocorrelating the data of hours and days duration
we convert the chaos of continuing microseismic noise
to something that might be the impulse response of the earth,
or something like it.
Autocorrelation converts a time axis of length of days to one of seconds.
From the autocorrelation  we might be able to draw conclusions in usual ways,
alternately,
we might learn how to make earth models from autocorrelations.
\par
Notice from Figure~\ref{fig:brad2}
that since the first two seconds of the signal vanishes
(travel time to ocean bottom),
the last two seconds of the autocorrelation must vanish
(longest nonzero lag on the data).
\par

%The 2-D autocorrelation of 2-D random noise is an impulse
%at the origin.
%All data has random noise in it; and this data has plenty.
%The huge burst in the neighborhood of the origin
%of the autocorrelation
%is not visible because it
%is simply clipped away by the plotting software.

\par
There are many issues on
Figure~\ref{fig:brad2} to intrigue an interpreter
(starting with signal polarity).
We also notice that the multiples on the autocorrelation
die off rapidly with increasing offset and wonder why,
and whether the same is true of primaries.
But today is not the day to start down these paths.

\par
In principal an autocorrelation is not comparable to the raw data
or to the ideal shot profile because
forming a spectrum squares amplitudes.
We can overcome this difficulty by use of
multidimensional spectral factorization ---
but that's an advanced mathematical concept
not defined in this book.
See my other book, Image Estimation.





\section{THE HALF-ORDER DERIVATIVE WAVEFORM}

%First we will review the calculus of step functions
%and power functions and their Fourier transforms
%leading up to the half-order differential operator.
%With this background we can answer the question,
%what is the difference between a line
%and the sequence of short dashes that make up the line?
%Likewise, what is the difference between a plane
%and a dense array of tiny patches that make up the plane?
%Is each tiny patch the same as a point source?
%These questions will be answered from the viewpoint
%of wave propagation where it is conceptually and computationally
%convenient to make plane waves by superposing an array
%of spherical waves.
%We will find that a plane wave with an impulsive waveform
%can be made from a superposition of circles (or hyperbolas)
%or spheres (or hyperboloids of revolution)
%but that in three dimensions the hyperboloid carries
%not an impulsive waveform,
%but the time derivative $d/dt$,
%and in two dimensions the hyperbola carries the half-order derivative waveform.
%These waveforms are generally clear on synthetic data
%but on field data
%they are usually obscured by waveforms from other sources.
%
%\subsection{Fractional order operators}

\par
Causal integration
is represented in the time domain
by convolution with a step function.
In the frequency domain this amounts to multiplication by $1/(-i\omega)$.
(There is also delta function behavior at $\omega=0$
which may be ignored in practice and since
at $\omega=0$, wave theory reduces to potential theory).
Integrating twice amounts to convolution by a ramp function,
$t\, {\rm step}(t)$, which in the Fourier domain is multiplication by
$1/(-i\omega)^2$.
Integrating a third time is convolution with
$t^2\, {\rm step}(t)$ which in the Fourier domain is multiplication by
$1/(-i\omega)^3$.
In general
\def\eq{\quad =\quad}
\begin{equation}
\label{eqn:iterint}
t^{n-1}\ {\rm step}(t) \eq {\rm FT}\ \left( { 1 \over (-i\omega)^n} \right)
\end{equation}
Proof of the validity of equation~(\ref{eqn:iterint}) for integer values of $n$
is by repeated indefinite integration which also indicates
the need of an $n!$ scaling factor.
Proof of the validity of equation~(\ref{eqn:iterint}) for fractional values of $n$
would take us far afield mathematically.
Fractional values of $n$, however,
are exactly what we need to interpret Huygen's secondary wave sources in 2-D.
The factorial function of $n$ in the scaling factor becomes a gamma function.
The poles suggest that a more thorough mathematical study of convergence
is warranted, but this is not the place for it.
\par
%We will see that the
The principal artifact
of the hyperbola-sum method of 2-D migration is the waveform
represented by equation~(\ref{eqn:iterint}) when $n=1/2$.
For $n=1/2$, ignoring the scale factor,
equation~(\ref{eqn:iterint}) becomes
\begin{equation}
\label{eqn:halfint}
{1\over \sqrt{t}} \ {\rm step}(t) \eq
{\rm FT}\ \left( { 1 \over \sqrt{-i\omega}} \right)
\end{equation}
A waveform that should come out to be an impulse
actually comes out to be equation~(\ref{eqn:halfint}) because Kirchhoff
migration needs a little more than summing or spreading on a hyperbola.
To compensate for the erroneous filter response of equation~(\ref{eqn:halfint})
we need its inverse filter.
We need $\sqrt{-i\omega}$.
To see what $\sqrt{-i\omega}$ is in the time domain,
we first recall that
\begin{equation}
\label{eqn:derivative}
{d \ \over dt} \eq
{\rm FT}\ \left(  -i\omega \right)
\end{equation}
A product in the frequency domain corresponds
to a convolution in the time domain.
A time derivative is like convolution with a doublet $(1,-1)/\Delta t$.
Thus, from
equation~(\ref{eqn:halfint}) and 
equation~(\ref{eqn:derivative})
we obtain
\begin{equation}
\label{eqn:huygens}
{d \ \over dt} \ {1\over \sqrt{t}} \ {\rm step}(t) \eq
{\rm FT}\ \left(  \sqrt{-i\omega} \,\right) 
\end{equation}
Thus, we will see the way to overcome
the principal artifact of hyperbola summation
is to apply the filter of equation~(\ref{eqn:huygens}).
In chapter~\ref{paper:dwnc}
we will learn more exact methods of migration.
There we will observe that an impulse in the earth
creates not a hyperbola with an impulsive waveform
but in two dimensions,
a hyperbola with the waveform of
equation~(\ref{eqn:huygens}),
and in three dimensions,
a hyperbola of revolution (umbrella?)
carrying a time-derivative waveform.

\subsection{Hankel tail}
\inputdir{hankel}
\par
The waveform in equation~(\ref{eqn:huygens}) often arises in practice
(as the 2-D Huygens wavelet).
Because of the discontinuities on the left side of equation~(\ref{eqn:huygens}),
it is not easy to visualize.
Thinking again of the time derivative
as a convolution with the doublet $(1,-1)/\Delta t$,
we imagine the 2-D Huygen's wavelet as a positive impulse followed
by negative signal decaying as $-t^{-3/2}$.
This decaying signal is sometimes called the ``\bx{Hankel tail}.''
In the frequency domain
$-i\omega= |\omega |e ^ {-i90^\circ}$
has a 90 degree phase angle and 
$\sqrt{-i\omega}= |\omega |^{1/2} e ^ {-i45^\circ}$
has a \bx{45 degree phase angle}.

\opdex{halfint}{half derivative}{58}{74}{api/c}
%\newslide

\par
In practice, it is easiest to represent
and to apply the 2-D Huygen's wavelet in the frequency domain.
Subroutine \texttt{halfint()} \vpageref{lst:halfint} is provided for that purpose.
Instead of using $\sqrt{-i\omega}$ which
has a discontinuity at the Nyquist frequency
and a noncausal time function,
I use the square root of a causal representation
of a finite difference, i.e.~$\sqrt{1-Z}$,
which is well behaved at the Nyquist frequency
and has the advantage that the modeling operator is causal
(vanishes when $t<t_0$).
% Fourier transform is done using subroutine \texttt{ftu()} \vpageref{lst:ftu}.
Passing an impulse function into subroutine {\tt halfint()}
gives the response seen in Figure~\ref{fig:hankel}.
\sideplot{hankel}{width=2.0in}{
  Impulse response (delayed) of finite difference operator of half order.
  Twice applying this filter is equivalent to once applying $(1,-1)$.
}

\begin{comment}
\subsection{Huygen's secondary source}

\par
\bx{Huygen's secondary source} is the concept
that a plane wave
can be regarded as broken into many point sources.
Each point makes a circular wave in $(x,z)$-space
or equivalently a hyperbola in $(x,t)$-space.
The hyperbolas shown in Figure~\ref{fig:hyplay}
are all tangent to the horizontal line.
Observe that the line density on the plot is great
near the line but weaker away from it.
We begin analysis from the assumption that
each hyperbola carries a simple impulse function of time.
We will add these hyperbolas together
and discover that the sum is a plane wave,
but the waveform is not an impulse function of time,
but another function with a tail in the time domain and
an $\omega^{-1/2}$ amplitude spectrum.
From this we conclude that if the original waveform
on the hyperbola
had an $\omega^{1/2}$ amplitude spectrum
(instead of being an impulse with a constant spectrum)
then we would have an improved
representation of the plane wave as a sum of points.
The phase of this $\sqrt{\omega}$
spectrum should be such that the final waveform is causal
as it appears in Figure~\ref{fig:hankel},
indicating the validity of representation
of the numerical representation of $\sqrt{-i\omega}$
found in subroutine \texttt{halfint()} \vpageref{lst:halfint}.
More theoretical details are found in IEI.

\subsection{Huygen's secondary source in two dimensions}
First we calculate the density of curves
as a function of time.
The density goes to infinity as the hyperbola separation
$\Delta x $ tends to zero
in Figure~\ref{fig:hyplay}.
Then the hyperbolas sum to a plane wave
which carries the waveform that we want to know.
\activesideplot{hyplay}{width=2.0in}{ER}{
	Many hyperbolas tangent to a line.
	}

\par
When a hyperbolic event carries a wavelet,
the wavelet covers an area in the $(t,x)$-plane.
Define this area as that between the two hyperbolas
shown in Figure~\ref{fig:hyparea}.
These two curves, $x_1(t)$ and $x_2(t)$ are
\begin{eqnarray}
t &=& v^{-1}\ \sqrt{z^2+x_1^2}		\\
t &=& v^{-1}\ \sqrt{z^2+x_2^2} +\Delta t
\end{eqnarray}

\activesideplot{hyparea}{width=2.0in}{ER}{
	Detailed view of one of the many hyperbolas
	in figure~\protect\ref{fig:hyplay}.
	Two hyperbolas are separated by $\Delta t$.
	An impulsive signal is defined as one that
	is zero everywhere but between the two hyperbolas
	where its amplitude is $1/\Delta t$.
	}

\par
In the limit $\Delta t \rightarrow 0$,
the waveform is an impulse function
whose temporal spectrum is constant.
In that limit, the many waveforms in Figure~\ref{fig:hyplay}
superpose giving a great concentration at the apex.
Since the many hyperbolas are identical with one another,
the time dependence of their sum is (within a scale factor)
the same time dependence of the shaded area of the single hyperbola
in Figure~\ref{fig:hyparea}.

\par
The separation between the two curves, $\Delta x$ is
\begin{equation}
\Delta x \eq 
x(t_1) - x(t_2) \eq
   \sqrt{ t          ^2v^2-z^2}  \ -\ 
   \sqrt{(t-\Delta t)^2v^2-z^2}
\end{equation}
The area in Figure~\ref{fig:hyparea} is a length, integrated over time.
Between the two hyperbolas of separation $\Delta t$,
we take a constant signal of strength $1/\Delta t$ which
adds up to an amplitude at time $t$ of $A_{\rm hyp}(t)$.
\begin{equation}
A_{\rm hyp}(t) \eq
{\Delta x \over \Delta t} \eq
{d \over dt} \ \sqrt{t^2 v^2-z^2} 
\eq
{t v^2\over \sqrt{t^2 v^2-z^2} }
\quad \quad {\rm for} ~ t>z/v
\label{eqn:Shyp}
\end{equation}
To avoid much clutter that belongs in a mathematics book
rather than in a seismology book,
we concentrate our attention on the discontinuity itself.
Mathematically, this amounts to ignoring all but the highest frequencies.
Seismologically, we say that
after a signal like equation~(\ref{eqn:Shyp}) passes through the filters
and gain control processes typical of data collection,
the numerator $t$ does not warrant comment,
but the power of $t$ at the pole itself is important.

\par
By making this high frequency approximation,
we are setting aside some calculations
that we should be able to do for a constant velocity medium
but which would not apply to a medium with velocity as a function of depth.
Divergence considerations, for example,
suggest that the amplitude of each hyperbola should not be
a constant, but should drop off proportional to $t^{-1/2}$.
Another example is that of parabolas instead
of hyperbolas, i.e.~$x(t)=\sqrt{t-t_0}$.
(When velocity changes with depth, we no longer have hyperbolas,
but the tops of the traveltime curve could be approximated by a parabola.)
Then the amplitude is $A_{\rm par}(t)= 1/\sqrt{t-t_0}$.
Notice that the divisor in equation~(\ref{eqn:Shyp}) for the hyperbola
$\sqrt{t^2 v^2-z^2}=\sqrt{tv-z}\sqrt{tv+z}$
has the same half power discontinuity at $z=tv$ as the parabola.
Thus the amplitude spectrum at high frequencies for either
the parabola or the hyperbola will drop off as the inverse
half power of frequency according to equation~(\ref{eqn:halfint}).
To get a plane wave with the constant spectrum of an impulse
instead of this inverse-square-root spectrum,
we must use hyperbolas (or parabolas),
not carrying an impulse function,
but instead carrying a square-root spectrum
(to cancel the {\em  inverse}-square-root spectrum that arises
from superposing the hyperbolas.)
Equation~(\ref{eqn:huygens}) gives us a causal waveform with the desired spectrum.
Thus the decomposition of an impulsive 2-D plane
wave into hyperbolas requires the hyperbolas to carry
the ``half-order differential'' waveform given in equation~(\ref{eqn:huygens}).

\subsection{Three dimensions}
Interestingly,
the Huygen's wavelet is different in \bx{three dimensions}
than in two dimensions.
In three dimensions, there is no Hankel tail.
In three dimensions we have the definition
$x^2+y^2=r^2$ and
from $t^2v^2=x^2+y^2+z^2$ we find on a plane of constant $z$,
that the equation for a circle expanding with time is
$r=\sqrt{t^2v^2-z^2}$.
Between time $t$ and $t+\Delta t$ is a ring with an area
$2\pi r \Delta r$.
Taking the signal amplitude in the ring to be $1/\Delta t$,
analogous to equation~(\ref{eqn:Shyp}) the amplitude at time $t$ is
\begin{eqnarray}
A(t) &=&  {\rm step}(t-z/v) \ 
	2\pi r \ {\Delta r \over \Delta t }				\\
A(t) &=&  {\rm step}(t-z/v)  \ 
	2\pi \sqrt{t^2v^2-z^2}\ {d~\over dt}\ \sqrt{t^2v^2-z^2}		\\
A(t) &=&  {\rm step}(t-z/v) \ 2\pi v^2 \ t 
\label{eqn:tstep}
\end{eqnarray}
As before, in seismology
we are interested in the high frequency behavior
so the scaling $t$ in equation~(\ref{eqn:tstep})
is not nearly so important as is the step function.
By equation~(\ref{eqn:iterint})
the step function causes the spectrum to decay as $\omega^{-1}$.
Our original erroneous assumption
that Huygen's hyperbola of revolution
should carry a positive impulse
leads to the contradiction
that an impulsive plane wave
decomposed into Huygen's sources and added together again
does not preserve the constant spectrum
of the original impulsive waveform.
We can get the missing $\omega$ back into the spectrum
by having the hyperbola of revolution carry
a time-derivative filter instead of an impulse.
Thus in three dimensions,
a plane wave can be regarded, approximately,
as the superposition of many hyperboloidal responses,
each carrying a $d/dt$ waveform.

\subsection{Amplitude versus offset on Huygen's wave}
\sx{amplitude}
\sx{AVO}
The analysis above did not show that in two dimensions
the hyperbolas should carry the scaling factor $x/t^{3/2}$
and in three dimensions the scaling factor $x/t^2$.
These scaling factors can be explained
as combination of two parts.
The first part of the scaling factor is
a divergence correction due
to expansion of the wavefront.
The second part of the scaling factor is
a cosine function $(\tau/t)$ called the obliquity function
which is explained as the difference between a point source
and a source like a short line or patch which has the
ability to radiate preferentially along the direction
of the maximum of the cosine.
\par
You might fear that analysis of weights and waveshapes
would be a big chore in stratified media $v(z)$.
Luckily it is all made easy by Fourier domain
methods in chapters~\ref{paper:ft1} and~\ref{paper:dwnc}.

\begin{exer}
\item	If we input a one layer model $\delta(t-t_0)\,{\rm const}(x)$
	to subroutine \texttt{kirchslow()} \vpageref{lst:kirchslow},
	we find an impulse function followed by a
	${\rm step}(t-t_0)/\sqrt{t}$ waveform.
	What data would result from
	a 3-D version of {\tt kirchslow()}
	on the model $\delta(t-t_0)\,{\rm const}(x,y)$?
\item   At the bottom of a well (deep hole in the ground) is an explosion.
	An air wave propagates up the well and when emerging at the surface
	it becomes a spherical wave.
	At the mouth of the well
	$(x,y,z)=(0,0,0)$ an air wave time function $f(t)$ is observed.
	The speed of sound in air is a constant 340 m/s.
	\par
	Write an equation
	for the air wave
	expanding spherically from the top of the well.
	Assume wavelengths and observation distances are 
	are large compared to the well diameter.
	Be sure your equation carries the correct time-dependent waveform.
\item
	Given the signal ${\rm step}(t)/\sqrt{t}$
	\begin{enumerate}
	\item What is its {\em  energy} spectrum?
	\item Would you say the spectral color is red, white, or blue?
	\end{enumerate}
\end{exer}

\end{comment}

% end of HideThis

\section{References}
\reference{
	Special issue on fast Fourier transform, June 1969:
	IEEE Trans.~on Audio and Electroacoustics
	(now known as IEEE Trans.~on Acoustics, Speech,
	and Signal Processing), {\bf AU-17}, entire issue (66-172).
	}

%\newpage
%\showiex
%\par
%\iex{Exer/Inter2}{inter2}
%\iex{Exer/ed1D}{Lab2}
%\newpage










































