% Would plot files be smaller and faster with #define HARDCOPY ?

\def\AACAKE{all}	% careful here, in case I split or rename directory.

\title{Antialiased hyperbolas}
\author{Jon Claerbout}
\maketitle
\label{paper:trimo}
\inputdir{all}

\par
A most universal practical problem in geophysics
is that we never have enough recordings.
This leads to the danger of spatial aliasing of data.
There is no universal cure for this problem
(although there are some specialized techniques of limited validity).
A related, but less severe problem
arises with Kirchhoff type operators.
This problem is called ``operator-aliasing''.
It has a cure, which we investigate in this chapter.

\par
Fourier and finite-difference methods of migration
are immune to the operator-aliasing malady suffered
by hyperbola summation (Kirchhoff) migration.
Here we will see a way to overcome the operator-aliasing malady
shared by all Kirchhoff-like operators
and bring them up to the quality of phase-shift methods.
The antialiasing methods we develop here also lead
to natural ways of handling irregularly sampled data.

\par
\sx{alias}
We like to imagine that our data is a continuum
and that our sums are like integrals.
For practical purposes, our data is adequately sampled in time,
but often it is not adequately sampled in space.
Sometimes the data is sampled adequately in space,
but our operators, such as hyperbolic integrations,
are not adequately represented by a summation
ranging over the $x$-coordinate
picking a value at the nearest time $t(x)$.
First we could improve nearest-neighbor interpolation
by using linear interpolation.
Linear interpolation, however, is not enough.
Trouble arises
when we jump from one trace to the next,
$x \rightarrow x+\Delta x$,
and find that $t(x)$ jumps more than a single $\Delta t$.
Then we need a bigger ``footprint'' on the time axis
than the two neighboring points used by linear interpolation.
See Figure~\ref{fig:nmotraj}.
Note that in some places each value of $x$ corresponds
to several values of $t$,
and other places it is the opposite where one value of $t$
corresponds to several values of $x$.
An aliasing problem arises
when we approximate a line integral
by a simple sum of points,
one for each value on the $x$-axis
instead of using the more complicated
trajectory that you see in Figure~\ref{fig:nmotraj}.
%
\sideplot{nmotraj}{width=2.0in}{
	To integrate along hyperbolas without aliasing,
	you should include (at least) the points shown.
	} 

%	Field arrays filter spatially.
%	On dipping arrivals this produces a temporal filtering effect.
%	To match field arrays,
%	Kirchhoff operators are designed to antialias with short rectangle
%	and triangle smoothing functions
%	whose duration depends on the local trace-to-trace moveout.
%	Integrals under triangular shaped weighting windows
%	are rapidly computed from the double integral of the trace.
%	Each integral is a weighted sum of three values of the double integral,
%	one from each corner of the triangle.

\subsection{Amplitude pitfall}
In geophysics we often discuss signal amplitude versus offset distance.
It sounds easy, but there are some serious pitfalls.
Such pitfalls are one reason why mathematicians
often use nonintuitive weasel words.
The best way for you to appreciate the pitfall
is for me to push you into the pit.

\par
Suppose we are writing a seismogram modeling program
and we wish to model an impulsive plane wave of unit amplitude.
Say the signal seen at $x$ is 
$(\cdots,0, 0,1,0,0, \cdots )$.
At $x+\Delta x$ the plane wave is shifted in time
so that the impulse lies half way between two points,
say it is
$(\cdots,0, 0,a,a,0,0, \cdots )$.
The question is, ``what should be the value of $a$?''
There are three contradictory points of view:
\begin{enumerate}
\item The amplitude $a$ should be 1 so that the peak amplitude
	is constant with $x$.
\item The amplitude $a$ should be $1/\sqrt{2}$ so
	that both seismic signals have the same energy.
\item The amplitude $a$ should be $1/2$ so
	that both seismic signals have the same area.
\end{enumerate}
Make your choice before reading further.
\par
What is important
in the signal
is not the high frequencies
especially those near the Nyquist.
We cannot model the continuous universe with sampled data
at frequencies above the Nyquist frequency
nor can we do it well or easily
at frequencies approaching the Nyquist.
For example, at half the Nyquist frequency,
a derivative is quite different from a finite difference.
What we must try to handle correctly is the low frequencies
(the adequately sampled signals).
The above three points of view are contradictory at low frequencies.
Examine only the zero frequency of each.  Sum over time.
Only by choosing equal areas $a=1/2$ do the two signals have equal strength.
The appropriate definition of amplitude on
a sampled representation of the continuum is the {\em  area per unit time.}
Think of each signal value as representing the integral of
the continuous amplitude from
$t-\Delta t/2$ to
$t+\Delta t/2$.
Amplitude defined in this way cannot be confounded by
functions oscillating between the sampled values.
\par
Consider the task of abandoning data:
We must reduce data sampled at a two millisecond rate to data
sampled at a four millisecond rate.
A method with aliasing is to
abandon alternate points.
A method with reasonably effective antialiasing
is to convolve with the rectangle $(1,1)$
(add two neighboring values)
and then abandon alternate values.
Without the antialiasing,
you could lose the impulse on the
$(\cdots,0, 0,1,0,0, \cdots )$ signal.
A method with no aliasing is to multiply in the frequency domain
by a rectangle function between $\pm$ Nyquist/2
(equivalent to convolving with a sinc function)
and then abandoning alternate data points.
This method perfectly preserves all frequencies up to the
new Nyquist frequency (which is half the original).

\section{MIMICING FIELD ARRAY ANTIALIASING}
\sx{field array}
In geophysical data recording there is usually
a local array whose elements are added locally
before a single channel is recorded.
For example, the SEP student group once laid out more than 4056 geophones
in a two-dimensional array of $13\times 13$ recorders
with 24 geophones added at each recorder.
We may think of the local superposition
as an integration over a small interval of space
to create a sampled space function from a continuous one.
With vibrator sources, it is also customary to vibrate on
various nearby source locations and sum them into a single signal.
Figure~\ref{fig:oversamp} is a caricature of what happens.
On the left a data field appears
to be a continuous function of space
(it is actually 500 spatial locations)
with various impulsive signals at different times and distances. %
\plot{oversamp}{width=6.00in,height=2.25in}{
	Quasicontinuous field (left) added in groups (right).
	} %
For simplicity, all signals have unit amplitude.
The 500 signals are segregated into 10 groups of 50
and each group of 50 is summed into a single channel.
The various signals sum to functions
that could be called ``slump shouldered rectangles.''
If both $x$ and $t$-meshes were refined further,
the ``slump shoulders'' on the rectangles would diminish
in importance and we would notice that the rectangles were still imperfect.
This is because the rectangle approximation
arises from the approximation that
the hyperbola is a straight line within the group.
In reality, there is curvature
and the effect of curvature is strongest near the apex,
so the rectangle approximation is poor at the apex.

\par
Some of the rectangles are longer than others.
The narrow ones are tall and the wide ones are short
because the area of each rectangle must be 50
(being the sum of 50 channels each holding a 1).
Since the rectangles all have the same area,
were we to lowpass filter the sparse data
we would recover the original characteristic
that all these signals have the same amplitude.


\par
\inputdir{subsamp}
Figure \ref{fig:subsampvrsaa} shows a quasisinusoidal signal
and compares subsampling to antialiasing
via field arrays as in
Figure \ref{fig:oversamp}.
\plot{subsampvrsaa}{width=6.00in,height=2.25in}{
	500 channels (left),
	subsampled to 20 (middle),
	added in groups of 25(right).
	} 
We see that aliased energy has been surpressed but not removed.
Let us see how we can understand the result
and how we could do better (but we won't).
Suppose that the 500 channels had been individually recorded.
The right panel in
Figure~\ref{fig:subsampvrsaa}
was computed simply by adding in groups of 25.
A lengthier explanation of the calculation
is that the 500 channels were convolved along
the horizontal $x$-axis with a 25 point long rectangle function.
Then the 500 channel output was subsampled to 20 channels.
This lengthier calculation gives the same result
but has a simple Fourier explanation:
Convolving with a rectangle function of $x$
is the Fourier equivalent to multiplying by a sinc function
$\sin(k_x \Delta x) /(k_x \Delta x)$
in the Fourier domain.
We have convolved with a rectangle in the physical domain
which amounts to multiplication by a sinc function in the Fourier domain.
Theoretically we would prefer to have done it the other way around,
convolved with a sinc in the physical domain, equivalently
multiplying with a rectangle in the Fourier domain.
The Fourier rectangle would drop to zero at half Nyquist
and thus subsampling would not fold back any energy from
above the half Nyquist to below it.
Although Figure~\ref{fig:subsampvrsaa} shows that the aliased
information is strongly suppressed, you can see that it
has not been eliminated.
Had we instead convolved with a sinc on the $x$-axis,
the Fourier function would have been a rectangle.
You would see the wavefronts in
Figure~\ref{fig:subsampvrsaa} (right panel)
vanishing where the dip reached a critical threshhold
instead of seeing the wavefronts gradually tapering off
and weak aliased events still being visible.


\subsection{Adjoint of data acquisition}
\inputdir{mod}
Knowing how data is recorded,
or how we would like it to be recorded,
suggests various possibilities for data processing.
Should we ignore the little rectangle functions,
or should we include them in the data processing?
Figure~\ref{fig:migalias} shows a simple model and its implied data,
along with migrations,
with and without attention to aliasing the horizontal space axis.
The figure shows that migration without attention to aliasing
leads to systematic noise and (apparently) random noise.
\plot{migalias}{width=4.30in}{
  Top left is a synthetic image.
  Top right is synthetic data from the synthetic image.
  Bottom are migrations of the data with and without
  antialiasing.
}
\par
This figure is based on realistic parameters
except that I compute and display the results
on a very coarse mesh ($20\times 100$)
to enable you to see clearly the phenomena of numerical analysis.
No additional values were used between mesh points
or off the edges of what is shown.

\par
The practical need to limit operator aliasing
is often reduced by three indirect measures.
First is temporal low pass filtering
which has the unfortunate side effect
of reducing the temporal bandwidth.
Second is dip limiting (limiting the aperture of the hyperbola)
which has the unfortunate side effect of limiting the dip bandwidth.
Third is interlacing the data traces.
Interpolating the data also interpolates the operator
so if enough trace interpolation is done, the operator is no longer subsampled.
A disadvantage of data interpolation is that the data becomes more bulky.
Here we attack the operator aliasing problem directly.

\inputdir{all}

\par
A simple program designed for antialiasing
gave the result in Figure~\ref{fig:boxmo1}.
A zero-offset signal is input to adjoint NMO to make
synthetic data which is then NMO'ed and stacked.
Notice that the end of each rectangle
is the beginning of the rectangle at the next offset. %
\sideplot{boxmo1}{width=3.00in,height=4in}{
	Rectangle smoothing during NMO and stacking.
	Notice that the end of one rectangle exactly coincides
	with the beginning of the rectangle at next larger offset.
	Thus, rectangle width increases with offset and decreases with time.
	({\tt antialias=1.})
	} %
You might fear the coding that led up to Figure~\ref{fig:boxmo1}
is a fussy and inefficient business
because of all the short little summation loops.
Luckily, there is a marvelous little formula that allows
us to express the integral under any of the little rectangles,
no matter how many points it contains,
by a single subtraction.
Integration is the key.
It is only necessary to realize that the sums are,
like a definite integral,
representable by the difference of the indefinite integral at each end.
In other words, to find the sum of all the values between
{\tt it} and {\tt it+n} we begin with
a recursive summation such as {\tt qq(it)=qq(it-1)+pp(it)}.
Then, any sum of values like {\tt pp(it)+$\cdots $+p(it+n)} is simply
{\tt qq(it+n+1) - qq(it) }.

\par
Figure~\ref{fig:boxmo1} is not fully consistent with Figure~\ref{fig:nmotraj}.
In Figure~\ref{fig:boxmo1} notice that the last point in each rectangular area
overlaps the next rectangular area by one point.
Overlap could be avoided by shortening each rectangle by one point,
but then rectangles near the apex of the hyperbola
would have {\em  zero length} which is wholly unacceptable.
Should we write a code to match Figure~\ref{fig:nmotraj}?
This would be better, but far from perfect.
Notice in Figure~\ref{fig:nmotraj} that a horizontal sum of the number of boxes
is not a smooth function of time.
To achieve more smoothness, we later turn to triangles,
but first we look at some implementation details for rectangles.

\subsection{NMO and stack with a rectangle footprint}
\sx{antialias!stack}
\sx{stack, antialias}

\par

A subroutine for causal summation is subroutine \texttt{causint()} \vpageref{lst:causint}.
Recall that the
adjoint of causal integration is anticausal integration.
For each reflector,
data modeling proceeds by throwing out two pulses of opposite polarity.
Then causal summation produces a rectangle between the pulses
(sometimes called ``box car'').
Since the last step in the modeling operator
is causal summation,
the first step in the adjoint operator (which does NMO) is
anticausal summation.
Thus each impulse in the data becomes a rectangle from the impulse to $t=0$.
Then subtracting values at rectangle ends gives the desired integral of data
in the rectangle.

\begin{comment}
The code is in subroutines {\tt boxmo()}
and {\tt boxstack()}.
The traveltime depth $\tau$ is denoted by {\tt z} in the code.
The inverse of the earth velocity $v(\tau)$,
called the slowness $s(\tau )$, is denoted by {\tt slow(iz)}.
\progdex{boxmo}{box footprint}

\par
To find the end points of the rectangular intervals,
given the vertical travel time,
I get the time {\tt t}, in the usual way.
Likewise I get the time, {\tt tp}, on the next further-out trace
for the ending location of the rectangle wavelet.
I introduce a parameter called {\tt antialias} that
can be used to increase or decrease the {\tt tp-t} gap.
Normally {\tt antialias=1.}

\par
Theoretical solutions to various problems lead to various
expressions for amplitude along the hyperbola.
I set the amplitude {\tt amp} by a complicated expression
that I do not defend or explain fully here but merely indicate that:
a ``divergence'' correction is in the factor $1/\sqrt{t}$;
a cosine like ``obliquity'' scale is $z/t$;
and the wavelet area must be conserved, so the height is
inversely proportional to the pulse width {\tt (itp - it)}.
Wavelet area is conserved to assure that after low-pass filtering,
the strength of a wave is independent of whether it straddled
two mesh points as $(.5,.5)$ or lay wholly on one of them as $(1,0)$.
\end{comment}

\par
To test a limiting case,
I set the antialias parameter to zero
and show the result in Figure~\ref{fig:boxmo0}
which is the same as the simple prescription to ``sum over the $x$-axis.''
We notice that the final stack is not the perfect impulses that we began with.
The explanation is:
information can be expanded in time and then compressed with no loss,
but here it is compressed first and then expanded,
so the original location is smeared.
Notice also that
the full amplitude is not recovered on the latest event.
The explanation is that a significant fraction of the angular
aperture has been truncated at the widest offset.

\sideplot{boxmo0}{width=3.00in}{
	Rectangles shortened to one point duration.
	({\tt antialias=0.})
	}

\subsection{Coding a triangle footprint}
\sx{antialias!stack}
\sx{triangle footprint}
\par
We should take some care with anti-aliasing in data processing.
The anti-aliasing measures we take, however,
need not match the field recording.
If the field arrays were rectangles,
we could use triangles or sincs in the data processing.
It happens that triangles are an easy extension
of the rectangle work that we have been doing
and triangles make a big step in the right direction.

\par
For an input pulse,
the output of integration is a step.
The output of a second integration is a ramp.
For an input triplet $(1, 0, 0, -2, 0, 0, 1)$
the output of two integrations is a short triangle.
An easy way to assure time alignment
of the triangle center with
the triplet center is to integrate
once causally and once anticausally
as done in subroutine \texttt{doubint()} \vpageref{lst:doubint}.
\moddex{doubint}{double integration}{30}{42}{system/seismic} %

\par
You can imagine placing the ends and apex of each triangle
at a nearest neighbor mesh point as we did with the rectangles.
Instead I place these ends more precisely on the mesh with
linear interpolation.

\begin{comment}
Subroutine \texttt{lint1()} \vpageref{lst:lint1} does linear interpolation,
but here we need weighted results
as provided by \texttt{spotw()} \vpageref{lst:spotw}.
\progdex{spotw}{weighted linear interp.}

\par
Using these subroutines,
I assembled the stacking subroutine {\tt tristack()}
and the NMO routine {\tt trimo()} with triangle wavelets.
\end{comment}

The triangle routines are like those for rectangles
except for some minor changes.
Instead of computing the theoretical locations
of impulses on nearer and further traces,
I assumed a straight line tangent to the hyperbola
$t^2 = \tau^2+x^2/v^2$.
Differentiating by $x$ at constant $\tau$ gives the slope
$dt/dx= x/(v^2t)$.
As before, the area of the the wavelets, now triangles must be preserved.
The area of a triangle is proportional to the base times the height.
Since the triangles are built from double integration ramp functions,
the height is proportional to the base length.
Thus to preserve areas, each wavelet is scaled by the inverse {\em  squared}
of the triangle's base length.
Results are shown in
Figures~\ref{fig:trimo0} and~\ref{fig:trimo1}.
\sideplot{trimo0}{width=3.00in}{
	Triangle wavelets,
	accurately positioned,
	but aliased
	({\tt antialias=0.})
	}
\sideplot{trimo1}{width=3.00in}{
	Antialiased triangle wavelets.
	({\tt antialias=1.})
	Where ever triangle duration is more than about three points,
	the end of one triangle marks the apex of the next.
	}

\begin{comment}
\progdex{tristack}{stack with triangle footprint} 

\progdex{trimo}{triangle footprint}
\end{comment}

From the stack reconstruction of the model in Figure~\ref{fig:trimo1}
we see the reconstruction is more blured with antialiasing
than it was without in Figure~\ref{fig:trimo0}.
The benefit of antialiasing will become clear next
in more complicated examples where events cross.

\section{MIGRATION WITH ANTIALIASING}
\sx{antialias!migration}

\inputdir{mig}

\begin{comment}
Subroutine {\tt aamig()} below does migration and diffraction modeling
using subroutine {\tt trimo()} as the workhorse. %
\progdex{aamig}{antialias migration} % 
\end{comment}
Figure~\ref{fig:aamod} shows the synthetic image
that is used for testing.
There is a horizontal layer, a dipping layer, and a few impulses.
The impulses are chosen stronger than the layers because
they will spread out in the synthetic data.
\sideplot{aamod}{width=3.00in}{
	Model image for migration study.
	}

The velocity is taken constant.
Figure~\ref{fig:aad0} shows synthetic data made without regard for aliasing.
The hyperbolas look fine---the way we expect.
The horizontal layer, however, is followed by many pseudo layers.
These pseudo layers are the result of modeling with an operator
that is spatially aliased.
\sideplot{aad0}{width=3.00in}{
	Synthetic data without regard for aliasing.
%	Made from model image with {\tt aamig()}
%	taking {\tt antialias=0.}
	}

Figure~\ref{fig:aad1} shows how the synthetic data improves dramatically
when aliasing is taken into account.
The layers look fine now.
The hyperbolas, however,
have a waveform that is rapidly changing
with offset from the apex.
This changing waveform is an inevitable consequence of the anti-aliasing.
The apex has a huge amplitude
because the temporal bandwidth is widest at the apex
(because the dip is zero there,
there is no filtering away of high spatial frequencies).
Simple low-pass temporal filtering (not shown)
will cause the wavelet to be largely independent of offset.
\sideplot{aad1}{width=3.00in}{
	Synthetic data accounting for aliasing.
%	Made from model image with {\tt aamig()}
%	taking {\tt antialias=1.}
	}

\par
Do not confuse aliased data with synthetic data made by an aliased operator.
To make aliased data, you would start from good data,
such as Figure~\ref{fig:aad1},
and throw out alternate traces.
More typically, the earth makes good data and we fail to record
all the needed traces for the quality of our field arrays.

\par
The horizontal layer in Figure~\ref{fig:aad1}
has a waveform that resembles a damped step function
which is related to the Hankel tail we studied in chapter~\ref{paper:ft1}
where subroutine \texttt{halfint()} \vpageref{lst:halfint} was introduced
to provide the filter required to convert
the waveform on the  horizontal layer in Figure~\ref{fig:aad1} back to an impulse.
This was done in Figure~\ref{fig:aad1h}.
You can see the final flat-layer waveform
is roughly the zero-phase shape we started with.
\sideplot{aad1h}{width=3.00in}{
	Best synthetic data.
%	Made from model image using {\tt aamig()}
%	with {\tt antialias=1} followed by
%	a causal half-order time derivative.
	Lowpass temporal filtering would make wavelets
	more independent of location on a hyperbola.
	}

Figure~\ref{fig:aamig1} shows my best migration of my best synthetic data.
All the features of the original model are apparent.
Naturally, high frequencies are lost,
more on the dipping bed than the level one.  
Likewise the broadening of the deeper point scatterer compared
to the shallow one is a well known aperture effect.

\sideplot{aamig1}{width=3.00in}{
	Best migration of best synthetic data.
%	Uses {\tt aamig()}
%	with {\tt antialias=2} followed by
%	an anticausal half-order time derivative.
	}

Figure~\ref{fig:aamig0} shows what happens when antialiasing
is ignored in migration.
Notice many false layers above the given horizontal layer.
Notice semicircles above the impulses.
Notice apparent noise everywhere.
But notice also that the dipping bed is sharper
than the antialiased result in Figure \ref{fig:aamig1}.
\sideplot{aamig0}{width=3.00in}{
	Migration of best synthetic data
	without regard for aliasing.
%	Uses {\tt aamig()}
%	with {\tt antialias=0.}
%	(and an anticausal half-order time derivative)
	}

\subsection{Use of the antialiasing parameter}
\sx{antialias}

\par
Migration requires antialiasing, even where the earth has zero dip.
This is because the earth's horizontal layers
cut across the migration hyperbola.
An interesting extension is where the earth has dipping layers.
There the {\tt slope} parameter could be biased to account for it.
\par
Where the earth contains hyperbolas,
they will cut steeply across our migration hyperbola.
Figure~\ref{fig:croshyp} suggests
that such hyperbolas require
an antialias parameter greater than unity, say {\tt antialias=2.}
\sideplot{croshyp}{width=2.5in}{
	Crossing hyperbolas that do not touch.
	Thus the points shown are not enough
	to prevent spatial aliasing a line integral
	along one trajectory of signal on the other.
	}

\subsection{Orthogonality of crossing plane waves}
Normally, waves do not contain zero frequency.
Thus the time integral of a waveform normally vanishes.
Likewise,
for a dipping plane wave, the time integral vanishes.
Likewise,
a line integral
across the $(t,x)$-plane
along a straight line that crosses a plane wave
or a dipping plane wave vanishes.
Likewise,
two plane waves with different slopes should be orthogonal
if one of them has zero mean.

\par
I suggest that spatial aliasing may be defined and analyzed
with reference to plane waves
rather than with reference to frequencies.
Aliasing is when two planes that should be orthogonal, are not.
This is like two different frequency sinusoids.
They are orthogonal except perhaps if there is aliasing.

\section{ANTIALIASED OPERATIONS ON A CMP GATHER}
\sx{antialias!velocity analysis}
A common-midpoint gather holding data with only one velocity
should stack OK without need for antialiasing.
It is nice when antialiasing is not required
because then high temporal frequencies need not be filtered away
simply to avoid aliased spatial frequencies.
When several velocities are simultaneously present
on a CDP gather, we will find crossing waves.
These waves will be curved,
but aliasing concepts drawn from plane waves are still applicable.
We designed the antialiasing of migration by expecting hyperbola
flanks to be orthogonal to horizontal beds or dipping beds
of some chosen dip.
With a CDP gather we chose not a dip, but a slowness $s_0$.
The slope of a wave of slowness $s$ on a CDP gather is $xs^2/t$.
The greater the contrast in dips, the more need for antialiasing.
The slope of a wave with slowness $s_0$ is $xs_0^2/t$.
The difference between this slope and that of another wave is
$xs^2/t - xs_0^2/t$
or
$(s^2-s_0^2)x/t$
which in the program is the {\tt slope} for
the purpose of antialiasing.
The choice of $s_0$ has yet to be determined according to the application.
For illustration, I prepared a figure with three velocities,
a very slow surface wave, a water wave, and a fast sediment wave.
I chose $s_0$ to match the water wave.
In practice $s_0$ might be the earth's slowness
as a function of traveltime depth.

\inputdir{veltran}
\sideplot{aacdp}{width=3.00in}{
	The air wave and fast wave are broadened increasingly with offset,
	but the water wave does not.
	This broadening enables crossing events
	to maintain their orthogonality.
	}

\subsection{Iterative velocity transform}
After we use data to determine a velocity model
(or slowness model)
with an operator $\bold A$
we may wonder whether synthetic data made from that model
with the adjoint operator $\bold A'$ resembles the original data.
In other words,
we may wonder how close the velocity transform $\bold A$
comes to being unitary.
The first time I tried this, I discovered that
large offsets and large slownesses were attenuated.
With a bit of experimentation I found that the scale factor $\sqrt{sx}$
seems to make the velocity transform approximately a unitary one.
Results are in Figure~\ref{fig:aavel1}. 
	% height shrunk on Jan 20,1999 to try to fit on half page
\activeplot{aavel1}{width=6.00in,height=3.7in}{ER}{
	Top left: Slowness model.  Top right: Data derived from
	it using the pseudounitary scale factor.
%	$\sqrt{sx}$.
	Bottom left: the velocity spectrum of top right.
	Bottom right: data made from velocity spectrum.
	}

Figure~\ref{fig:aavel1} shows that on a second pass,
the velocity spectrum of the slow wave is much smoothed.
This suggests that it might be more efficient to parameterize
the data with slowness {\em  squared} rather than slowness itself.
Another interesting thing about using slowness squared as 
an independent variable is that when slowness squared is negative
(velocity imaginary)
the data is matched by ellipses curving up
instead of hyperbolas curving down.
	% height shrunk on Jan 20,1999 to try to fit on half page
\activeplot{aavel0}{width=6.00in,height=3.7in}{ER}{
	Like Figure~\protect\ref{fig:aavel1} but with {\tt antialias=0.}
	This synthetic data presumes no receiver groups
	in the field recording.
	}

\par
Figure~\ref{fig:aavel0} shows the effect of no antialiasing
in either the field recording or the processing.
The velocity spectrum is as sharp, if not sharper,
but it is marred by a large amount of low-level noise.

\par
Aliased data gives an interesting question.
Should we use an aliased operator as in Figure~\ref{fig:aavel0}
or should we use an antialiased operator as that in Figure~\ref{fig:aavel1}?
Figure~\ref{fig:adataavel} shows the resulting velocity analysis.
The antialiased operator seems well worth while,
even when applied to aliased data.
\activesideplot{adataavel}{width=2.60in}{ER}{
	Aliased data analyzed with antialiased operator.
	Compare to the lower left of Figure~\protect\ref{fig:aavel0}.
	}
\par
In real life, the field arrays are not ``dynamic''
(able to respond with space and time variable $s_0$)
but the data processing can be dynamic.
Fourier and finite-difference methods of wave propagation
and data processing are generally immune to aliasing difficulties.
On the other hand,
dynamic arrays in the data processing
are a helpful feature of the ray approach whose counterparts
seem unknown with Fourier and finite-difference techniques.

\par
Since $\sqrt{sx}$ does not appear in physical modeling,
people are sometimes hesitant to put it in the velocity analysis.
If $\sqrt{sx}$ is omitted from the modeling,
then $|sx|$ should be put in the velocity analysis.
Failing to do so will give a result like in figure~\ref{fig:velsmear}.
The principal feature of such a velocity analysis
is the velocity smearing.
A reason for smearing is that the zero-offset signal
is strong in all velocities.
Multiplying by $\sqrt{sx}$ kills that signal
(which is never recorded in the field anyway).
The conceptual advantage of a pseudounitary transformation 
like Figure~\ref{fig:aavel1} is that points in velocity space
are orthogonal components like Fourier components
whereas for nonunitary transforms
like with Figure~\ref{fig:velsmear} 
the different points in velocity space
are not orthogonal components.
\activeplot{velsmear}{width=6.00in,height=2.25in}{ER}{
	Like Figure~\protect\ref{fig:aavel1}
	omitting pseudounitary scaling. {\tt psun=0.}
	Right is synthetic data and left the analysis of it
	which is badly smeared.
	}

\par
Subroutine {\tt veltran()} does the work.
\progdex{veltran}{antialiased velocity transform}

%\subsection{Fourier interpolation}
%Since these topics are near the heart of production seismology,
%considerable attention is often given to numerical analysis.
%Rectangle and triangle functions are instances of low-pass filters.
%The ``ideal'' low-pass filter is the sinc function.
%It is constant in the passband (so it disturbs nothing)
%and zero in the reject band (so it rejects perfectly).
%Given the slope $dt/dx$ of the moveout curve,
%the cutoff frequency $\omega$ of the sinc function
%should be chosen in relation to the spatial Nyquist frequency $k$
%so that $dt/dx = k/\omega$.
%Dave Hale (electronic mail) says:
%%
%\begin{quotation}
%	\noindent
%	The ``production'' approach to anti-aliasing
%	Kirchhoff migration is
%        \begin{verbatim}
%for each input trace {
%        apply a suite of high-cut filters to input trace
%        for each output trace {
%                for each depth {
%                        determine slope of migration operator;
%                        use slope of operator to determine
%                        which filtered version of input to use;
%                        map (interpolate) input to output
%                }
%        }
%}
%	\end{verbatim}
%%
%%	\noindent
%	Because each input trace gets mapped to lots of output traces,
%	the overhead of the initial high-cut filtering is negligible.
%	Furthermore, most folks do this in the frequency domain and
%	take the opportunity to supersample the trace (by padding
%	with zeros in the frequency domain) so that the mapping in the
%	innermost loop can be done with linear or even nearest
%	neighbor interpolation.  Obviously, this approach implies
%	``semicircle superposition'' instead of ``hyperbola summation''.
%\end{quotation}
%
%\noindent
%Hale might have the best method,
%but an argument against sinc interpolations is that
%the sharpness of the cutoff frequency loses relevance
%in view of arbitrariness
%of the three parameters that determine it,
%{\tt antialias},
%$s_0$, and regional dip parameter.
%An alternate approach is to interpolate the data first
%with a Fourier method (for accuracy)
%and then use the triangle method given here (for speed and simplicity).
%
%
%
%\section{EXTENSIONS AND FUTURE WORK}
%
%\subsection{Velocity derivative}
%Changing one velocity to another
%moves the location of a triangle
%on the time axis.
%The derivative of a triangle function
%is a box doublet.
%Thus to calculate a derivative with respect to velocity,
%it seems the main programming change to {\tt trimo()}
%is to change ramp functions to step functions,
%i.e.~to replace the call to {\tt doubint()} by a call
%to {\tt causint()}.
%The amplitude also changes by the scale $dt/dv$.
%It would be fun to continue here.
%
%
%\subsection{Optimizing migration}
%\par
%I notice 2-3 dozen computational operations for each triangle wavelet.
%While these many operations are of no concern in
%demonstrations and experimental work,
%for production work it is desirable to eliminate them if possible.
%The midpoint axis is the key.
%With velocity transformations,
%the inner loop can then be a sum over midpoint.
%With migration, this speedup is only possible if the 
%trace spacing and velocity are constant
%over the $x$-axis.
%Subroutine {\tt aamig()} implements this convolution over midpoint
%outside the loop on $t$
%but the midpoint loop can be brought inside the inner loops
%in the {\tt spotw()} subroutine.
%Then the outer overhead would be of little consequence
%and the migration should speed up by a factor of 2-3 dozen.
%
%\par
%It would be fun to have movies of migration versus
%the antialiasing parameter to see
%if there is a practical importance to the distinction between
%{\tt antialias=1} and
%{\tt antialias=2.}
%
%\par
%Given the ample computer power that we enjoy nowadays,

\begin{exer}
\item
	What circumstances would suggest that the
	linear interpolation in subroutine \texttt{trimo()} \vpageref{lst:trimo}
	be replaced by nearest-neighbor interpolation?
\item
	Show how to adapt the programs of this chapter
	to variable trace spacing and missing data.
	Hint: Split {\tt trimo()} into two parts,
	the first determining the location of the neighboring
	traces and the second using that information.
\end{exer}

% This is too specialized for a basic text.
%\par
%Most seismic data is recorded with low frequencies strongly filtered,
%however, I have seen recorded data with substantial zero frequency energy.
%This seems to be from pressure sensors that stay at
%a constant depth from the bottom as surface water waves roll by
%effectively at zero velocity.
%I conjecture the double integral of such data would be so strongly
%dominated by the low frequency that a precision problem would arise.
%One method of solving this problem is to prefilter the data.
%Another would be to use tapered rectangles and tapered triangles
%starting from something like $(1- (.99Z)^n)/(1-.99Z)$.
%
%
%%%%%%%%














































































































