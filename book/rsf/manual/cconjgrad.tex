\section{Conjugate-gradient with shaping regularization for complex numbers (cconjgrad.c)}




\subsection{{norm}}
Returns the $L_2$ norm of the complex number with double-precision, or the sum of $L_2$ norms, if there is an array of complex numbers.

\subsubsection*{Call}
\begin{verbatim}prod = norm (n, x);\end{verbatim}

\subsubsection*{Definition}
\begin{verbatim}
static double norm (int n, const sf_complex* x) 
/* double-precision L2 norm of a complex number */
{
   ...
}
\end{verbatim}

\subsubsection*{Input parameters}
\begin{desclist}{\tt }{\quad}[\tt x]
   \setlength\itemsep{0pt}
   \item[n] size of the array of complex numbers (\texttt{int}).  
   \item[x] a complex number (\texttt{sf\_complex*}).  
\end{desclist}

\subsubsection*{Output}
\begin{desclist}{\tt }{\quad}[\tt ]
   \setlength\itemsep{0pt}  
   \item[prod] $L_2$ norm of the complex number. It is of type \texttt{static double}.
\end{desclist}





\subsection{{sf\_cconjgrad\_init}}\label{sec:sf_cconjgrad_init}
Initializes the complex conjugate gradient solver by initializing the required variables and allocating the required space.

\subsubsection*{Definition}
\begin{verbatim}sf_cconjgrad_init (np, nx, nd, nr, eps, tol, verb, hasp0);\end{verbatim}

\subsubsection*{Definition}
\begin{verbatim}
void sf_cconjgrad_init(int np     /* preconditioned size */, 
                       int nx     /* model size */, 
                       int nd     /* data size */, 
                       int nr     /* residual size */, 
                       float eps  /* scaling */,
                       float tol  /* tolerance */, 
                       bool verb  /* verbosity flag */, 
                       bool hasp0 /* if has initial model */) 
/*< solver constructor >*/
{
   ...
}
\end{verbatim}

\subsubsection*{Input parameters}
\begin{desclist}{\tt }{\quad}[\tt hasp01]
   \setlength\itemsep{0pt}
   \item[np]    the size of the preconditioned data (\texttt{int}).  
   \item[nx]    size of the model (\texttt{int}).  
   \item[nd]    size of the data (\texttt{int}).  
   \item[nr]    size of the residual (\texttt{int}).  
   \item[eps]   the scaling parameter (\texttt{float}).  
   \item[tol]   tolerance to the error in the solution  (\texttt{float}).  
   \item[verb]  verbosity flag (\texttt{bool}).  
   \item[hasp0] if there is a initial model (\texttt{bool}).  
\end{desclist}





\subsection{{sf\_cconjgrad\_close}}
Frees the space allocated for the complex conjugate gradient solver by \hyperref[sec:sf_cconjgrad_init]{\texttt{sf\_cconjgrad\_init}}.

\subsubsection*{Definition}
\begin{verbatim}sf_cconjgrad_close();\end{verbatim}

\subsubsection*{Definition}
\begin{verbatim}
void sf_cconjgrad_close(void) 
/*< Free allocated space >*/
{
   ...
}
\end{verbatim}




\subsection{{sf\_cconjgrad}}
Applies the complex conjugate gradient solver with the shaping filter to the input data.

\subsubsection*{Definition}
\begin{verbatim}sf_cconjgrad (prec, oper, shape, p, x, dat, niter);\end{verbatim}

\subsubsection*{Definition}
\begin{verbatim}
void sf_cconjgrad(sf_coperator prec     /* data preconditioning */, 
                  sf_coperator oper     /* linear operator */, 
                  sf_coperator shape    /* shaping operator */, 
                  sf_complex* p         /* preconditioned model */, 
                  sf_complex* x         /* estimated model */, 
                  const sf_complex* dat /* data */, 
                  int niter             /* number of iterations */)
/*< Conjugate gradient solver with shaping >*/
{
   ...
}
\end{verbatim}

\subsubsection*{Input parameters}
\begin{desclist}{\tt }{\quad}[\tt shape]
   \setlength\itemsep{0pt}
   \item[prec]  preconditioning operator (\texttt{sf\_coperator}).  
   \item[oper]  the operator (\texttt{sf\_coperator}).  
   \item[shape] the shaping operator (\texttt{sf\_coperator}).  
   \item[p]     the preconditioned model (\texttt{sf\_complex*}).  
   \item[x]     estimated model  (\texttt{sf\_complex*}).  
   \item[dat]   the data (\texttt{sf\_complex*}).  
   \item[niter] number of iterations (\texttt{int}).  
\end{desclist}


