
\def\sx#1{}
\def\bx#1{#1}
\def\eq{\quad =\quad}

\title{Preconditioning}
\author{Jon Claerbout}
\maketitle

\label{paper:prc}
\sx{precondition}


\par
In Chapter \ref{paper:ajt}, we developed adjoints and
in Chapter \ref{paper:lsq}, we developed inverse operators.
Logically, correct solutions come only through inversion.
Real life, however, seems nearly the opposite.
This situation is puzzling but intriguing.
It seems an easy path to fame and profit would
be to go beyond adjoints by introducing
some steps of inversion.
It is not that easy.
Images contain so many unknowns.
Mostly, we cannot iterate to completion
and need concern ourselves with the rate of convergence.
Often, necessity limits us to a handful
of iterations whereby in principle,
millions or billions are required.

\par
When you fill your car with gasoline,
it derives more from an adjoint than an inverse.
Industrial seismic data processing relates more to
\sx{seismology}
adjoints than to inverses
though there is a place for both, of course.
It cannot be much different with medical imaging.
\par
First consider cost.
For simplicity, consider a data space with $N$ values
and a model (or image) space of the same size.
The computational cost of applying a dense adjoint
operator increases in direct proportion to the number
of elements in the matrix, in this case $N^2$.
To achieve the minimum discrepancy between modeled data
and observed data (inversion) theoretically requires $N$ iterations
raising the cost to $N^3$.
\par
Consider an image of size $m\times m=N$.
Continuing, for simplicity, to assume a dense matrix of relations between
model and data,
the cost for the adjoint is $m^4$, whereas, the cost for inversion is $m^6$.
We consider computational costs for the year 2000, but
noticing that costs go as the sixth power of the mesh size,
the overall situation will not change much in the foreseeable future.
Suppose you give a stiff workout to a powerful machine;
you take an hour to invert a 4,096$\times$ 4,096 matrix.
The solution, a vector of $4096$ components could
be laid into an image of size 64 $\times$ 64= $2^6\times 2^6$ = 4,096.
Here is what we are looking at for costs:
\par
\begin{center}
\begin{tabular}{||r|r|r|r|r||}                                          \hline
adjoint cost &$(m\times m )^2$ & $(512\times 512)^2$ & $(2^9 2^9)^2$ & $2^{36}$
\\ \hline
inverse cost &$(m\times m )^3$ & $  (64\times 64)^3$ & $(2^6 2^6)^3$ & $2^{36}$
\\ \hline
\end{tabular}
\end{center}
\par\noindent
These numbers tell us that for applications with dense operators,
the biggest images that we are likely to see coming from inversion methods
are $64\times 64$, whereas, those from adjoint methods are $512\times 512$.
For comparison, your vision is comparable
to your computer screen at 1,000 $\times$ 1,000.
%We might summarize by saying that while adjoint methods are less than perfect, inverse methods are ``legally blind'' :-)
\sideplot{512x512}{width=.8\textwidth,height=.8\textwidth}{
	Jos greets Andrew, ``Welcome back Andrew''
	from the Peace Corps.
	At a resolution of $512\times 512$, this picture
	is approximately the same as the resolution
	as the paper on which it is printed,
	or the same as your viewing screen,
	if you have scaled it up to 50\%\ of screen size.}
%\newslide

\par\noindent
\url{http://sepwww.stanford.edu/sep/jon/family/jos/gifmovie.html} holds a movie
blinking between Figures \ref{fig:512x512} and \ref{fig:64x64}.
\par
This cost analysis is oversimplified in that most applications
do not require dense operators.
With sparse operators, the cost advantage of adjoints is even more
pronounced because for adjoints,
the cost savings of operator sparseness translate directly to
real cost savings.
The situation is less favorable and more muddy for inversion.
The reason that Chapter 2 covers iterative methods
and neglects exact methods is that in practice
iterative methods are not run to theoretical completion,
but until we run out of patience.
But that leaves hanging the question of what percent
of theoretically dictated work is actually necessary.
If we struggle to accomplish merely one percent of the theoretically required work,
can we hope to achieve anything of value?

\par
Cost is a big part of the story, but the story has many other parts.
Inversion, while being the only logical path to the best answer,
is a path littered with pitfalls.
The first pitfall is that the data is rarely able to 
determine a complete solution reliably.
Generally, there are aspects of the image that are not learnable
from the data.

\sideplot{64x64}{width=.8\textwidth,height=.8\textwidth}{
	Jos greets Andrew, ``Welcome back Andrew'' again.
	At a resolution of $64\times 64$,
	the pixels are clearly visible.
	From far the pictures are the same.
	From near, examine their glasses.}
%\newslide


When I first realized that practical imaging methods with wide
industrial use amounted merely to the adjoint of forward modeling,
I (and others) thought an easy way to achieve fame and fortune
would be to introduce the first steps toward inversion
along the lines of Chapter \ref{paper:lsq}.
Although inversion generally requires a prohibitive number
of steps, I felt that moving in the gradient direction,
the direction of steepest descent, would move us rapidly
in the direction of practical improvements.
This optimism was soon exposed.
Improvements came too slowly.
But then, I learned about the conjugate gradient method that
spectacularly overcomes a well-known speed problem with the
method of steepest descent.
I came to realize it was still too slow.
I learned by watching the convergence in Figure
\ref{fig:conv},
which led me to the helix method in Chapter 4.
Here we see how it speeds many applications.

\par
We also come to understand why the gradient is such a poor direction
both for steepest descent and conjugate gradients.
An indication of our path is found in the contrast between
an exact solution and the gradient.
\begin{eqnarray}
\label{eqn:herestart}
\bold m &=& (\bold A\T\bold A)^{-1}\bold A\T\bold d
\\
\label{eqn:herego}
\Delta \bold m &=& \quad\quad\quad \bold A\T\bold d
\end{eqnarray}
Equations
(\ref{eqn:herestart}) and
(\ref{eqn:herego})
differ by the factor $(\bold A\T\bold A)^{-1}$.
This factor is sometimes called a spectrum,
and in some situations,
it literally is a frequency spectrum.
Our updates do not have the spectrum
of the thing we are trying to build.
No wonder it's slow!
Here we find for many applications that
``preconditioning'' with the helix is a better way.



\section{PRECONDITIONED DATA FITTING}
\par
Iterative methods (like conjugate-directions) can sometimes be accelerated
by a \bx{change of variables}.
The simplest change of variable is called a ``trial solution.''
Formally, we write the solution as:
\begin{equation}
\bold m \eq \bold S \bold p
\end{equation}
where $\bold m$ is the map we seek,
columns of the matrix $\bold S$ are ``shapes'' we like
and coefficients in $\bold p$ are unknown coefficients
to select amounts of the favored shapes.
The variables $\bold p$  are often called the ``preconditioned variables.''
It is not necessary that $\bold S$ be an invertible matrix,
but we see later that invert-ability  is helpful.
Inserting the trial solution $\bold m = \bold S\bold p$
into $\bold 0\approx\bold F\bold m -\bold d$
gives:
\begin{eqnarray}
\bold 0 \quad & \approx & \quad  \bold F \bold m \ -\  \bold d
\\
\bold 0 \quad & \approx & \quad  \bold F \bold S \bold p \ -\  \bold d
\end{eqnarray}
We pass the operator $\bold F \bold S$ to our iterative solver.
After finding the best fitting                                      $\bold p$,
we merely evaluate
                                                $ \bold m = \bold S \bold p$
to get the solution to the original problem.

\par
We hope this change of variables has saved effort.
For each iteration, there is a little more work:
Instead of the iterative application of
                                                $\bold F$ and $\bold F\T$,
we have iterative application of
                                        $\bold F\bold S$ and $\bold S\T\bold F\T$.

Our hope is that the number of iterations decreases, because we are clever
or because we have been lucky in our choice of $\bold S$.
Hopefully,
the extra work of the preconditioner operator $\bold S$
is not large compared to $\bold F$.
If we should be so lucky that
$\bold S= \bold F^{-1}$,
then we get the solution immediately.
Obviously we would try any guess with
$\bold S\approx \bold F^{-1}$.
Where I have known such $\bold S$ matrices,
I have often found that convergence is accelerated,
but not by much.
Sometimes, it is worth using $\bold F\bold S$ for a while in the beginning;
but later, it is cheaper and faster to use only $\bold F$.
A practitioner might regard the guess of $\bold S$
as prior information,
like the guess of the initial model $\bold m_0$.

\par
For a square matrix $\bold S$,
the use of a preconditioner should not change the ultimate solution.
Taking $\bold S$ to be a tall rectangular matrix
reduces the number of adjustable parameters,
changes the solution,
gets it quicker, but lowers resolution.

\subsection{Preconditioner with a starting guess}
We often have a starting solution
$\bold m_0$.
You might worry that
you could not find the starting preconditioned variable 
$\bold p_0= \bold S^{-1}\bold m_0$,
because you did not know the inverse of $\bold S$.
\par
We solve this problem using a shifted unknown $\tilde {\bold m}$.
\par
\begin{center}
\begin{tabular}{rllr}
$\bold 0 $&$\approx$ & $\bold F\bold m -\bold d $     & typical regression
\\
$\bold 0 $&$\approx$ & $\bold F(\tilde{\bold m}+\bold m_0) -\bold d $  &
                                                        Define $\bold m =\tilde{\bold m}+\bold m_0$
\\
$\bold 0 $&$\approx$ & $\bold F\tilde{\bold m}+\bold F \bold m_0 -\bold d $  &
\\
$\bold 0 $&$\approx$ & $\bold F\tilde{\bold m}-\tilde {\bold d} $  & Defines $\tilde{\bold d}$
\\
        &          &               & Implicitly define $\bold p$ by $\tilde{\bold m}=\bold S\bold p$.
\\
$\bold 0 $&$\approx$ & $\bold F\bold S \bold p-\tilde {\bold d} $  & You iterate for $\bold p$.
\\
$\tilde{\bold m}$&=&$\bold S\bold p$  & from your definition
\\
$\bold m$ &=&$ \tilde{\bold m} + \bold m_0 $& Got the answer.
\end{tabular}
\end{center}
\par\noindent
which solves the problem never needing $\bold S^{-1}$.
Unfortunately, as we see later,
this conclusion is only valid while there is no regularization.

\subsection{Guessing the preconditioner}
We are tasked with coming up with ``trial solution''---a pretty vague assignment.
Some kind of a scaling, smoothing, or shaping transformation $\bold S$
of some mysterious ``preconditioned space'' $\bold p$
should represent the model $\bold m$ we seek.
We begin by investigating how the shaper $\bold S$ alters the gradient.
\par
\begin{center}
\begin{tabular}{rllr}
$\bold m $&=&$ \bold S \bold p$		& introduces $\bold S$, implicitly defines $\bold p$
\\
$\Delta\bold m $&=&$ \bold S \Delta\bold p$	& consequence of the above
\\
$\Delta\bold m $&=&$ \bold F\T \bold r$ 	& gradient is adjoint upon residual
\\
$\bold 0\approx \bold r $&=&$ \bold F\bold m-\bold d$	         & residual in terms of $\bold m$
\\
$\bold r $&=&$ \bold F(\bold S\bold p) -\bold d $ 		& residual in terms of $\bold p$
\\
$\bold 0\approx \bold r $&=&$  (\bold F\bold S)\bold p-\bold d$ 		& reordering calculation
\\
$\Delta\bold p $&=&$ (\bold F\bold S)\T \bold r 	$& gradient is adjoint upon residual
\\
$\Delta\bold p $&=&$  \bold S\T\bold F\T\bold r	$&          reordering
\\
$\Delta\bold m $&=&$ (\bold S\bold S\T) \bold F\T\bold r$   & recalling $\Delta\bold m=\bold S\Delta\bold p$
\end{tabular}
\end{center}
\par
We may compare the gradient $\Delta\bold m$ with and without preconditioning.
\par
\begin{center}
\begin{tabular}{rllr}
$\Delta\bold m $&=&$ \ \quad \quad \bold F\T \bold r$ 	    & original
\\
$\Delta\bold m $&=&$ (\bold S\bold S\T) \bold F\T\bold r$   & with preconditioning transformation
\end{tabular}
\end{center}
When the first vanishes, the second also vanishes.
When the second vanishes, the first vanishes provided $ (\bold S\bold S\T)$ is a nonsingular matrix.
As our choice of $\bold S$ is quite arbitrary,
it is marvelous the freedom we have to monkey with the gradient.
\par
Remember that $\bold r$ starts off being $-\bold d$.
Compare the $(\bold S\bold S\T)$ scaled gradient to the analytic solution.
\par
\begin{center}
\begin{tabular}{rllr}
$\Delta\bold m $&=&$ (\bold S\bold S\T)\ \ \  \bold F\T\bold r	$& modified gradient
\\
$\bold m $&=&$ (\bold F\T\bold F)^{-1} \bold F\T\bold d		$& analytic solution
\end{tabular}
\end{center}
Mathematically,
we see it would be delightful if
$ (\bold S\bold S\T)$ were something like $(\bold F\T\bold F)^{-1}$,
but we rarely have ideas how to arrange it.
We do,
however, have some understanding of the world of images,
and understand where on the image we would like iterations to concentrate first,
and what spatial frequencies are more relevant than others.
If we cannot go all the way, as we cannot in giant imaging problems,
it is important to make the important steps early.


\section{PRECONDITIONING THE REGULARIZATION}

%\subsection{Recasting a fitting problem in white variables}

\par
The basic formulation of a geophysical estimation problem
consists of setting up
{\em  two}
goals,
one for data fitting
and the other for model shaping.
With two goals, preconditioning is somewhat different.
The two goals may be written as:
\begin{eqnarray}
\bold 0 &\approx& \bold F \bold m - \bold d \\
\bold 0 &\approx& \bold A \bold m
\end{eqnarray}
which defines two residuals,
a so-called ``data residual'' and ``model residual'' that
are usually minimized by conjugate-direction, least-squares methods.
\par
To fix ideas, let us examine a toy example.
The data and the first three rows of the following matrix 
are random numbers truncated to integers.
The model-roughening operator $\bold A$
is a first-differencing operator times 100.

%\newslide
{\samepage
\par\noindent
\footnotesize
\begin{verbatim}
 d(m)     F(m,n)                                            iter   Sum(|grad|)

-100.     62.  18.   2.  75.  99.  45.  93. -41. -15.  80.     1    69262.0000
 -83.     31.  80.  92. -67.  72.  81. -41.  87. -17. -38.     2    19012.8203
  20.      3. -21.  58.  38.   9.  18. -81.  22. -14.  20.     3    10639.0791
   0.    100.-100.   0.   0.   0.   0.   0.   0.   0.   0.     4     4578.7988
   0.      0. 100.-100.   0.   0.   0.   0.   0.   0.   0.     5     2332.3352
   0.      0.   0. 100.-100.   0.   0.   0.   0.   0.   0.     6     1676.6978
   0.      0.   0.   0. 100.-100.   0.   0.   0.   0.   0.     7      622.7415
   0.      0.   0.   0.   0. 100.-100.   0.   0.   0.   0.     8      454.1242
   0.      0.   0.   0.   0.   0. 100.-100.   0.   0.   0.     9      290.6053
   0.      0.   0.   0.   0.   0.   0. 100.-100.   0.   0.    10      216.0749
   0.      0.   0.   0.   0.   0.   0.   0. 100.-100.   0.    11        1.0488
   0.      0.   0.   0.   0.   0.   0.   0.   0. 100.-100.    12        0.0061
   0.      0.   0.   0.   0.   0.   0.   0.   0.   0. 100.    13        0.0000
\end{verbatim}
}
\normalsize

\par
The right-most column shows the sum of the absolute values of the gradient.
Notice at the 11th iteration,
the gradient suddenly plunges.
Because there are ten unknowns and the matrix is obviously full-rank,
conjugate-gradient theory tells us to expect
the exact solution at the 11th iteration.
This sudden convergence is the first miracle of conjugate gradients.
Failure to achieve a precisely zero gradient at the 11th step
is a precision issue that could be addressed
with double precision arithmetic.
The residual magnitude (not shown) does not approach zero,
because 13 linear equations defeat the ten adjustable coefficients.


\subsection{The second miracle of conjugate gradients}

The second miracle of conjugate gradients is exhibited in the following.
The data and data fitting matrix are the same,
but the model damping is simplified.

%\newslide
\par\noindent
\footnotesize
\begin{verbatim}
 d(m)     F(m,n)                                            iter   Sum(|grad|)

-100.     62.  18.   2.  75.  99.  45.  93. -41. -15.  80.     1  69262.0000
 -83.     31.  80.  92. -67.  72.  81. -41.  87. -17. -38.     2   5486.2095
  20.      3. -21.  58.  38.   9.  18. -81.  22. -14.  20.     3   2755.6702
   0.    100.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4      0.0012
   0.      0. 100.   0.   0.   0.   0.   0.   0.   0.   0.     5      0.0011
   0.      0.   0. 100.   0.   0.   0.   0.   0.   0.   0.     6      0.0006
   0.      0.   0.   0. 100.   0.   0.   0.   0.   0.   0.     7      0.0006
   0.      0.   0.   0.   0. 100.   0.   0.   0.   0.   0.     8      0.0005
   0.      0.   0.   0.   0.   0. 100.   0.   0.   0.   0.     9      0.0005
   0.      0.   0.   0.   0.   0.   0. 100.   0.   0.   0.    10      0.0012
   0.      0.   0.   0.   0.   0.   0.   0. 100.   0.   0.    11      0.0033
   0.      0.   0.   0.   0.   0.   0.   0.   0. 100.   0.    12      0.0033
   0.      0.   0.   0.   0.   0.   0.   0.   0.   0. 100.    13      0.0000
\end{verbatim}
\normalsize
\par\noindent
Even though the matrix is full-rank,
we see the residual drop about six decimal places after the third iteration!
This convergence behavior is well known
in the computational mathematics literature.
Despite its practical importance,
it does not seem to have a name or identified discoverer.
So, I call it the ``second miracle.''

\par
Practitioners usually do not like
the identity operator for model-space shaping.
Generally, they prefer to penalize wiggliness.
For practitioners,
the lesson of the second miracle of conjugate gradients
is that we have a choice of many iterations
or learning to transform
independent variables so
the regularization operator becomes an identity matrix.
Basically, such a transformation reduces the iteration count
from    something the size of the model space
to      something the size of the data space.
Such a transformation is called ``preconditioning.''

\par
More generally,
the model goal $\bold 0 \approx \bold A \bold m$
introduces a roughening operator like a gradient,
a Laplacian, or
in Chapter \ref{paper:mda},
a Prediction-Error Filter (PEF).
Thus, the model goal is usually a filter,
unlike the data-fitting goal
that involves all manner of geometry and physics.
When the model goal is a filter, its inverse is also a filter.
Of course, this includes multidimensional filters with a helix.


\par
The preconditioning transformation
$\bold m = \bold S \bold p$
gives us:
\begin{equation}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F \bold S \bold p - \bold d \\
        \bold 0 &\approx & \bold A \bold S \bold p
        \label{eqn:invintprecond}
        \end{array}
\end{equation}
The operator $\bold A$ is a roughener, while $\bold S$ is a smoother.
The choices of both $\bold A$ and $\bold S$ are somewhat subjective.
This freedom of choice suggests we eliminate $\bold A$ altogether
by {\em  defining} it to be proportional to the inverse of $\bold S$,
thus $\bold A\bold S=\bold I$.
The fitting goals become:
\begin{equation}
        \label{eqn:whitevar1}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F  \bold S \bold p - \bold d \\
        \bold 0 &\approx & \epsilon\ \bold p
        \end{array}
\end{equation}
which enables us to benefit from the ``second miracle.''
After finding $\bold p$,
we obtain the final model with $\bold m = \bold S \bold p$.
\par
The solution $\bold m$ is likely to come out smooth,
because we typically over-sample axes of physical quantities.
We typically penalize roughness in it by our choice of a regularization operator
which means the preconditioning variable $\bold p$ typically
has a wider frequency bandwidth than $\bold m$.
In Chapter \ref{paper:mda}, we see
how to make the spectrum of $\bold p$
come out white (tending to flat spectrum).
\sx{flattening ! spectrum}

\subsection{Importance of scaling}

Another simple toy example shows us the importance of scaling.
We use the same example as the previous one,
except we make the diagonal penalty function
vary slowly with location.

%\newslide
\par\noindent
\footnotesize
\begin{verbatim}
 d(m)     F(m,n)                                            iter   Sum(|grad|)

-100.     62.  16.   2.  53.  59.  22.  37. -12.  -3.   8.     1  42484.1016
 -83.     31.  72.  74. -47.  43.  40. -16.  26.  -3.  -4.     2   8388.0635
  20.      3. -19.  46.  27.   5.   9. -32.   7.  -3.   2.     3   4379.3032
   0.    100.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4   1764.9844
   0.      0.  90.   0.   0.   0.   0.   0.   0.   0.   0.     5    868.9418
   0.      0.   0.  80.   0.   0.   0.   0.   0.   0.   0.     6    502.5179
   0.      0.   0.   0.  70.   0.   0.   0.   0.   0.   0.     7    450.0512
   0.      0.   0.   0.   0.  60.   0.   0.   0.   0.   0.     8    185.2923
   0.      0.   0.   0.   0.   0.  50.   0.   0.   0.   0.     9    247.1021
   0.      0.   0.   0.   0.   0.   0.  40.   0.   0.   0.    10    338.7060
   0.      0.   0.   0.   0.   0.   0.   0.  30.   0.   0.    11    119.5686
   0.      0.   0.   0.   0.   0.   0.   0.   0.  20.   0.    12     34.3372
   0.      0.   0.   0.   0.   0.   0.   0.   0.   0.  10.    13      0.0000
\end{verbatim}
\normalsize
We observe that solving the same problem for the scaled variables
has required a severe increase
in the number of iterations required to get the solution.
We lost the benefit of the second CG miracle.
Even the rapid convergence predicted for the 11th iteration
is delayed until the 13th.
\par
Another curious fact may be noted here.
The gradient does not decrease monotonically.
It is known theoretically that the residual does decrease monotonically,
but the gradient need not.
I did not show the norm of the residual,
because I wanted to display a function that vanishes at convergence,
and the residual does not.

%COMMENTED OUT BECAUSE THEY HAVEN'T YET HAD PEF'S.
%%\subsection{Statistical interpretation}
%%This book is not a statistics book.
%%Never-the-less, many of you have some statistical knowledge
%%that allows you a statistical interpretation
%%of these views of preconditioning.
%%\par
%%A statistical concept is that we can combine many streams
%%of random numbers into a composite model.
%%Each stream of random numbers is generally taken
%%to be uncorrelated with the others,
%%to have zero mean, and to have the same variance
%%as all the others.
%%This is often abbreviated as IID, denoting
%%Independent, Identically Distributed.
%%Linear combinations
%%like filtering and weighting operations
%%of these IID random streams
%%can build correlated random functions much like those
%%observed in geophysics.
%%A geophysical practitioner seeks to do the inverse,
%%to operate on the correlated unequal random variables
%%and create the statistical ideal random streams.
%%The identity matrix required for the ``second miracle'',
%%and our search for a good preconditioning transformation
%%are related ideas.
%%The relationship will become more clear in chapter \ref{paper:mda}
%%when we learn how to estimate the best roughening operator $\bold A$
%%as a prediction-error filter.
%%\par
%%\boxit{
%%        Two philosophies to find a preconditioner:
%%        \begin{enumerate}
%%        \item
%%        Dream up a smoothing operator $\bold S$.
%%        \item
%%        Estimate a prediction-error filter $\bold A$,
%%        and then use its inverse $\bold S = \bold A^{-1}$.
%%        \end{enumerate}
%%        }
%%\par
%%\boxit{
%%        Deconvolution on a helix is an all-purpose
%%        preconditioning strategy for multidimensional model regularization.
%%        }
%%\par
%%The outstanding acceleration of convergence by preconditioning
%%suggests that the philosophy of image creation by optimization
%%has a dual orthonormality:
%%First, Gauss (and common sense) tells us that the data residuals
%%should be roughly equal in size.  Likewise in Fourier space
%%they should be roughly equal in size, which means they should
%%be roughly white, i.e. orthonormal.
%%(I use the word ``orthonormal''
%%because white means the autocorrelation is an impulse,
%%which means the signal is statistically orthogonal
%%to shifted versions of itself.)
%%Second,
%%to speed convergence of iterative methods,
%%we need a whiteness, another orthonormality, in the solution.
%%The map image, the physical function that we seek, might
%%not be itself white, so we should solve first for another variable,
%%the whitened map image, and as a final step,
%%transform it to the ``natural colored'' map.
%

\subsection{You better make your residuals IID!}
In the statistical literature is a concept that repeatedly arises,
the idea that some statistical variables are IID, namely Independent, Identically Distributed.
In practice, we see many random-looking variables,
some much closer than others to IID.
Theoretically, the ID part of IID means the random variables come from Identical
probability Density functions.
In practice, the ID part mostly means the variables have the same variance.
The ``I'' before the ID means the variables are statistically Independent of one another.
Neighboring values should not be positively correlated, meaning low frequencies are present.
In the subject area of this book, signals, images, and Earth volumes,
the ``I'' before the ID means our residual spaces are white---have
all frequencies present in roughly equal amounts.
In other words the ``I'' means the statistical variables
have no significant correlation in time or space.
Chapter~\ref{paper:mda} gives a method of finding a
filter as a model styler (regularizer) that accomplishes this goal.
IID random variables have fairly uniform variance in both physical space and in Fourier space.
\par
\boxit{
IID random variables have uniform variance in both physical space and Fourier space.
}

\par
In a geophysical project,
it is important the residual between observed data and modeled data is not far from IID.
To raw residuals,
we should apply weights and filters to get IID residuals.
We minimize sums of squares of residuals.
If any residuals are small, the squares are tiny,
so such regression equations are effectively ignored.
We would hardly ever want residuals ignored.
Echo seismograms  get weak at late time.
\sx{seismology}
So,
even with a bad fit,
the difference between real and theoretical seismograms
is necessarily weak at late times.
We do not want the data at late times to be ignored.
So,
we boost up the residual there.
We choose $\bold W$ to be a diagonal matrix that boosts late times
in the regression $\bold 0 \approx \bold r = \bold W(\bold F\bold m-\bold d)$.
\par
An example with too much low (spatial) frequency in a residual might arise in a topographic study.
It is not unusual for the topographic wavelength to exceed the survey size.
Here, we should choose $\bold W$ to be a filter to boost up the higher frequencies.
Perhaps, $\bold W$ should contain a derivative or a Laplacian.
If you set up and solve a data-modeling problem
and then find $\bold r$ is not IID,
you should consider changing your $\bold W$.
Chapter~\ref{paper:mda} provides a systematic
approach to whitening residuals.
\par
Now, let us include regularization $\bold 0 \approx\bold A\bold m$
and a preconditioning variable $\bold p$.
We have our data-fitting goal and our model-styling goal;
the first with a residual $\bold r_d$
in data space, the second with a residual $\bold r_m$
in model space.
We have had to choose a regularization operator $\bold A = \bold S^{-1}$ and a scaling factor $\epsilon$.
\begin{eqnarray}
\label{eqn:tworegressions}
0 & \approx & \bold r_d
\ = \ \bold W (\bold F  \bold S \bold p -       \bold d) 
\ = \ \tilde{  \bold F} \bold S \bold p -\tilde{\bold d} 
\\
0 & \approx& \bold r_m  \ = \ \epsilon \ \bold p
\end{eqnarray}
This system of two regressions could be packed into one;
the two residual vectors stacked on top of each other,
likewise the operators $\bold F$ and $\epsilon \bold I$.
The IID notion seems to apply to this unified system
which gives us a clue as to
how we should have chosen the regularization operator $\bold A$.
Not only should $\bold r_d$ be IID, but also should $\bold r_m$---within a scale $\epsilon$, $\bold r_m=\bold p$.
Thus, the preconditioning variable is not simply something to speed computational convergence.
It is a variable that should be IID.
If it is not coming out that way, we should consider changing $\bold A$.
Chapter \ref{paper:mda} addresses the task of choosing an $\bold A$,
so $\bold r_m$ comes out IID.
\par
\boxit{We should choose a weighting function (and/or operator) $\bold W$,
so data residuals are IID.
We should also choose our regularization operator $\bold A= \bold S^{-1}$
so the preconditioning variable $\bold p$ comes out IID.}


\subsection{Choice of a unitless epsilon}
\sx{epsilon, unitless}
\sx{unitless epsilon}
The parameter epsilon $\epsilon$ strikes the balance between
our data-fitting goal and our model-styling goal.
These two regression systems typically have differing physical units;
therefore, the numerical value of $\epsilon$ is accidental,
for example comparing milliseconds to meters.
%\begin{eqnarray}
\begin{equation}
\label{eqn:epsilonstory}
\begin{array}{rcl}
\bold 0 \ &\approx&\   \bold r_d = \bold W (\bold F\bold S \bold p -\bold d)
\\
\bold 0 \ &\approx&\   \bold r_m = \epsilon \  \bold p
\end{array}\end{equation}
%\end{eqnarray}
The numerical value of $\epsilon$ is meaningless before
we learn to express the idea in a unitless (dimensionless) manner.
Without pretending we are doing physics,
let us use some of the language of thermodynamics,
a physical field that does deal with equilibria and random fluctuations.
Define an energy ratio $u$ and a volume ratio $v$ that can
be used to bring $\epsilon$ to unitless form.
Naturally, the square roots arise,
because we are minimizing quadratic functions of residuals.
$$
u = {\rm energy\ ratio} \quad=\quad \ \sqrt{
                           \frac{\bold r_d\cdot \bold r_d}
			        {\bold p\cdot\bold p}
			   }
$$

$$
v = {\rm volume\ ratio} \quad=\quad \ \sqrt{
			    \frac{n_{r_d}}{n_p}
			   }
$$
Can we really think of ``volume'' as related to
the number $n_p$ of components in the model space?  Perhaps.
Likewise the data space?  Less likely.
And, is the energy measure really an appropriate one?
Maybe.
What is the goal of these speculative thoughts?
The goal is to give you a starting numerical value for $\epsilon$,
say $\epsilon=1$.
Your final guide is your own experimental experience.
Try either one of these next two regressions:

\begin{eqnarray}
\bold 0 \ \approx \ 
\bold r_m &=& \epsilon_{\rm extrinsic} \ u \ \bold p
\\
\bold 0 \ \approx \ 
\bold r_m &=& \epsilon_{\rm intrinsic}\ (u/v) \ \bold p
\end{eqnarray}



\section{THE PRECONDITIONED SOLVER}
Summing up the previous ideas,
we start from fitting goals:
\begin{equation}
\begin{array}{lll}
\label{eq:main}
\bold 0 &\approx& \bold F \bold m \ -\  \bold d \\
\bold 0 &\approx& \bold A \bold m
\end{array}
\end{equation}
and we change variables from
$\bold m$ to $\bold p$ using
$\bold m = \bold A^{-1} \bold p$.
\begin{equation}
\begin{array}{llllcl}
\bold 0 &\approx &  \bold F \bold m \ -\  \bold d   &=&
    \bold F  \bold A^{-1} &\bold p  \ -\  \bold d
\\
\bold 0 &\approx &  \bold A \bold m       &=&   \bold I        & \bold p
\end{array}
\label{eqn:precsummary}
\end{equation}
Preconditioning means iteratively fitting
by adjusting the $\bold p$ variables
and then finding the model by using
$\bold m = \bold A^{-1} \bold p$.

\begin{comment}
You notice the following code allows for common additional features,
a weighting function on the data residuals,
starting $\bold p_0$,
masking constraints $\bold J$ on $\bold p$,
and scaling the regularization by an $\epsilon$.
\par
A new reusable
preconditioned solver is
the module \texttt{solver_prc} \vpageref{lst:solver_prc}.
%The variable \texttt{x} in \texttt{prec\_solver} refers to $\bold m$.
Likewise the modeling operator $\bold F$ is called \texttt{Fop}
and the smoothing operator $\bold A^{-1}$ is called \texttt{Sop}.
Details of the code are only slightly different from
the regularized solver
\texttt{solver-reg} \vpageref{lst:solver-reg}.

%REMEMBER TO PUT THE PROGRAM BACK IN HERE !
%\begin{notforlecture}
\moddex{solver_prc}{Preconditioned solver}
%\end{notforlecture}
\end{comment}



\section{OPPORTUNITIES FOR SMART DIRECTIONS}
Recall the fitting goals (\ref{eqn:tworegressions})
with weights $\bold W$
being absorbed into the operator $\bold F$ and the data $\bold d$.
\begin{equation}
\begin{array}{llllllcl}
\bold 0 &\approx& \bold r_d &=&  \bold F \bold m \ -\  \bold d   &=&
    \bold F  \bold A^{-1} &\bold p  \ -\  \bold d
    \\
\bold 0 &\approx& \bold r_m &=&  \bold A \bold m       &=&
    \bold I        & \bold p
\end{array}
\label{eqn:precsummary2}
\end{equation}
Without preconditioning, we have the search direction:
\begin{equation}
\Delta \bold m_{\rm bad} \eq
\left[
	\begin{array}{cc}
	\bold F\T & \bold A\T
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold r_d \\
	\bold r_m
	\end{array}
\right]
\end{equation}
and with preconditioning, we have the search direction:
\begin{equation}
\Delta \bold p_{\rm good} \eq
\left[
	\begin{array}{cc}
	(\bold F\bold A^{-1})\T & \bold I
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold r_d \\
	\bold r_m
	\end{array}
\right]
\end{equation}
\par
The essential feature of preconditioning is not that we perform
the iterative optimization in terms of the variable $\bold p$.
The essential feature is that we use a search direction
that is a gradient with respect to $\bold p\T$ not $\bold m\T$.
Using $\bold A\bold m=\bold p$, we have
$\bold A\Delta \bold m=\Delta \bold p$,
which enables us to define a good search direction in $\bold m$ space.
\begin{equation}
\Delta \bold m_{\rm good} \eq \bold A^{-1}
\Delta \bold p_{\rm good} \eq
	\bold A^{-1} (\bold A^{-1})\T
	\bold F\T \bold r_d + \bold  A^{-1} \bold r_m
\end{equation}
Define the gradient by $\bold g=\bold F\T\bold r_d$, and
notice that $\bold r_m=\bold p$.
\begin{equation}
\Delta \bold m_{\rm good} \eq
	\bold A^{-1} (\bold A^{-1})\T \ \bold g
%	\frac{ \bold g}{\bold A \bold A\T}
	+ \bold m
\label{eqn:newdirection}
\end{equation}

\par
The search direction (\ref{eqn:newdirection}) 
shows a positive-definite operator scaling the gradient.
Each component of any gradient vector is independent of each other.
All independently point (negatively) to a direction for descent.
Obviously, each can be scaled by any positive number.
Now, we have found that we can also scale a gradient vector by
a positive definite matrix, and we can still expect
the conjugate-direction algorithm to descend, as always,
to the ``exact'' answer in a finite number of steps.
The reason is that modifying the search direction with
$ \bold A^{-1} (\bold A^{-1})\T$ is equivalent to solving
a conjugate-gradient problem in $\bold p$.
We'll see in Chapter~\ref{paper:mda}, that
our specifying $ \bold A^{-1} (\bold A^{-1})\T$
amounts to us specifying
a prior expectation
of the spectrum of the model $\bold m$.

%\subsubsection{Implications for better preconditioning}
%\par
%The search direction (\ref{eqn:newdirection})
%brings us both good news and bad news.
%First the bad news.
%Equation (\ref{eqn:newdirection}) looks like something new
%but it will probably turn out to be equivalent to all the
%preconditioning work we have done with the operator $\bold A^{-1}$
%being polynomial division.
%Actually, we do polynomial division
%followed by truncation of the infinite series.
%Because of the truncation we really do not properly construct
%$ \bold A^{-1} (\bold A^{-1})\T$, and for this we can expect
%some difficulty at one boundary.

%\par
%The good news is that we now see new preconditioning opportunities.
%Formerly we began from a preconditioning 
%operator $\bold A^{-1}$ but now we see that we really only
%need its spectrum $\bold A^{-1} (\bold A^{-1})\T$.
%That might not be any real advantage since our basic representation
%of a positive definite matrix is any matrix times its transpose.
%Anyway, we can conclude that the operator $\bold A$
%may be chosen as any cascade of weighting and filtering operators.
%
%\par
%We are also reminded that what we really need
%is not simply $\bold A^{-1} (\bold A^{-1})\T$
%but $(\bold F\T\bold F+\bold A\T\bold A)^{-1}$.
%Likewise we imagine this being built of
%any cascade of weighting and filtering operators
%followed by the adjoint.

\subsection{The meaning of the preconditioning variable $\bold p$}

To accelerate convergence of iterative methods, we often change variables.
The model-styling regression $\bold 0 \approx \epsilon \bold A \bold m$
is changed to $\bold 0 \approx \epsilon \bold p$.
Experience shows, however, that the variable $\bold p$ is often more interesting
to look at than the model $\bold m$.
Why should a new variable introduced for computational convenience
turn out to have more interpretive value?
There is a little theory explaining why.  Begin from
\begin{eqnarray}
\bold 0 &\approx& \bold W (\bold F \bold m -\bold d)
\\
\bold 0 &\approx& \epsilon\, \bold A \bold m
\end{eqnarray}
Introduce the preconditioning variable $\bold p$.
\begin{eqnarray}
\bold 0 &\approx& \bold W (\bold F \bold A^{-1}\bold p -\bold d)
\\
\bold 0 &\approx& \epsilon\, \bold p
\end{eqnarray}
Rewriting as a single regression:
\begin{equation}
\label{eqn:singlereg}
\bold 0 
\quad\approx\quad
\left[
\begin{array}{c}
\bold r_d \\
\bold r_m
\end{array}
\right]
\quad=\quad
\left[
\begin{array}{c}
\bold W\bold F\bold A^{-1} \\
\epsilon\, \bold I
\end{array}
\right]
\bold p
\quad - \quad
\left[
\begin{array}{c}
\bold W \bold d \\
\bold 0
\end{array}
\right]
\end{equation}
The gradient vanishes at the best solution.
To get the gradient,
we put the residual into the adjoint operator.
Thus,
we put the residuals (column vector) in
(\ref{eqn:singlereg})
into the transpose of the operator in
(\ref{eqn:singlereg}),
the row $( (\bold W\bold F\bold A^{-1})\T, \epsilon \bold I$).
Finally,
replace the $\approx$ by $=$.
Thus,
\begin{eqnarray}
\nonumber
\bold 0 &=& (\bold W\bold F\bold A^{-1})\T\,\bold r_d + \epsilon \, \bold r_m 
\\
\label{eqn:fight}
\bold 0 &=& (\bold W\bold F\bold A^{-1})\T\,\bold r_d + \epsilon^2 \, \bold p
\end{eqnarray}
The two terms in Equation (\ref{eqn:fight})
are identical but oppositely signed.
These terms represent images in model space.
This image represents the fight between
the data space residual and the model space residual.
You really do want to plot this image.
It shows the battle of
the model wanted by the data
against
our preconceived statistical model expressed by our model styling goal.
That is why the preconditioned variable $\bold p$ is interesting to inspect and interpret.
It is not simply a computational convenience.
It is telling you what you have learned from data
(that someone has recorded at great expense!).
\par
\boxit{
The preconditioning variable $\bold p$ is not simply a computational convenience.
This model-space image $\bold p$ tells us where our data contradicts our prior model.
Admire it!
Make a movie of it evolving with iteration.
}

\par
If I were young and energetic like you,
I would write a new basic tool for optimization.
Instead of scanning only the space of the gradient and previous step,
it would scan also over the ``smart'' direction.  
Using both directions should offer the benefit of preconditioning
the regularization at early iterations
while offering more assured fitting data at late iterations.
The improved 
module for \texttt{cgstep}
%\vpageref{/prog:cgstep}
would need to solve a $3\times 3$ matrix.
I would also be looking for ways to
assure all $\Delta\bold m$ directions
were scaled to have the prior model spectrum and prior energy function of space.


\subsection{Need for an invertible preconditioner}
It is important to use regularization to solve many examples.
It is important to precondition,
because in practice,
computer power is often a limiting factor.
It is important to be able to begin from a nonzero starting solution,
because in nonlinear problems we must restart from an earlier solution.
Putting all three requirements together leads to a little problem.
It turns out the three together lead us to needing 
a preconditioning transformation that is invertible.
Let us see why this is so.
\begin{equation}
\begin{array}{lll}
\label{eq:main2}
\bold 0 &\approx & \bold F \bold m \ -\  \bold d \\
\bold 0 &\approx & \bold A \bold m
\end{array}
\end{equation}
First,
we change variables from $\bold m$ to $\bold u = \bold m - \bold m_0$.
Clearly,
$\bold u$ starts from $\bold u_0=0$, and $\bold m = \bold u + \bold m_0$.
Then,
our regression pair becomes:
\begin{equation}
\begin{array}{lll}
\label{eq:main3}
\bold 0 &\approx & \bold F \bold u \ +\  (\bold F\bold m_0 -\bold d) \\
\bold 0 &\approx & \bold A \bold u \ +\  \bold A \bold m_0
\end{array}
\end{equation}
This result differs from the original regression in only two minor ways,
(1) revised data, and (2) a little more general form of the regularization,
the extra term $\bold A \bold m_0$.
\par
Now,
let us introduce preconditioning.
From the regularization,
we see preconditioning introduces the preconditioning variable $\bold p = \bold A\bold u$.
Our regression pair becomes:
\begin{equation}
\begin{array}{lll}
\label{eq:main4}
\bold 0 &\approx & \bold F \bold A^{-1} \bold p \ +\  (\bold F\bold m_0 -\bold d) \\
\bold 0 &\approx &                      \bold p \ +\  \bold A \bold m_0
\end{array}
\end{equation}
Here is the problem:
We now require both $\bold A$ and $\bold A^{-1}$ operators.
In 2- and 3-dimensional spaces, we do not know very many operators
with an easy inverse.
That reason is why I found myself pushed to come up with the helix methodology 
of Chapter \ref{paper:hlx}---because it provides invertible operators for
smoothing and roughening.



\section{INTERVAL VELOCITY}
\sx{interval velocity}\sx{velocity, interval}
A bread-and-butter problem in seismology is building the velocity
as a function of depth (or vertical travel time)
starting from certain measurements.
The measurements are described in many books, for example
my book
{\em BEI} (Basic Earth Imaging).
They amount to measuring the integral of the velocity squared
from the surface down to the reflector,
known as the root-mean-square (RMS) velocity.
Although good quality echoes may arrive often,
they rarely arrive continuously for all depths.
Good information is interspersed unpredictably with poor information.
Luckily, we can
also estimate
the data quality by the ``coherency'' or the
``stack energy.''
In summary, what we get from observations and preprocessing
are two functions of travel-time depth:
(1) the integrated (from the surface) squared velocity, and
(2) a measure of the quality of the integrated velocity measurement.
Needed definitions are as follows:
\begin{description}
\item  [$\bold d$]
is a data vector in which components range over the vertical
traveltime depth $\tau$.
Its component values contain the scaled RMS velocity squared
$\tau v_{\rm RMS}^2/\Delta \tau $,
where
$\tau /\Delta \tau $ is the index on the time axis.
\item [$\bold W$]
is a diagonal matrix along which we lay the given measure
of data quality.  We use it as a weighting function.
\item  [$\bold C$]
is the matrix of causal integration, a lower triangular matrix of ones.
\item  [$\bold D$]
is the matrix of causal differentiation, namely, $\bold D=\bold C^{-1}$.
\item [$\bold u$]
is a vector containing the interval velocity squared $v_{\rm interval}^2 $
ranging over the vertical traveltime depth $\tau$.
\end{description}
From these definitions,
under the assumption of a stratified Earth with horizontal reflectors
(and no multiple reflections),
the theoretical (squared) interval velocities
enable us to define the theoretical (squared) RMS velocities by:
\begin{equation}
\bold C\bold u \eq \bold d 
\end{equation}
In other words, any component of $\bold d_i$ measures
the integral of a material property from the Earth surface to the depth of $i$.
We wish to find the material property everywhere, which is $\bold u$.
If we integrate it from the surface downward with causal integration $\bold C$,
we should get the measurements $\bold d$.
\par
With imperfect data, our data fitting goal is to minimize the residual:
\begin{equation}
\bold 0
\quad\approx\quad
\bold W
\left[
\bold C\bold u
-
\bold d
\right]
\end{equation}
where $\bold W$ is some weighting function,
we need to choose.
To find the interval velocity
where there is no data (where the stack power theoretically vanishes),
we have the ``model damping'' goal to minimize
the wiggliness $\bold p$
of the squared interval velocity $\bold u$.
\begin{equation}
\bold 0
\quad\approx\quad
\bold D \bold u \eq \bold p
\end{equation}
\par
We precondition these two goals
by changing the optimization variable from
interval velocity squared
$\bold u$ to its wiggliness $\bold p$.
Substituting $\bold u=\bold C\bold p$ gives the two goals
expressed as a function of wiggliness $\bold p$.
\begin{eqnarray}
\label{eqn:model}
\bold 0
&\approx&
\bold W
\left[
\bold C^2\bold p
-
\bold d
\right]
\\
\bold 0
&\approx&
\epsilon \, \bold p
\end{eqnarray}
\subsection{Balancing good data with bad}
\inputdir{bob}
Choosing the size of $\bold \epsilon$ chooses
the stiffness of the curve that connects regions of good data.
Our first test cases gave solutions
we interpreted to be
too stiff at early times and too flexible at later times,
which suggests we weaken $\epsilon$ at early times and strengthen it later.
Because we wanted to keep $\epsilon$ constant with time,
we strengthened $\bold W$ at early times
and weakened it at later times
as you see in the following program:

\plot{clapp}{width=\textwidth,height=.5\textwidth}{
        Raw CMP gather (left),
        semblance scan (middle),
        and semblance value used for weighting function (right).
        %\viewit{clapp}
	}


\plot{stiff}{width=\textwidth,height=.5\textwidth}{
  Observed RMS velocity
  and that predicted by a stiff model with
  $\epsilon=4$.
  (Clapp)
}

\plot{flex}{width=\textwidth,height=.5\textwidth}{
  Observed RMS velocity
  and that predicted by a flexible model with
  $\epsilon=.25$
  (Clapp)
}

%\subsection{Bigsolver}
%The regression (\ref{eqn:model}) includes a weighting function,
%so we need yet another solver module
%very much like the regularized and preconditioned solvers
%that we used earlier.
%The idea is essentially the same.
%Instead of preparing such a solver here,
%I refer you to the end of the book for
%\texttt{solver\_mod} \vpageref{lst:bigsolver},
%a big solver that incorporates everything
%that we need in the book.
%Hopefully we will not need to look at solvers again for a while.
%Module \texttt{vrms2int} \vpageref{lst:vrms2int}
%was written by Bob Clapp to get the results
%in Figures \ref{fig:clapp} to \ref{fig:flex}.
%Notice that he is using
%\texttt{solver\_mod} \vpageref{lst:bigsolver}.

\begin{comment}
\moddex{vrms2int}{Converting RMS to interval velocity}
\end{comment}

\subsection{Lateral variations}
The previous analysis appears 1-dimensional in depth.
Conventional interval velocity estimation builds a velocity-depth model
independently at each lateral location.
Here, we have a logical path for combining measurements
from various lateral locations.
We can change the regularization
to something like $\bold 0\approx \nabla\bold u$.
Instead of merely minimizing the vertical gradient of velocity,
we minimize its spatial gradient.
Luckily, we have preconditioning and the helix to speed the solution.
\par
%Likewise the construction of the data quality screen $\bold G$
%would naturally involve the full three-dimensional setting.

\subsection{Blocky models}
\inputdir{.}

Sometimes, we seek a velocity model that increases smoothly
with depth through our scattered
measurements of good-quality RMS velocities.
Other times, we seek a blocky model.
(Where seismic data is poor,
a well log could tell us whether or not to choose smooth or blocky.)
Here, we see an estimation method that can choose the blocky alternative,
or some combination of smooth and blocky.

\par
Consider the five-layer model in Figure \ref{fig:rosales}.
Each layer has unit traveltime thickness
(so integration is simply summation).
Let the squared interval velocities be $(a,b,c,d,e)$
with strong reliable reflections at the base of layer $c$ and layer $e$,
and weak, incoherent, ``bad'' reflections at bases of $(a,b,d)$.
Thus, we measure $V_c^2$ the RMS velocity squared of the top three layers
and $V_e^2$ for all five layers.
Because we have no reflection from at the base of the fourth layer,
the velocity in the fourth layer is not measured but a matter for choice.
In a smooth linear fit, we would want $d=(c+e)/2$.
In a blocky fit, we would want $d=e$.

\sideplot{rosales}{width=0.5\textwidth}{
  A layered Earth model.
  The layer interfaces cause reflections.
  Each layer has a constant velocity in its interior.
}


\par
Our screen for good reflections looks like $(0,0,1,0,1)$,
and our screen for bad ones looks like the complement $(1,1,0,1,0)$.
We put these screens on the diagonals of diagonal matrices
$\bold G$ and $\bold B$.
Our fitting goals are:
\begin{eqnarray}
3V_c^2 &\approx& a+b+c
\\
5V_e^2 &\approx& a+b+c+d+e
\\
u_0 &\approx& a
\\
0 &\approx& -a+b
\\
0 &\approx& -b+c
\\
0 &\approx& -c+d
\label{eqn:block}
\\
0 &\approx& -d+e
\end{eqnarray}
For the blocky solution, we do not want the fitting goal (\ref{eqn:block}).
Further explanations await completion of examples.

%We can remove it by multiplying the model goals by a diagonal-matrix
%badpass screen $\bold B$ 
%eliminating the goal $0 \approx -c+d$.
%In abstract, our fitting goals become
%\begin{eqnarray}
%\bold 0 &\approx& \bold r \eq \bold C \bold u  - \bold d
%\\
%\bold 0 &\approx& \bold p \eq \bold B\bold D\bold u - \bold u_0
%\end{eqnarray}
%where $\bold u_0$ is a zero vector with a top component of $u_0$.
%Since $\bold B$ is not invertable,
%we cannot backsolve the preconditioned variable $\bold p$ for
%the squared interval velocity
%$\bold u= \bold D^{-1}\bold B^{-1}(\bold p + \bold u_0)$.
%Instead, we use $\bold G$ for $\bold B^{-1}$
%thus redefining
%the implicit relationship for $\bold u$.
%\begin{equation}
%\bold u \eq \bold u_0 + \bold D^{-1}\bold G\bold p
%\label{eqn:ufromp}
%\end{equation}
%where $\bold G$ is the goodpass screen.
%Since $\bold D^{-1}=\bold C$ the fitting goals become
%
%
%\begin{equation}
%\begin{array}{lll}
%\bold 0 &\approx& \bold r \eq \bold C^2\bold G \bold p
%+\bold C \bold u_0 
%-\bold d
%\\
%\bold 0 &\approx& \bold p
%\end{array}
%\label{eqn:logical}
%\end{equation}
%After fitting with $\bold p$,
%we define the squared interval-velocity
%$\bold u$ using (\ref{eqn:ufromp}).
%
%\par
%The formulation
%(\ref{eqn:logical})
%is so logical that we might have guessed it:
%The goal $\bold 0 \approx \bold p $ says that $\bold p$ is mostly zero.
%What emerges from $\bold G$ is a sprinkling of impulses.
%Then $\bold C^2$ converts the pulses to ramp functions
%(zero until a certain place, then growing linearly)
%which are used to fit the data (integrated velocity).
%Differentiating the data-fitting ramps
%converts them to the desired blocks of constant velocity.
%One iteration is required for each impulse.
%
%\par
%Choosing $\bold G$ to be an identity $\bold I$ gives smooth velocity models,
%such as caused by the increasing consolidation of the rocks with depth.
%Choosing the screen $\bold G$ to have a sprinkling of pass locations
%picks the boundaries of blocks of constant velocity.
%The choice can be made by people with subjective criteria (like geologists)
%or we can assist by using the data itself
%in various ways to select our degree of preference between
%the blocky and smooth models.
%For example,
%we could put seismic coherency or amplitude
%on the goodpass diagonal matrix $\bold G$.
%Clearly, much remains to be gained from experience.
%



%\begin{notforlecture}



\section{INVERSE LINEAR INTERPOLATION}
\inputdir{sep94}
\sideplot{data}{width=.8\textwidth,height=.3\textwidth}{
  The input data are irregularly sampled.
}
The first example is a simple synthetic test for 1-D inverse
interpolation. The input data were randomly subsampled (with
decreasing density) from a sinusoid (Figure \ref{fig:data}). The
forward operator $\bold L$ in this case is linear interpolation. We seek
a regularly sampled model that could predict the data with a
forward linear interpolation. Sparse irregular distribution of the
input data makes the regularization enforcement a necessity.
I applied convolution with the simple $(1,-1)$
difference filter as the operator $\bold D$ that forces model continuity
(the first-order spline).
An appropriate preconditioner $\bold S$ in this
case is recursive causal integration. 

%Figures \ref{fig:im1} and
%\ref{fig:fm1} show the results of inverse interpolation after
%exhaustive 300 iterations of the conjugate-direction method.
%As a result of using the causal integration for preconditioning,
%the rightmost part of the model in the data-space case stays at a
%constant level instead of decreasing to zero. If we specifically
%wanted a zero-value boundary condition, it wouldn't be difficult to
%implement it by adding a zero-value data point at the boundary.

%\sideplot{im1}{width=3in,height=1.5in}{ Estimation of a continuous
%  function b regularization. The regularization operator $\bold A$ is
%  the derivative operator (convolution with $(1,-1)$).}

%\sideplot{fm1}{width=3in,height=1.5in}{Estimation of a continuous
%  function by preconditioning model regularization. The
%  preconditioning operator $\bold P$ is causal integration.}

\plot{conv}{width=\textwidth,height=1.16\textwidth}{
  Convergence history of inverse
  linear interpolation. Left: regularization, right: preconditioning.
  The regularization operator $\bold A$ is
  the derivative operator (convolution with $(1,-1)$. The
  preconditioning operator $\bold S$ is causal integration.}

%\plot{conv2}{width=6in,height=7in}{Convergence history of inverse
%  linear interpolation. Left: regularization, right: preconditioning.
%  The regularization operator $\bold A$ is
%  the second derivative operator (convolution with $(1,-2,1)$. The
%  preconditioning operator $\bold P$ is the corresponding inverse filtering.}

As expected,
preconditioning provides a much faster rate of convergence.
Because iteration to the exact solution
is never achieved in large-scale problems,
the results of iterative optimization may turn out quite differently.
Bill Harlan points out that the two goals
in (\ref{eq:main}) conflict with each other:
the first one enforces ``details'' in the model,
while the second one tries to smooth away the details.
Typically, regularized optimization creates
a complicated model at early iterations.
At first, the data-fitting goal (\ref{eq:main}) plays a more important role.
Later, the styling goal (\ref{eq:main}) comes into play
and simplifies (smooths) the model as much as needed.
Preconditioning acts differently.
The very first iterations create a simplified (smooth) model.
Later, the data-fitting goal adds more details into the model.
If we stop the iterative process early,
we end up with an insufficiently complex model,
not an insufficiently simplified one.
Figure \ref{fig:conv} provides a clear illustration of Harlan's observation.

\par
Figure \ref{fig:schwab1}
measures the rate of convergence by the model residual,
which is a distance from the current model to the final solution.
It shows that preconditioning saves many iterations.
Because the cost of each iteration for each method is roughly equal,
the efficiency of preconditioning is evident.
\sideplot{schwab1}{width=.8\textwidth}{
  Convergence of the iterative optimization,
  measured in terms of the model residual.
  The ``p'' points stand for preconditioning;
  the ``r'' points,
  regularization.
}

\begin{comment}
\par
The module \texttt{invint2} \vpageref{lst:invint2}
invokes the solvers to make
Figures \ref{fig:conv}
and
\ref{fig:schwab1}.
We use convolution with
\texttt{helicon} \vpageref{lst:helicon}
for the regularization
and we use deconvolution with
\texttt{polydiv} \vpageref{lst:polydiv}
for the preconditioning.
The code looks fairly straightforward except for
the oxymoron
\texttt{known=aa\%mis}.

\moddex{invint2}{Inverse linear interpolation}        
\end{comment}

%\par
%This example suggests that the philosophy of image creation by
%optimization has a dual orthonormality: First, Gauss (and common
%sense) tells us that the data residuals should be roughly equal in
%size.  Likewise in Fourier space they should be roughly equal in size,
%which means they should be roughly white, i.e. orthonormal.  (I use
%the word ``orthonormal'' because white means the autocorrelation is an
%impulse, which means the signal is statistically orthogonal to shifted
%versions of itself.)  Second, to speed convergence of iterative
%methods, we need a whiteness, another othonormality, in the solution.
%The map image, the physical function that we seek, might not be itself
%white, so we should solve first for another variable, the whitened map
%image, and as a final step, transform it to the ``natural colored''
%map.
%
%\par
%Often geophysicists create a preconditioning matrix $\bold B$ by
%inventing columns that ``look like'' the solutions that they seek.
%Then the space $\bold x$ has many fewer components than the space of
%$\bold m$.  This approach is touted as a way of introducing geological
%and geophysical prior information into the solution.  Indeed, it
%strongly imposes the form of the solution.  Perhaps this approach
%deserves the diminutive term ``curve fitting'' instead of the
%grandiloquent ``geophysical inverse theory.''  Our preferred approach
%is not to invent the columns of the preconditioning matrix, but to
%estimate the prediction-error filter of the model and use its inverse.
%



\section{EMPTY BINS AND PRECONDITIONING}
There are at least three ways to fill empty bins.
Two require a roughening operator $\bold A$ while
the third requires a smoothing operator, which
(for comparison purposes) we denote $\bold A^{-1}$.
The three methods are generally equivalent
though they differ in significant details.

\par
The original way in
Chapter \ref{paper:iin} is to
restore missing data
by ensuring the restored data,
after specified filtering,
has minimum energy, say
$\bold A\bold m\approx \bold 0$.
Introduce the selection mask operator $\bold K$, 
a diagonal matrix with
ones on the known data and zeros elsewhere
(on the missing data).
Thus,
$ \bold 0 \approx \bold A(\bold I-\bold K+\bold K)\bold m $ or:
\begin{equation}
\bold 0 \quad\approx\quad
\bold A (\bold I-\bold K) \bold m
\ +\ 
\bold A \bold m_k\;,
\label{eqn:style0}
\end{equation}
where we define $\bold m_k$ to be the data
with missing values set to zero by 
$\bold m_k=\bold K\bold m$.

\par
A second way to find missing data is with the set of goals:
\begin{equation}
\begin{array}{rrrrr}
\bold 0 & \approx &          \bold K \bold m  & - & \bold m_k \\
\bold 0 & \approx & \epsilon \bold A \bold m  &   &
\end{array}
\label{eqn:style1}
\end{equation}
and take the limit as the scalar $\epsilon \rightarrow 0$.
At that limit, we should have the same result
as equation (\ref{eqn:style0}).

\par
There is an important philosophical difference between
the first method and the second.
The first method strictly honors the known data.
The second method acknowledges that when data misfits
the regularization theory, it might be the fault of the data,
so the data need not be strictly honored.
Just what balance is proper falls to the numerical choice of $\epsilon$,
a nontrivial topic.

\par
A third way to find missing data is to precondition
equation (\ref{eqn:style1}),
namely, try the substitution 
$\bold m = \bold A^{-1} \bold p $.
\begin{equation}
\begin{array}{rrrrr}
\bold 0 & \approx & \bold K \bold A^{-1} \bold p  &-& \bold m_k \\
\bold 0 & \approx & \epsilon             \bold p  & &
\end{array}
\label{eqn:style2}
\end{equation}
There is no simple way of knowing beforehand
what is the best value of $\epsilon$.
Practitioners like to see solutions for various values of $\epsilon$.
Of course, that can cost a lot of computational effort.
Practical exploratory data analysis is more pragmatic.
Without a simple clear theoretical basis,
analysts generally begin from $\bold p=0$
and abandon the fitting goal $\epsilon \bold I \bold p\approx 0$.
Implicitly, they take $\epsilon=0$.
Then, they examine the solution as a function of iteration,
imagining that the solution at larger iterations
corresponds to smaller $\epsilon$.
There is an eigenvector analysis
indicating some kind of basis for this approach,
but I believe there is no firm guidance.

\subsection{SeaBeam}
\inputdir{seabeam}
Figure \ref{fig:seaprc} shows an image of deep seawater bottom
in the Pacific of a sea-floor spreading center produced acoustically
by a device called SeaBeam.
Students here tried all three methods of filling empty bins on the this data
using the Laplacian as a regularizer.
From an interpretive point of view,
differences among the three methods were minor and as expected,
therefore, only one is shown
in Figure \ref{fig:seaprc}.
\plot{seaprc}{width=\textwidth,height=.5\textwidth}{
	Seabeam data
	before and after
	empty bin filling with a laplacian.
	}


\section{GIANT PROBLEMS}
This book does not solve giant problems,
but it does solve personal-computer-sized problems
in the manner of giant problems.
There is big money in solving giant problems.
Big money brings specialist solutions beyond the scope of this book.
But let us take a look.
Closest to me is the seismic survey industry.
\sx{seismology}
Model space is 3-D, a cube, roughly a thousand 2-D screen fulls,
a screen full being roughly 1,000$\times$1,000,
a gigabyte in total.
Data space is 5-dimensional.
A seismogram is a thousand time points.
Our energy source lies in two dimensions on the Earth surface plane,
as do our receivers.
1+2+2=5.
All this compounds roughly to 1,000 to the $5^{\rm th}$ power,
a thousand terabytes, a petabyte.
Fully convergent solutions needing $10^{15}$ iterations
of operators is ridiculous, while more than a handful are nearly so.
We think mainly of using only the adjoint.
Theory and experimentation offer some guidance.
Remember that adjoints are great when unitary (already an inverse).
Adjoints are improved if they can be made more unitary.
The basic strategy for improving an adjoint is
finding one good diagonal-weighting function before the adjoint and another after it.
Recalling ``IID,'' adjoints are also made more unitary
by filter matrices that have the effect of whitening output.
Simple filter matrices are the gradient and the Laplacian.
More generally,
a compact way to whiten spectra
is multidimensional \bx{autoregression},
expounded in Chapter \ref{paper:mda}.

\subsection{A hundred iterations}
Lurking in every giant problem are many problems of smaller size.
In the large-scale seismic imaging problem
lie problems of velocity estimation, multiple-reflection elimination, and many more.
\par
Envision a large problem feasible in a hundred iterations.
Many of my colleagues work on such problems.
Maybe half would also use exotic parallel computer architectures.
Those with ample energy and intellectual capacity to tackle such machines
are rewarded by speed-up factors of 10 to a 100,
rewarded also by a diverse population of industries hiring.
This skill stays in demand because new architectures
rapidly obsolete earlier generations.
The other half, people like me, have the luxury
of software (like in this book) decaying at a slower pace,
leaving us needed time to tune our imaginations
to extracting the structure of more complex problems.
\par
It is a giant leap of faith that we can accomplish something of value
with a mere hundred iterations in a task
that theoretically demands quadrillions.
Experience shows that we often do,
and we do so by experimenting with ``intuitive'' methods.
The first,
I shall call ``faking the epsilon.''



\subsection{Faking the epsilon}\sx{epsilon, faking}

Burdened by a problem of oppressive size,
any trick, honest or sly, is nice to know.
I will tell you a trick that is widely used.
Many studies are done ignoring (abandoning)
the model styling regression (second fitting regression as follows):
\begin{equation}
\begin{array}{lll}
\label{eq:main5}
\bold 0 &\approx & \bold F \bold A^{-1} \bold p \ -\   \bold d \\
\bold 0 &\approx &           \epsilon \   \bold p 
\end{array}
\end{equation}
Because we have a numerically poor idea of what epsilon should be,
it is nice to be rid of it.
The pragmatic person iterates the data-fitting regression only,
watches the solution as a function of iteration,
and stops when tired
or (more hopefully) stops at the iteration that is subjectively most pleasing.
The epsilon-faking trick does not really speed things.
But,
it eliminates the need for scan over epsilon.
It also simplifies the coding  (insert smiley emoticon).
\par
Why does this crude approximation seem to work?
The preconditioner is based on an analytic solution
($\bold A^{-1}$ is an inverse)
to the regularization,
so naturally, early iterations tend to already fit the regularization,
so early iterations are struggling instead to fit the data.
The longer you run though, the better the data fit,
and the more the actual regularization should be coming into play.
But ongoing research often fails to run that far.
\par
Figure \ref{fig:conv} shows the idea
that early iterations fit the straight lines.
Straight lines honor the preconditioner.
At later iterations the data fits better.
Why do straight-line solutions honor the regularization?
Refer to the discussion near Figure \ref{fig:im1-1a90}.

\subsection{When preconditioning becomes a liability}
Theoretically, preconditioning does not reduce the number of iterations
required for an exact solution,
but it gets us closer quicker;
so,
we may hope to omit all the work of the later iterations.
Surprisingly and unfortunately,
several of my colleagues have observed
later iterations in which preconditioning actually slows convergence.
Then,
we are better off reverting to
the nonpreconditioned initial form.
Sorry, but I am unable to offer guidance
or any method to cope with this issue
other than your own application-dependent experimentation.



\subsection{Earthquake depth illustrates a null space}\sx{earthquake}\sx{null space}\sx{starting guess}
 
In the dawn of the era of computerized earthquake seismology,
\sx{seismology, earthquake}
someone decided to add earthquake depth to their catalog.
Traditionally, they had solved for only three unknowns,
latitude, longitude, and time of source at the source, i.e., origin time.
Now,
they would add a fourth, the depth.
They wrote down the $4\times 4$ system of equations and solved it.
Erratic results.
So then,
they froze the depth at zero,
solved for the old three variables;
only then introducing the depth.
Problem solved.
(Compared to seismograph station separation, zero depth is an excellent approximation.)

I first understood the earthquake experience
as an issue with nonlinear problems.
True that earthquake travel time is not a linear function of distance,
so the nonlinearity could lead to difficulty.
But,
something more is going on.
When all seismometers are far from the earthquake,
the waves arrive propagating nearly vertically (Earth curvature and $v(z)$ ray bending).
Source depth affects such data in much the same way as time origin shift,
so they are near a null space.
Whenever near a null space,
especially with a nonlinear problem,
a good starting solution is needed.


\subsection{The starting solution matters!}
\sx{starting solution matters}\sx{null space}
In principle, regularization solves the null-space problem,
but that is only for those people lucky enough to have applications
so small they can afford to iterate to completion.
Think of this trivial 2-D null-space situation:
A parabolic penalty on one spatial axis
with no penalty on the other axis.
Imagine a house facing northeast with a parabolic rain gutter mounted perfectly
horizontally on one edge of the house roof.
The null space is anywhere on the center line along the bottom of the gutter.
Anywhere you begin, steepest descent brings you immediately to the gutter bottom
in a location that depends on where you began.
Now tilt the gutter a little bit so the water drips off one end of the rain drain.
Steepest descent now overshoots a little so,
as we saw in Chapter \ref{paper:lsq},
a tortuous path of right-angle turning ensues.
(Recall Figure \ref{fig:yunyue}.)
The conjugate direction method quickly solves this trivial 2-D problem,
but in a 150,000 dimensional lake bottom problem,
conjugate directions taken only a few dozen iterations do not do as well.
When the data-modeling operator contains a null space,
only the regularization can pull us away from it,
and a small number of iterations may be unable to do the job.
So,
we need a good starting location.
\par

\boxit{
	Textbook theory may tell us final solutions are independent of the starting location,
	but we learn otherwise from nonlinear problems,
	and we learn otherwise from linear but large problems.
	}


\subsection{Null space versus starting solution}
\sx{null space}
The simplest null-space problem has one data point $d$ emerging from two model points.
\begin{equation}
\label{eqn:fullblown}
d \quad \approx \quad
\left[
	\begin{array}{cc}
		a & b
	\end{array}
\right]
\
\left[
	\begin{array}{c}
		x\\y
	\end{array}
\right]
\end{equation}

\par\noindent
The null space is any solution that produces no data.
You can add an arbitrary amount $\beta$ of the null space
getting another solution as good as the first.
Here is the full solution.
\begin{equation}
\left[
	\begin{array}{r}
		x\\y
	\end{array}
\right]
\quad \approx  \quad
\frac{d}{2ab}
\left[
	\begin{array}{r}
		b\\ a
	\end{array}
\right]
\ + \
\beta\,
\left[
	\begin{array}{r}
		-b\\a
	\end{array}
\right]
\end{equation}

Iterative methods can neither subtract nor add any null space
to your initial solution.
It is obvious in this simple case,
because the gradient
(here the matrix adjoint)
dotted into the null-space vector vanishes.
Suppose $a$ and $b$ are matrices,
while $d$, $x$, and $y$ are vectors.
Although more complicated, something similar happens.
You can test if an application involves a null space
by comparing the results of various starting solutions.

\par
Other traps arise in the world of images.
Rarely are we able to iterate to full completion,
so we might say, ``practically speaking, this application has null spaces.''
For example, if we know that zero frequency is theoretically
a null space, we would say,
``The null space contains low frequencies.''
We cannot avoid such issues.

\par
The textbook way of dealing with null spaces is to require
the researcher to set up model styling goals (regularizations).
Finding such goals demands assumptions from the researcher,
assumptions that are often hard to specify.
Luckily,
there is another path to consider.
%We could choose the initial solution more carefully.
Thinking more like a physicist, we could choose the initial solution more carefully.

In regression
(\ref{eqn:fullblown})
extended to images,
we might hope not to have a null-space problem
when we begin iterating from
$(\bold x,\bold y) =(\bold 0,\bold 0)$,
but this is not true.
It is a pitfall,
which in an application context,
took me some years to recognize.
Notice what happens the first step you move away from
$(\bold x,\bold y) =(\bold 0,\bold 0)$.
Your solution becomes a constant $\beta$ times the gradient.
The image extension of (\ref{eqn:fullblown})
being:
\begin{equation}
\label{eqn:firststep}
\left[
	\begin{array}{c}
		\bold x \\ \bold y
	\end{array}
\right]
\quad=\quad
\beta\ 
\left[
	\begin{array}{c}
		\bold A\T \bold d
		\\
		\bold B\T \bold d
	\end{array}
\right]
\end{equation}
If the operators
$\bold A$ and
$\bold B$ resemble filters,
it is pretty clear that
$\bold x$ and $\bold y$ are correlated,
which physically could be nonsense.
We might be trying to discover if and how
$\bold x$ and $\bold y$ are correlated.
Or we might wish to demand they be uncorrelated.

\par
I have no general method for you,
but offer a suggestion that works for one family of applications
and may be suggestive for others.
Traditionally, it might happen
that $\bold y$ is ignored,
effectively taking $\bold y=\bold 0$
which happens when the data is better explained by $\bold A$ alone than by  $\bold B$ alone.
Solve first for $\bold x$ without $\bold y$.
Call it $\bold x_0$.
Now define a new variable
$\tilde {\bold x}$ such that
$\bold x = \bold x_0 +\tilde {\bold x}$.
Introducing your innovative concept (estimating $\bold y$) your regression becomes:
\begin{eqnarray}
\bold 0 &\approx & \bold r 
\quad = \quad
\bold A ( \bold x_0 +\tilde {\bold x}) + \bold B \bold y \ -\ \bold d
\\
\bold 0 &\approx & \bold r 
\quad = \quad
\bold A \tilde {\bold x} + \bold B \bold y - (\bold d -\bold A\bold x_0)
\end{eqnarray}
Start off from
$(\tilde{\bold x},\bold y)=(\bold 0,\bold 0)$.
Like Equation (\ref{eqn:firststep}),
the first step leads to:
\begin{equation}
\left[
        \begin{array}{c}
                \bold {\tilde x} \\ \bold y
        \end{array}
\right]
\quad=\quad
\beta\ 
\left[
        \begin{array}{c}
                \bold A\T \bold r
                \\
                \bold B\T \bold r
        \end{array}
\right]
\end{equation}
which is very different
from Equation (\ref{eqn:firststep}),
because $\bold r$ is very different from $\bold d$.
Although we may still have an annoying or inappropriate
correlation between $\tilde{\bold x}$ and $\bold y$,
it is a lot less annoying than a correlation between
$\bold x$ and $\bold y$.

%\par
%We begin with the usual definition
%of preconditioned variables $\bold m =\bold S\bold p$ and
%the opposite of our usual sign convention for the residual.
%\begin{eqnarray}
%-\epsilon \ \bold r &=&  \bold F         \bold m - \bold d \\
%-\epsilon \ \bold r &=&  \bold F \bold S \bold p - \bold d
%\label{eq:r}
%\end{eqnarray}
%which we arrange with a column vector of unknowns that includes
%not only $\bold p$ but also $\bold r$.
%\begin{equation}
%\bold 0
%\quad\approx\quad \hat {\bold r}
%\eq 
%\left[ \bold F \bold S \quad \epsilon \ \bold I \right]
%\
%\left[
%\begin{array}{c}
%\bold p \\
%\bold r
%\end{array}
%\right]
%\ - \  \bold d
%\end{equation}
%The interesting thing is that we can use our familiar
%conjugate-gradient programs with this system of equations.
%We only need to be careful to distinguish the residual in the bottom half
%of our solution vector,
%from the virtual residual $\hat {\bold r}$
%(which should vanish identically)
%in the iterative fitting problem.
%The fitting begins with the initializations:
%\begin{equation}
%\hat{\bold p}_0  \eq \left[
%\begin{array}{c}
%\bold p_0 \\
%\bold 0
%\end{array}
%\right]
%\end{equation}
%and
%\begin{equation}
%\hat{\bold r}_0  \eq
%      \bold F\bold S \bold p_0 -\bold d \eq
%      \bold F        \bold m_0 -\bold d
%\end{equation}
%As the iteration begins we have gradients
%of the two parts of the model
%\begin{eqnarray}
%\bold g_m &=& \bold S\T \bold F\T \hat{\bold r} \\
%\bold g_d &=& \epsilon \        \hat{\bold r}
%\end{eqnarray}
%which imply a perturbation in the theoretically zero residual
%\begin{equation}
%\Delta\hat{\bold r}  \eq
%      \bold F\bold S \bold g_m + \epsilon\; \bold g_d
%\end{equation}
%Then step sizes and steps are determined
%as usual for conjugate-direction fitting.
%\par
%The preconditioning module \texttt{vr\_solver}
%%\vpageref{lst:vrsolver}
%takes three functions as its arguments.
%Functions \texttt{oper} and \texttt{prec} correspond to the linear
%operators $\bold F$ and $\bold S$.
%Function \texttt{solv} implements one step of an optimization descent.
%Its arguments are a logical parameter \texttt{forget},
%which controls a conditional restarting of the optimization,
%the current effective model \texttt{x0},
%the gradient vector \texttt{g}, the data residual vector \texttt{rr}, and
%the conjugate gradient vector \texttt{gg}.
%Subroutine \texttt{solver\_vr} constructs
%the effective model vector \texttt{x},
%which consists of the model-space part \texttt{xm}
%and the data-space part \texttt{xd}.
%Similarly, the effective gradient vector \texttt{g}
%is split into the the model-space part \texttt{gm}
%and the data-space part \texttt{gd}.
%\par
%For brevity I am omitting the code
%for the virtual-residual solver \texttt{vrsolver}
%which is in the library.
%It follows the same pattern as
%\texttt{prec\_solver} \vpageref{lst:precsolver}.
%%\moddex{vrsolver}{Virtual-residual solver}



%\unboldmath


%\clearpage
