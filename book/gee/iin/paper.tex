% copyright (c) 1997 Jon Claerbout

\title{Regularization is model styling}
\author{Jon Claerbout}
\maketitle

\label{paper:iin}
	
{\em Regularization} is a method used in mathematics and statistics
to deal with insufficient information.
The reader must supply additional information in the form of an operator.
From where is this operator to come, and what does it mean?
It amounts to us, as practitioners, specifying a ``style'' of model.
Where the model is a signal or an image,
it amounts to specifying one weighting function in physical space
and another in Fourier space.

%\par
%In this chapter we study the simplest, most transparent example
%of data insufficiency.
%Data exists at irregularly spaced positions in a plane.
%We set up a cartesian mesh and we discover that some
%of the bins contain no data points.
%What then?

\section{EMPTY BINS AND INVERSE INTERPOLATION}
\inputdir{miss1figs}
\par
A method for restoring \bx{missing data} is to ensure that the restored data,
after specified filtering, has minimum energy.
Specifying the filter is choosing the interpolation philosophy.
Generally the filter is a \bx{roughening} filter.
\sx{filter ! roughening}
When a roughening filter goes off the end of smooth data,
it typically produces a large transient at the end.
Minimizing energy implies a choice for unknown data values
at the end to minimize the transient.
We examine five cases and then make some generalizations.
\par
\boxit{A method for restoring missing data
        is to ensure that the restored data,
        after specified filtering, 
        has \protect\bx{minimum energy}.}
\par
Let $u$ denote an unknown (missing) value.
The dataset on which the examples are based is
$(\cdots, u, u,$ 
$1, u, 2, 1, 2, u, u, \cdots )$.
Theoretically, 
we could adjust the missing $u$ values (each different)
to minimize the energy in the unfiltered data.
Those adjusted values would obviously turn out to be all zeros.
The unfiltered data is data that has been filtered by
an impulse function.
To find the missing values
that minimize energy out of other filters,
we can use subroutine \texttt{mis1()}.
%\vpageref{lst:mis1}.
Figure~\ref{fig:mlines}
shows interpolation of the dataset with $(1,-1)$ as a roughening filter.
The interpolated data matches the given data where they overlap.%
\sideplot{mlines}{width=.73\textwidth}{
  Top is given data.
  Middle is given data with interpolated values.
  Missing values seem to be interpolated by straight lines.
  Bottom shows the filter $(1,-1)$.
  Its output (not shown) has minimum energy.
}
\sideplot{mparab}{width=.73\textwidth}{
  Top is the same input data as in Figure~\protect\ref{fig:mlines}.
  Middle is interpolated.
  Bottom shows the filter $(-1,2,-1)$.
  The missing data seems to be interpolated by parabolas.
}
\sideplot{mseis}{width=.73\textwidth}{
  Top is the same input.
  Middle is interpolated.
  Bottom shows the filter $(1,-3,3,-1)$.
  The missing data is very smooth.
  It shoots upward high off the right end of the observations,
  apparently to match the data slope there.
}
%\ACTIVESIDEPLOT{msmo}{width=2.2in}{miss1figs}{
%       The filter \protect\linebreak[2]
%       $(-1,$\protect\linebreak[2]$-1,$\protect\linebreak[2]$4,$
%       \protect\linebreak[2]$-1,$\protect\linebreak[2]$-1)$ gives
%%linebreaks are kludgy way to fix an line breaking bug 
%       interpolations with stiff lines.  They resemble
%       the straight lines of Figure~\protect\FIG{mlines},
%       but they project through a cluster of given values
%       instead of projecting to the nearest given value.
%       Thus, this interpolation tolerates noise in the given data
%       better than the interpolation shown in
%       Figure~\protect\FIG{mseis}.
%       }
\sideplot{moscil}{width=2.2in}{
  Bottom shows the filter $(1,1)$.
  The interpolation is rough.
  Like the given data itself, the interpolation
  has much energy at the Nyquist frequency.
  But unlike the given data, it has little zero-frequency energy.
}
\par
Figures~\ref{fig:mlines}--\ref{fig:moscil}
illustrate the rougher the filter,
the smoother the interpolated data,
and vice versa.
Switch attention from the residual spectrum
to the residual.
The residual for Figure~\ref{fig:mlines}
is the {\it slope} of the signal
(because the filter $[1,-1]$ is a {\it first derivative}),
and the slope is constant (uniformly distributed) along the straight lines
where the least-squares procedure is choosing signal values.
So, these examples confirm the idea
that the \bx{least-squares method} abhors large values
(because they are squared).
Thus, least squares tends to distribute residuals uniformly
in both time and frequency to the extent
allowed by the \bx{constraint}s.
\par
This idea helps us answer the question,
what is the best filter to use?
It suggests choosing
the filter to have an amplitude spectrum
that is inverse to the spectrum we want for the interpolated data.
A systematic approach is given in Chapter \ref{paper:mda},
but I offer a simple subjective analysis here:
Looking at the data, we see that all points are positive.
It seems, therefore, that
the data is rich in low frequencies;
thus, the filter should contain something like $(1,-1)$,
which vanishes at zero frequency.
Likewise, the data seems to contain Nyquist frequency;
so, the filter should contain $(1,1)$.
The result of using the filter $(1,-1)\ast (1,1)=(1,0,-1)$
is shown in Figure~\ref{fig:mbest}.
Foregoing is my best subjective interpolation
based on the idea that the missing data should look like the given data.
The resulting \bx{interpolation} and \bx{extrapolation}s are so good that
you can hardly guess which data values are given
and which are interpolated.
We care about this because the goal in geophysical image making is to create an image
that hides locations of our measurements (and missing measurements!).
\sideplot{mbest}{width=.73\textwidth}{
  Top is the same as in
  Figures~\protect\ref{fig:mlines} to \protect\ref{fig:moscil}.
  Middle is interpolated.
  Bottom shows the filter $(1,0,-1)$, which comes from
  the coefficients of $(1,-1)\ast (1,1)$.
  Both the given data and the interpolated data
  have significant energy at
  both zero and Nyquist frequencies.
}

\subsection{Missing-data program}
\sx{missing data}
Now, let us see how Figures \ref{fig:mlines}-\ref{fig:mbest} were calculated.
Matrices could have been pulled apart according to subscripts of known and missing data.
Instead I computed them with operators,
and applied only operators and their adjoints.
First, 
we inspect the matrix approach
because it is more conventional.

\subsubsection{Matrix approach to missing data}
\sx{missing data}
\par
Customarily, we have referred to data by the symbol $\bold d$.
Now that we are dividing the data space into two parts,
known and unknown (or missing),
we refer to this complete space
as the model (or map) space $\bold m$.
\par
There are 15 data points in Figures \ref{fig:mlines}-\ref{fig:mbest}.
Of the 15, 4 are known and 11 are missing.
Denote the known by $k$ and the missing by $u$.
Then, the sequence of missing and known
is $(u,u,u,u,k,u,k,k,k,u,u,u,u,u,u)$.
Because I cannot print $15\times 15$ matrices,
please allow me to describe instead a data space of 6 values
$(m_1, m_2, m_3, m_4, m_5, m_6)$ with known values only $m_2$ and $m_3$,
$\bold m$ arranged as $(u,k,k,u,u,u)$.
\par
Our approach is to minimize the energy in the residual,
which is the filtered map (model) space.
We state the fitting goals
$\bold 0\approx \bold F\bold m$ as:
\begin{equation} 
\left[ 
\begin{array}{c}
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0 \\ 
  0
  \end{array} \right] 
\quad \approx \quad
\bold r
\quad =\quad
\left[ 
\begin{array}{cccccc}
  a_1 & 0   & 0    & 0   & 0   & 0   \\
  a_2 & a_1 & 0    & 0   & 0   & 0   \\
  a_3 & a_2 & a_1  & 0   & 0   & 0   \\
  0   & a_3 & a_2  & a_1 & 0   & 0   \\
  0   & 0   & a_3  & a_2 & a_1 & 0   \\
  0   & 0   & 0    & a_3 & a_2 & a_1 \\
  0   & 0   & 0    & 0   & a_3 & a_2 \\
  0   & 0   & 0    & 0   & 0   & a_3 
  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  m_1 \\ 
  m_2 \\ 
  m_3 \\ 
  m_4 \\ 
  m_5 \\ 
  m_6
  \end{array} \right]
\label{eqn:headache}
\end{equation}

Rearranging these fitting goals,
and bringing the columns multiplying known data values
($m_2$ and $m_3$) to the left,
we get $\bold y =-\bold F_k \bold m_k \approx \bold F_u \bold m_u$.
\begin{equation} 
\left[ 
\begin{array}{c}
  y_1 \\ 
  y_2 \\ 
  y_3 \\ 
  y_4 \\ 
  y_5 \\ 
  y_6 \\ 
  y_7 \\ 
  y_8
  \end{array} \right] 
\quad = \quad  -
\left[ 
\begin{array}{cc}
   0   & 0    \\
   a_1 & 0    \\
   a_2 & a_1  \\
   a_3 & a_2  \\
   0   & a_3  \\
   0   & 0    \\
   0   & 0    \\
   0   & 0    
  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  m_2 \\ 
  m_3 
  \end{array} \right]
\quad \approx \quad
\left[ 
\begin{array}{cccc}
  a_1  & 0   & 0   & 0   \\
  a_2  & 0   & 0   & 0   \\
  a_3  & 0   & 0   & 0   \\
  0    & a_1 & 0   & 0   \\
  0    & a_2 & a_1 & 0   \\
  0    & a_3 & a_2 & a_1 \\
  0    & 0   & a_3 & a_2 \\
  0    & 0   & 0   & a_3 
  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  m_1 \\ 
  m_4 \\ 
  m_5 \\ 
  m_6
  \end{array} \right]
\label{eqn:wristpain}
\end{equation}
Equation (\ref{eqn:wristpain}) 
is the familiar form of an overdetermined system of equations
$\bold y \approx \bold F_u \bold m_u$
that we could solve
for $\bold m_u$
as illustrated earlier by conjugate directions
or by a wide variety of well-known methods.

\par
The trouble with this matrix approach is that it is awkward to program
the partitioning of the operator into the known and missing parts,
particularly if the application of the operator uses arcane techniques,
such as those used by the fast--Fourier-transform operator
or various numerical approximations to differential, or partial
differential, operators that depend on regular data sampling.
Even for the modest convolution operator,
we already have a library of convolution programs
that handle a variety of end effects,
and it would be much nicer to use the library as it is
rather than recode it for all possible geometrical arrangements
of missing data values.
\par
%Note:
%Here I take the main goal to be the clarity of the code,
%not the efficiency or accuracy of the solution.
%So, if your application consumes too many resources,
%and if you have many more known points than missing ones,
%maybe you should fit
%$\bold y \approx \bold F_u \bold m_u$
%and ignore the suggestions below.

\subsubsection{Operator approach to missing data}
\sx{missing data}
\par
For the operator approach to the fitting goal
$ -\bold F_k \bold m_k \approx \bold F_u \bold m_u$,
we rewrite it as
$ -\bold F_k \bold m_k \approx \bold F \bold J \bold m $ where

\begin{equation} 
-\bold F_k \bold m_k
\ \approx \ 
\left[ 
\begin{array}{cccccc}
  a_1 & 0   & 0    & 0   & 0   & 0   \\
  a_2 & a_1 & 0    & 0   & 0   & 0   \\
  a_3 & a_2 & a_1  & 0   & 0   & 0   \\
  0   & a_3 & a_2  & a_1 & 0   & 0   \\
  0   & 0   & a_3  & a_2 & a_1 & 0   \\
  0   & 0   & 0    & a_3 & a_2 & a_1 \\
  0   & 0   & 0    & 0   & a_3 & a_2 \\
  0   & 0   & 0    & 0   & 0   & a_3 
  \end{array} \right] 
\;
\left[ 
\begin{array}{cccccc}
  1   & .   & .    & .   & .   & .   \\
  .   & 0   & .    & .   & .   & .   \\
  .   & .   & 0    & .   & .   & .   \\
  .   & .   & .    & 1   & .   & .   \\
  .   & .   & .    & .   & 1   & .   \\
  .   & .   & .    & .   & .   & 1  
  \end{array} \right] 
\;
\left[ 
\begin{array}{c}
  m_1 \\ 
  m_2 \\ 
  m_3 \\ 
  m_4 \\ 
  m_5 \\ 
  m_6
  \end{array} \right]
\ =\ \bold F \bold J \bold m
\label{eqn:migraine}
\end{equation}
Notice the introduction of the new diagonal matrix $\bold J$,
called a \bx{mask}ing matrix or a \bx{constraint-mask} matrix
because it multiplies
constrained variables by zero
leaving freely adjustable variables untouched.
Experience shows that a better name than ``mask matrix'' is
``\bx{selector} matrix''
because what comes out of it,
that which is selected,
is a less-confusing name for it than which is rejected.
With a selector matrix, the whole data space seems freely adjustable,
both the missing data values and known values.
We see that the CD method does not change the known (constrained) values.
In general, we derive the fitting goal (\ref{eqn:migraine}) by:
\begin{eqnarray}
 \label{eqn:misfitgoal}
 \bold 0 &\approx& \bold F \bold m \\
 \bold 0 &\approx& \bold F( \bold J + (\bold I-\bold J) ) \bold m \\
 \bold 0 &\approx& \bold F\bold J\bold m + \bold F(\bold I-\bold J)\bold m \\
 \label{eqn:misfitgoal2}
 \bold 0 &\approx& \bold F\bold J\bold m + \bold F\bold m_{\rm known} \\
                                         \bold 0 \quad\approx\quad
 \bold r &=&       \bold F\bold J\bold m + \bold r_0
\end{eqnarray}
As usual, we find a direction to go $\Delta \bold m$
by the gradient of the residual energy.
\begin{equation}
\Delta\bold m
= \frac{\partial }{\partial \bold m\T}\ \bold r\T\bold r
= \left( \frac{\partial }{\partial \bold m\T} \ \bold r\T \right) \bold r
= \left( \frac{\partial }{\partial \bold m\T} \
(\bold m\T\bold J\T\bold F\T + \bold r\T_0)                 \right) \bold r
=  \bold J\T \bold F\T \bold r
\label{eqn:fooo}
\end{equation}

\par
We begin the calculation with
the known data values where missing data values are replaced by zeros, namely
$(\bold I-\bold J)\bold m$.
Filter this data,
getting $\bold F(\bold I-\bold J)\bold m$,
and load it into the residual $\bold r_0$.
With this initialization completed,
we begin an iteration loop.
First we compute $\Delta \bold m$ from equation (\ref{eqn:fooo}).
\begin{equation}
\Delta\bold m \quad\longleftarrow\quad \bold J\T \bold F\T \bold r
\end{equation}
$\bold F\T$ applies a {\it crosscorrelation} of the filter to the residual
and then $\bold J\T$ sets to zero any changes proposed to known data values.
Next, compute the change in residual $\Delta\bold r$
from the proposed change in the data $\Delta\bold m$.
\begin{equation}
\Delta\bold r \quad \longleftarrow \quad \bold F \bold J \Delta \bold m
\label{eqn:lastmistheory}
\end{equation}
Equation (\ref{eqn:lastmistheory}) applies the filtering again.
Then, use the method of steepest descent (or conjugate direction)
to choose the appropriate scaling (or inclusion of previous step)
of $\Delta \bold m$ and $\Delta \bold r$,
and update $\bold m$ and $\bold r$, accordingly,
and iterate.

\begin{comment}
\par
I could have passed a new operator $\bold F \bold J$
into the old solver,
but found it worthwhile to write a new,
more powerful solver having built-in constraints.
To introduce the masking operator $\bold J$ into the \texttt{solver-smp}
subroutine \vpageref{lst:solver-tiny},
I introduce an optional operator \texttt{Jop},
which is initialized with a logical array of the model size.
 %The values of \texttt{known} are \texttt{.true.}~for the known data locations,
 %and \texttt{.false.}~for the unknown data.
Two lines in the \texttt{solver-tiny} module \vpageref{lst:solver-tiny}
\par\indent
\footnotesize
\begin{verbatim}
stat = Fop( T, F, g, rd)                          #  g = F' Rd
stat = Fop( F, F, g, gd)                          #  G = F  g
\end{verbatim}
\normalsize
\par\noindent
become three lines in the standard library module \texttt{solver\_smp}.
(We use a temporary array \texttt{tm} of the size of model space.)
$\Delta \bold m$ is {\tt g} and
$\Delta \bold r$ is {\tt gg}.
\par\noindent
\footnotesize
\begin{verbatim}
stat = Fop( T, F, g, rd)                                  # g = F' Rd
if ( present( Jop)) { tm=g;  stat= Jop( F, F, tm, g)      # g = J g
stat = Fop( F, F, g, gg)                                  # G = F g
\end{verbatim}
\normalsize

\par
The full code includes all the definitions we had earlier
in \texttt{solver-tiny} module \vpageref{lst:solver-tiny}.
Merging it with the above bits of code we have
the simple solver \texttt{solver-smp}.


%In Fortran77, I simply multiplied by $\bold J$.
%In Fortran90, subroutines can have optional arguments.
%The expression
%\texttt{if( present( known))} says, ``if the argument
%\texttt{known}, a logical array, is an argument of the call.''
%The expression,
%\texttt{where( known) dm = 0} means that each component 
%of the logical array \texttt{known} is examined;
%if the value of that component is \texttt{.true.}~then
%the corresponding component of $\Delta \bold m$ is set to zero.
%In other words, we are not going to change the given, known data values.


\moddex{solver-smp}{simple solver}

\par
There are two methods of invoking the solver.
Comment cards in the code indicate the slightly more verbose
method of solution which matches the theory presented in the book.
\end{comment}

\par
The subroutine to find missing data is \texttt{mis1()}.
It assumes that zero values in the input data
correspond to missing data locations.
It uses our convolution operator
\texttt{tcai1()}.
% \vpageref{lst:tcai1}.
You can also check the book Index for other
operators and modules.
%\progdex{mis1}{1-D missing data}
\moddex{mis1}{1-D missing data}{54}{76}{user/gee}
\noindent

%Here \texttt{known} is declared in \texttt{solver\_mod} to be a logical vector.
%The call argument \texttt{ known=(xx/=0.)} sets the mask vector components
%to \texttt{.true.} where components of $\bold x$ are nonzero and sets it
%to \texttt{.false.} where the components are zero.

\par
I sought reference material on conjugate gradients with constraints
and did not find anything,
leaving me to fear that this chapter was in error,
and I had lost the magic property of convergence
in a finite number of iterations.
I tested the code, and it did converge in a finite number of iterations.
The explanation is that these constraints are almost trivial.
We pretended we had extra variables,
and computed a $\Delta \bold m =\bold g$ for each.
Using $\bold J$ we then set the gradient $\Delta \bold m =\bold g$ to zero,
therefore making no changes to anything,
like as if we had never calculated the extra  $\Delta \bold m$s.


\section{WELLS NOT MATCHING THE SEISMIC MAP}
\inputdir{chevron}
Accurate knowledge comes from \bx{wells},
but wells are expensive and far apart.
Less accurate knowledge comes from surface seismology,
but this knowledge is available densely in space
and can indicate significant \bx{trend}s between the wells.
For example,
a prospective area may contain 15 wells
but 600 or more seismic stations.
To choose future well locations,
it is helpful to match the known well data with the seismic data.
Although the seismic data is delightfully dense in space,
it often mismatches the wells
because there are systematic differences in the nature of the measurements.
These discrepancies
are sometimes attributed to velocity \bx{anisotropy}.
To work with such measurements,
we do not need to track down the physical model,
we need only to merge the information somehow
to appropriately \bx{map} the trends between wells
and make a proposal for the next drill site.
Here, we consider only a scalar value at each location.
Take $\bold w$ to be a vector of 15 components,
each component being the seismic travel time to some fixed depth in a well.
Likewise, let $\bold s$ be a 600-component vector
each with the seismic travel time to that fixed depth
as estimated wholly from surface seismology.
Such empirical corrections are often called ``\bx{fudge factor}s''.
An example is the Chevron oil field in Figure \ref{fig:wellseis}.
\plot{wellseis}{width=\textwidth,height=.5\textwidth}{
  Binning by data push.
  Left is seismic data.
  Right is well locations.
  Values in bins are divided by numbers in bins.
  (Toldi)
}
The binning of the seismic data in Figure \ref{fig:wellseis}
is not really satisfactory when we have available
the techniques of missing data estimation
to fill the empty bins.
Using the ideas of subroutine \texttt{mis1()} ,
we can extend the seismic data into the empty part of the plane.
We use the same principle that we minimize the energy in 
the filtered map where the map must match the data where it is known.
I chose the filter $\bold A = \nabla\T\,\nabla=-\nabla^2$
to be the Laplacian operator (actually, its negative)
to obtain the result in Figure \ref{fig:misseis}.
\plot{misseis}{width=\textwidth,height=.5\textwidth}{
  Seismic binned (left) and extended (right)
  by minimizing energy in $\nabla^2 \bold s$.
}
\par
There are basically two ways to handle boundary conditions.
First, as we did in Figure \ref{fig:mlines},
by using a transient filter operator
that assumes zero outside to the region of interest.
Second is to use an internal filter operator.
Internal filters introduce the hazard that
solutions could be growing at the boundaries.
Growing solutions are rarely desirable.
In that case,
it is better to assign boundary values,
which is what I did here in Figure \ref{fig:misseis}.
I did not do it because it is better,
but did it to minimize the area surrounding the data of interest.

\par
The first job is to fill the gaps in the seismic data.
We did such a job in one dimension in Figures \ref{fig:mlines}--\ref{fig:mbest}.
More computational details later come later.
Let us call the extended seismic data  $\bold s$.

\par
Think of a map of a model space $\bold m$
of infinitely many hypothetical wells that must match the real wells,
where we have real wells.
We must find a map that matches the wells exactly
and somehow matches the seismic information elsewhere.
Let us define the vector $\bold w$, as shown in Figure \ref{fig:wellseis}
so $\bold w$ is
observed values at wells and zeros elsewhere.

\par
Where the seismic data contains sharp bumps or streaks,
we want our final Earth model to have those features.
The wells cannot provide the rough features, because the wells
are too far apart to provide high-spatial frequencies.
The well information generally conflicts with the seismic data 
at low-spatial frequencies because of systematic discrepancies between
the two types of measurements.
Thus we must accept that $\bold m$ and $\bold s$ may differ
at low-spatial frequencies (where gradient and Laplacian are small).

\par
Our final map $\bold m$ would be very unconvincing if
it simply jumped from a well value at one point
to a seismic value at a neighboring point.
The map would contain discontinuities around each well.
Our philosophy of finding an Earth model $\bold m$
is that our Earth map should contain no obvious 
``footprint'' of the data acquisition (well locations).
We adopt the philosophy that the difference
between the final map (extended wells),
and the seismic information $\bold x=\bold m-\bold s$ should be smooth.
Thus,
we seek the minimum residual $\bold r$,
which is the roughened difference between the seismic data $\bold s$
and the map $\bold m$ of hypothetical omnipresent wells.
With roughening operator $\bold A$ we fit:
\begin{equation}
\bold 0\quad\approx\quad \bold r \eq \bold A ( \bold m - \bold s )
	\eq \bold A \bold x
\label{eqn:unconmap}
\end{equation}
along with the constraint
that the map should match the wells at the wells.
We honor this constraint by initializing the map $\bold m = \bold w$
to the wells (where we have wells, and zero elsewhere).
After we find the gradient direction to suggest some changes
to $\bold m$, we simply do not allow those changes at well locations
by using a mask.
We apply a ``missing data selector'' to the gradient
which zeros out possible changes at well locations.
Like with the goal (\ref{eqn:misfitgoal2}),
we have:
\begin{equation}
\bold 0\quad\approx\quad \bold r \eq
\bold A \bold J \bold x + \bold A \bold x_{\rm known}
\end{equation}
After minimizing $\bold r$ by adjusting $\bold x$,
we have our solution $ \bold m =  \bold x + \bold s $.

\par
Now, we prepare some roughening operators $\bold A$.
We have already coded a 2-D gradient operator
\texttt{igrad2}.
% \vpageref{lst:igrad2}.
Let us combine it with its adjoint to get the 2-D Laplacian operator.
(You might notice that the Laplacian operator is ``self-adjoint,'' meaning
that the operator does the same calculation that its adjoint does.
Any operator of the form $\bold A\T\bold A$ is self-adjoint because
$(\bold A\T\bold A)\T=\bold A\T (\bold A\T)\T=\bold A\T\bold A$. )
\par
\opdex{laplac2}{Laplacian in 2-D}{46}{84}{system/generic} 
Subroutine \texttt{lapfill()}
%\vpageref{lst:lapfill} 
is the same idea as \texttt{mis1()}
%\vpageref{lst:mis1} 
except that 
the filter $\bold A$ has been specialized to the 
Laplacian
implemented by module \texttt{laplac2}.
% \vpageref{lst:laplac2}. 


\moddex{lapfill}{Find 2-D missing data}{64}{79}{system/generic}

\par
Subroutine \texttt{lapfill()}
can be used for each of our two applications:
(1) extending the seismic data to fill space, and
(2) fitting the map exactly to the wells and approximately to the seismic data.
When extending the seismic data,
the initially non-zero components $\bold s \ne \bold 0$ are fixed
and cannot be changed.
\begin{comment}
That is done by calling
\texttt{lapfill()} with \texttt{mfixed=(s/=0.)}.
When extending wells,
the initially non-zero components $\bold w \ne \bold 0$ are fixed
and cannot be changed.
That is done by calling
\texttt{lapfill()} with \texttt{mfixed=(w/=0.)}.
\end{comment}

\par
The final map is shown in Figure \ref{fig:finalmap}.
\plot{finalmap}{width=\textwidth,height=.5\textwidth}{
  Final map based on Laplacian roughening.
}

\par
Results can be computed with various filters.
I tried both $\nabla^2$ and $\nabla$.
There are disadvantages of each,
$\nabla$ being too cautious and
$\nabla^2$ perhaps being too aggressive.
Figure \ref{fig:diffdiff} shows the difference $\bold x$ between
the extended seismic data and the extended wells.
Notice that for $\nabla$ the difference shows
a localized ``tent pole'' disturbance about each well.
For $\nabla^2$, there could be a large overshoot between wells,
especially if two nearby wells have significantly different values.
I do not see that problem here.
\par
My overall opinion is that the Laplacian does the better job in this case.
I have that opinion because in viewing the extended gradient,
I can clearly see where the wells are.
The wells are where we have acquired data.
We would like our map of the world to not show where we acquired data.
Perhaps our estimated map of the world cannot help but show where
we have and have not acquired data, but we would like to minimize that aspect.
\par
\boxit{ A good image of the Earth hides our data \bx{acquisition footprint}.  }


\plot{diffdiff}{width=\textwidth,height=.375\textwidth}{
  Difference between wells (the final map)
  and the extended seismic data.
  Left is plotted at the wells (with gray background for zero).
  Center is based on gradient roughening and shows
  tent-pole-like residuals at wells.
  Right is based on Laplacian roughening.
}

\par
To understand the behavior theoretically,
recall that in one dimension 
the filter $\nabla$ interpolates with straight lines
and $\nabla^2$ interpolates with cubics.
The reason is that the fitting goal
$\bold 0 \approx \nabla \bold m$,
leads to
$\frac{\partial }{\partial \bold m\T} \bold m\T\nabla\T\,\nabla \bold m = \bold 0$
or $\nabla\T\,\nabla \bold m = \bold 0$; whereas, the fitting goal 
        $\bold 0 \approx \nabla^2 \bold m$
leads to
        $\nabla^4 \bold m = \bold 0$,
which is satisfied by cubics.
In two dimensions, minimizing the output of $\nabla$
gives us solutions of Laplace's equation with sources at the known data.
It is as if $\nabla$ stretches a rubber sheet over poles at each well;
whereas, $\nabla^2$ bends a stiff plate.
\par
Just because $\nabla^2$ gives smoother maps than  $\nabla$
does not mean those maps are closer to reality.
An objectively better choice for the model styling goal is addressed in Chapter~\ref{paper:mda}.
It is the same issue we noticed when comparing
Figures \ref{fig:mlines}-\ref{fig:mbest}.

\section{SEARCHING THE SEA OF GALILEE}
\sx{Sea of Galilee}
\sx{Galilee, Sea of}
\inputdir{galilee}
Figure \ref{fig:locfil} shows a bottom-sounding survey
of the Sea of Galilee\footnote{
        Data collected by Zvi \bx{ben Avraham}, TelAviv University.
        Please communicate with him {\tt zvi@jupiter1.tau.ac.il}
        for more details or if you make something
        publishable with his data.
        }
at various stages of processing.
The ultimate goal is not only a good map of
the depth to bottom,
but images useful for the purpose
of identifying \bx{archaeological}, geological, or
geophysical details of the sea bottom.
The Sea of Galilee is unique,
because it is a {\it fresh}-water lake {\it below} sea-level.
It seems to be connected to the great rift (pull-apart)
valley crossing east Africa.
We might delineate the Jordan River delta.
We might find springs on the water bottom.
We might find archaeological objects.
\par
\plot{locfil}{width=\textwidth,height=1.4\textwidth}{
  Views of the bottom of the Sea of Galilee.
}
The raw data is 132,044 triples, $(x_i,y_i,z_i)$,
where $x_i$ ranges over 12 km and
where $y_i$ ranges over 20 km.
The lines you see in Figure \ref{fig:locfil}
are sequences of data points, i.e., the track of the survey vessel.
The depths $z_i$ are recorded at a discretization interval of 10 cm.
\par
The first frame in Figure~\ref{fig:locfil} shows simple binning.
A coarser mesh would avoid the empty bins but lose resolution.
As we refine the mesh for more detail,
the number of empty bins grows,
as does the care needed in devising a technique
for filling them.
This first frame uses the simple idea from Chapter \ref{paper:ajt} of
spraying all the data values to the nearest bin
with \texttt{bin2()} 
%\vpageref{lst:bin2}
and dividing by the number in the bin.
Bins with no data obviously need to be filled in some other way.
I used a missing data program like that in the recent section
on ``wells not matching the seismic map.''
Instead of roughening with a Laplacian, however,
I used the gradient operator \texttt{igrad2}. 
% I used the \bx{roughening operator} \texttt{rufftri2()} \vpageref{lst:rufftri2},
% which is an impulse minus a broad smoothing operator.
The solver is \texttt{grad2fill()}.
\moddex{igrad2}{finite-difference gradient}{48}{60}{api/c}
\moddex{grad2fill}{low cut missing data}{55}{63}{api/c}

%\par
%Having a roughener with an adjustable width in the smoothing operator
%allows us to choose a cutoff frequency.
%This is interesting for two reasons:
%(1) The output of the roughening operator
%is interesting to look at; and
%(2) Larger filters allow missing data iterations to converge more quickly.

\par
The output of the roughening operator is an image,
a filtered version of the depth,
a filtered version of something real.
Such filtering can enhance the appearance of interesting features.
For example,
in scanning the shoreline of the roughened image
(after missing data was filled),
we see several ancient shorelines, now submerged.
%\boxit{
%The adjoint is the easiest image to build.
The roughened map is often more informative than the map.
%}
\par
The views expose several defects
of the data acquisition and of our data processing.
The impulsive glitches (St.~Peter's fish?)
need to be removed; but we must be careful not to throw
out the sunken ships along with the bad data points.
Even our best image shows clear evidence of the recording vessel's tracks.
Strangely, some tracks are deeper than others.
Perhaps the survey is assembled from work done in different seasons,
and the water level varied by season.
Perhaps, some days the vessel was more heavily loaded
and the depth sounder was on a deeper keel.
As for the navigation equipment,
we can see that some data values are reported outside the lake!
\par
%\boxit{ A good image of the earth hides our data \bx{acquisition footprint}.  }
%\par
We want the sharpest possible view
of this classical site.
A treasure hunt is never easy
and no one guarantees we can
find anything of great value;
but at least the exercise is a good warm-up
for submarine petroleum exploration.


\section{CODE FOR THE REGULARIZED SOLVER}
\inputdir{invint}
In Chapter \ref{paper:ajt} we defined \bx{linear interpolation}
\sx{interpolation}
as the extraction of values from between mesh points.
In a typical setup (occasionally the role of data and model are swapped),
a model is given on a uniform mesh,
and we solve the easy problem of extracting values
between the mesh points with subroutine \texttt{lint1()}.
% \vpageref{lst:lint1}.
The genuine problem is the inverse problem, which we attack here.
Data values are sprinkled all around,
and we wish to find a function on a uniform mesh
from which we can extract that data by \bx{linear interpolation}.
The adjoint operator for subroutine {\tt lint1()}
simply piles data back into its proper location in model space
without regard to how many data values land in each region.
Thus, some locations may receive much data while other locations get none.
We could interpolate by minimizing the energy in the model gradient,
in the second derivative of the model,
or in any roughening model.
\par
Formalizing now our wish
that data $\bold d$ be extractable by \bx{linear interpolation} $\bold F$,
from a model $\bold m$,
and our wish that application of a roughening filter
with an operator $\bold A$ have minimum energy, we write the fitting goals:
\begin{equation}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F \bold m - \bold d \\
        \bold 0 &\approx & \bold A \bold m
        \label{eqn:invintwish}
        \end{array}
\end{equation}
Suppose we take the roughening filter to be the second difference operator
$(1,-2,1)$
scaled by a constant $\epsilon$,
and suppose we have a data point near each end of the model
and a third data point exactly in the middle.
Then,
for a model space 6 points long,
the fitting goal could look like:
\def\E{\epsilon}
\begin{equation} { 
\left[ 
\begin{array}{rrrrrr}
   .8 & .2 & .  & .  & .  & .  \\
   .  & .  & 1  & .  & .  & .  \\
   .  & .  & .  & .  & .5 & .5 \\
   \hline
   \E & .  & .  & .  & .  & .  \\
  -2\E& \E & .  & .  & .  & .  \\
   \E &-2\E&  \E& .  & .  & .  \\
   .  & \E &-2\E&  \E& .  & .  \\
   .  & .  &  \E&-2\E&  \E& .  \\
   .  & .  & .  &  \E&-2\E&  \E\\
   .  & .  & .  & .  &  \E&-2\E\\
   .  & .  & .  & .  & .  &  \E
  \end{array} \right] 
\left[ 
        \begin{array}{c}
          m_0 \\ 
          m_1 \\ 
          m_2 \\ 
          m_3 \\ 
          m_4 \\ 
          m_5 
        \end{array}
\right] 
\ -\ 
\left[ 
\begin{array}{c}
  d_0 \\ 
  d_1 \\ 
  d_2 \\ 
  \hline
  0   \\
  0   \\
  0   \\
  0   \\
  0   \\
  0   \\
  0   \\
  0
  \end{array} \right] 
\eq
\left[ 
\begin{array}{c}
  \bold r_d \\ 
  \bold r_m 
  \end{array} \right] 
\quad \approx \ \bold 0
\label{eqn:tworegexam}
} \end{equation}
\par
The residual vector has two parts,
a data part $\bold r_d$ on top
and a model part $\bold r_m$ on the bottom.
The data residual
should vanish except where contradictory data values
happen to lie in the same place.
The model residual is the roughened model.

\par
Finding something unexpected is good science and engineering.
We should look both in data space and model space.
In data space, we look at the residual $\bold r$.
In model space, we look at the residual projected there
$\Delta \bold m =\bold F\T\bold r$.
After iterating to completion, we have
$\Delta \bold m = \bold 0 = \bold F\T \bold r_d + \bold A\T\bold r_m$,
a sum of two images identical but for polarity.
This image tells us what we have learned from the data;
and how the model differs from what we thought it would be.

\begin{comment}
\par
Two fitting goals (\ref{eqn:invintwish}) are so common in practice
that it is convenient to adopt our least-square fitting
subroutine \texttt{solver-smp} \vpageref{lst:solver-smp} accordingly.
The modification
is shown in module \texttt{solver-reg} \vpageref{lst:solver-reg}.
In addition to specifying the ``data fitting'' operator $\bold F$
(parameter \texttt{Fop}),
we need to pass the ``model regularization'' operator $\bold A$
(parameter \texttt{Aop}) and the
size of its output (parameter \texttt{nAop}) for proper memory allocation.

%\begin{notforlecture}
\par
(When I first looked at module \texttt{solver-reg} I was appalled
by the many lines of code, especially all the declarations.
Then I realized how much much worse was Fortran 77 where
I needed to write a new solver for every pair of operators.
This one solver module works for all operator pairs
and for many optimization descent strategies
because these ``objects'' are arguments.
These more powerful objects require declarations that are more complicated
than the simple objects of Fortran 77.
As an author I have a dilemma:
To make algorithms compact (and seem simple) requires many careful definitions.
When these definitions put in the code, they are careful,
but the code becomes annoyingly verbose.
Otherwise, the definitions must go in the surrounding natural language
where they are not easily made precise.)


\moddex{solver-reg}{generic solver with regularization}
\end{comment}

\par
% The subroutine for this job does not initialize the original model.
% The caller might want to initialize the model with $\bold m=\bold 0$
% or with some other model $\bold m_0$.
After all the definitions,
we load the negative of the data into the residual.
If a starting model $\bold m_0$ is present,
then we update the data part of the residual
$\bold r_d=\bold F \bold m_0 - \bold d$,
and we load
the model part of the residual
$ \bold r_m = \bold A \bold m_0$.
Otherwise, we begin from a zero model $\bold m_0=\bold 0$; and thus,
the model part of the residual $ \bold r_m$ is also zero.
After this initialization, subroutine 
\texttt{solver\_reg()} % \vpageref{lst:solver_reg}
begins an iteration loop by first computing
the proposed model perturbation $\Delta \bold m$
(called \texttt{g} in the program)
with the adjoint operator:
\begin{equation}
 \Delta \bold m
 \quad\longleftarrow\quad
 \left[
 \begin{array}{cc}
   \bold F\T &   \bold A\T
 \end{array}
 \right]
 \
 \left[
 \begin{array}{c}
   \bold r_d \\
   \bold r_m
 \end{array}
 \right]
\end{equation}
\begin{comment}
I chose to implement the model roughening operator $\bold A$
with the convolution subroutine \texttt{tcai1()} % \vpageref{lst:tcai1},
which has transient end effects
(and an output length equal to the input length plus the filter length).
The adjoint of subroutine {\tt tcai1()} suggests perturbations
in the convolution input (not the filter).
\end{comment}
Using this value of $\Delta \bold m$,
% {\tt invint1()} 
we can
find the implied change in residual $\Delta \bold r$ as:
\begin{equation}
        \Delta
        \left[
        \begin{array}{c}
        \bold r_d \\
        \bold r_m
        \end{array}
        \right]
\quad\longleftarrow\quad
        \left[
        \begin{array}{c}
        \bold F \\
        \bold A
        \end{array}
        \right]
        \
        \Delta \bold m
\end{equation}
and the last thing in the loop is to use
the optimization step function \texttt{stepper()} 
% conjugate-direction subroutine \texttt{cgplus()} \vpageref{lst:cgplus}
to decide the length of the step size
and to decide how much of the previous step to include.
\par
An example of using the new solver is subroutine \texttt{invint1}.
%\vpageref{lst:invint1}.
I chose to implement the model roughening operator $\bold A$
with the convolution subroutine \texttt{tcai1()}
% \vpageref{lst:tcai1},
which has transient end effects
(and an output length equal to the input length plus the filter length).
The adjoint of subroutine {\tt tcai1()} suggests perturbations
in the convolution input (not the filter).
\moddex{invint1}{invers linear interp.}{24}{40}{user/gee}
%\end{notforlecture}
\par
Figure \ref{fig:im1-2+1} shows an example for a $(1,-2,1)$ filter with $\epsilon = 1$.
The continuous curve representing the model $\bold m$
passes through the data points.
Because the models are computed with transient convolution end-effects,
the models tend to damp linearly to zero outside the region where
signal samples are given.
\sideplot{im1-2+1}{width=.8\textwidth,height=.35\textwidth}{
  Sample points and estimation of a continuous function through them.
}
\par
To show an example where the result is clearly a theoretical answer,
I prepared another figure with the simpler filter $\bold A = (1,-1)$.
When we minimize energy in the first derivative of the waveform,
the residual becomes distributed uniformly
between data points
so the solution there is a straight line.
Theoretically, it should be a straight line,
because a straight line has a vanishing second derivative;
and that condition arises by differentiating by $\bold x\T$,
the minimized quadratic form
$\bold x\T \bold A\T\bold A \bold x$, and getting
$         \bold A\T\bold A \bold x=\bold 0$.
(By this logic, the curves between data points in Figure \ref{fig:im1-2+1}
must be cubics.)
The $(1,-1)$ result is shown in Figure \ref{fig:im1-1a}.
\sideplot{im1-1a}{width=.8\textwidth,height=.35\textwidth}{
  The same data samples
  and a function through them that minimizes
  the energy in the first derivative.
}
\par
The example of
Figure \ref{fig:im1-1a}
has been a useful test case for me.
You will see it again in later chapters.
What I would like to show you here is a movie showing the convergence
to Figure \ref{fig:im1-1a}.
Convergence occurs rapidly where data points are close together.
The large gaps, however, fill at a rate of one point per iteration.

\subsection{Abandoned theory for matching wells and seismograms}
\par
Let us consider theory to
construct a map $\bold m$ that fits dense seismic data
$\bold s$ and the well data $\bold w$.
The first goal
$\bold 0 \approx  \bold L \bold m - \bold w$
says that when we linearly interpolate from the map,
we should get the well data.
The second goal
$\bold 0 \approx \bold A (\bold m - \bold s)$
(where $\bold A$ is a roughening operator like $\nabla$ or $\nabla^2$)
says that the map $\bold m$ should match the seismic data $\bold s$
at high frequencies but need not do so at low frequencies.
\begin{equation}
        \begin{array}{lll}
        \bold 0 &\approx & \bold L \bold m - \bold w \\
        \bold 0 &\approx & \bold A (\bold m - \bold s)
        \label{eqn:toldisregression}
        \end{array}
\end{equation}
\par
Although (\ref{eqn:toldisregression}) is the way I originally formulated
the well-fitting application, I abandoned it for several reasons:
First, the map had ample pixel resolution compared to other sources of error,
so I switched from linear interpolation to binning.
Once I was using binning,
I had available the simpler empty-bin approaches.
These approaches have the advantage that it is not necessary
to experiment with the relative weighting between
the two goals in (\ref{eqn:toldisregression}).
A formulation like (\ref{eqn:toldisregression}) is more likely
to be helpful where we need to handle rapidly changing functions
where binning is inferior to linear interpolation,
perhaps in reflection seismology where high resolution is meaningful.

\section{PRECONCEPTION AND CROSS VALIDATION}

First, we first look at data $\bold d$.
Then we think about a model $\bold m$,
and an operator $\bold L$ to link the model and the data.
Sometimes, the operator is merely the first term in a series expansion
about $(\bold m_0,\bold d_0)$.
Then, we fit
$\bold d-\bold d_0 \approx \bold L ( \bold m-\bold m_0)$.
To fit the model, we must reduce the fitting residuals.
Realizing the importance of a data residual
is not always simply the size of the residual,
but is a function of it,
we conjure up (topic for later chapters)
a weighting function (which could be a filter) operator $\bold W$.
With $\bold W$ we define our data residual:
\begin{equation}
\bold r_d \eq \bold W
[ \bold L
        ( \bold m-\bold m_0)
\ -\ 
        ( \bold d-\bold d_0)
]
\end{equation}

\par
Next, we realize that the data might not be adequate to determine the model,
perhaps because our comfortable dense sampling of the model
ill fits our economical sparse sampling of data.
Thus we adopt a fitting goal that mathematicians call ``regularization,''
and we might call a ``model styling'' goal
or more simply,
a quantification of our preconception of the best model.
We quantify our goals  by choosing an operator $\bold A$,
often simply a roughener like a gradient
(the choice again a topic in this and later chapters).
It defines our model residual by
$\bold A \bold m$ or
$\bold A ( \bold m-\bold m_0)$, say we choose:
\begin{equation}
\bold r_m \eq \bold A \bold m 
\end{equation}

\par
In an ideal world,
our model preconception (\bx{prejudice}?)
would not conflict with measured data,
but real life is much more interesting than that.
The reason we pay for data acquisition
is that conflicts between data and preconceived notions invariably arise.
We need an adjustable parameter
that measures our ``\bx{bullheadedness},'' how much we intend
to stick to our preconceived notions in spite of contradicting data.
This parameter is generally called epsilon $\epsilon$,
because we like to imagine that our bullheadedness (prejudice?) is small.
(In mathematics, $\epsilon$ is often taken to be
an infinitesimally small quantity.)
Although any bullheadedness seems like a bad thing,
it must be admitted that measurements are imperfect too.
Thus, as a practical matter, we often find ourselves minimizing:
\begin{equation}
\min \quad := \quad
\bold r_d \cdot \bold r_d \ +\  \epsilon^2\ \bold r_m \cdot \bold r_m 
\end{equation}
and wondering what to choose for $\epsilon$.
I have two suggestions:
My simplest suggestion is to choose $\epsilon$
so that the residual of data fitting matches that of model styling.
Thus:
\begin{equation}
\label{eqn:epsilondata}
\epsilon \eq \sqrt{\frac{ \bold r_d \cdot \bold r_d }{ \bold r_m \cdot \bold r_m }}
\end{equation}
My second suggestion is to think of the force on our final solution.
In physics, force is associated with a gradient.
We have a gradient for the data fitting
and another for the model styling:
\begin{eqnarray}
\bold g_d &=& \bold L\T \bold W\T \bold r_d  \\
\bold g_m &=& \bold A\T \bold r_m
\end{eqnarray}
We could balance these forces by the choice:
\begin{equation}
\label{eqn:epsilongrad}
\epsilon \eq \sqrt{
\bold g_d \cdot \bold g_d
\over
\bold g_m \cdot \bold g_m 
}
\end{equation}

Although we often ignore $\epsilon$ in discussing the formulation
of an application, when time comes to solve the problem, reality intercedes.
Generally, $\bold r_d$ has different physical units than $\bold r_m$
(likewise $\bold g_d$ and $\bold g_m$),
and we cannot allow our solution
to depend on the accidental choice of units
in which we express the problem.
I have had much experience choosing $\epsilon$, but it is
only recently that I boiled it down to the  suggestions of equations (\ref{eqn:epsilondata}) and
(\ref{eqn:epsilongrad}).
Normally I also try other values, like double or half previous choices,
and I examine the solutions for subjective appearance.
The epsilon story continues
in Chapter \ref{paper:prc}.
\par
Computationally, we could choose a new $\epsilon$ with each iteration,
but it is more expeditious
to freeze $\epsilon$, solve the problem,
recompute $\epsilon$, and solve the problem again.
I have never seen a case in which more than one repetition was necessary.

\par
People who work with small applications
(less than about $10^3$ vector components)
have access to an attractive theoretical approach
called ``cross-validation.''
Simply speaking,
we could solve the problem many times,
each time omitting a different data value.
Each solution would provide a model
that could be used to predict
the omitted data value.
The quality of these predictions
is a function of $\epsilon$
which provides a guide to finding it.
My objections to cross validation are two-fold:
First, I do not know how to apply it in the large applications
we solve in this book
(I should think more about it);
and second,
people who worry much about $\epsilon$,
perhaps first should think 
more carefully about
their choice of the filters $\bold W$ and $\bold A$,
which is the focus of this book.
Notice that both $\bold W$ and $\bold A$
can be defined with a scaling factor like $\epsilon$.
Often more important in practice,
with $\bold W$ and $\bold A$
we have a scaling factor that need not be constant but
can be a function of space or spatial frequency
within the data space and/or model space.


\begin{comment}

\section{References}

\reference{Gill, P.E., Murray, W., and Wright, M.H., 1981,
        Practical optimization:  Academic Press.
        }
\reference{Hestenes, M.R., and Stiefel, E., 1952,
        Methods of
        conjugate gradients for solving linear systems:
        J. Res. Natl. Bur. Stand., {\bf 49}, 409-436.
        }
\reference{Luenberger, D.G., 1973,
        Introduction to linear and nonlinear programming:
        Addison-Wesley.
        }
\reference{Nolet, G., 1985,
        Solving or resolving inadequate and noisy
        tomographic systems:
        J. Comp. Phys., {\bf 61}, 463-482.
        }
\reference{Paige, C.C., and Saunders, M.A., 1982a,
        LSQR: an algorithm for sparse linear equations
        and sparse least squares:
        Assn. Comp. Mach. Trans. Mathematical Software,
        {\bf 8,} 43-71.
        }
\reference{Paige, C.C., and Saunders, M.A., 1982b,
        Algorithm 583, LSQR:
        sparse linear equations and least squares problems:
        Assn. Comp. Mach. Trans. Mathematical Software,
        {\bf 8,}  195-209.
        }
%\reference{Press, W.H. et al., 1989,
%        Numerical recipes: the art of scientific computing:
%        Cambridge University Press.
%        }
\reference{Strang, G., 1986,
        Introduction to applied mathematics:
        Wellesley-Cambridge Press.
        }
\end{comment}

\begin{exer}
\item
        Figures~\ref{fig:mlines}--\ref{fig:moscil}
        seem to extrapolate to vanishing signals at the side boundaries.
        Why is that so, and what could be done to leave the sides
        unconstrained in that way?
\item
        Show that the interpolation curve in Figure~\ref{fig:mparab} is not
        parabolic as it appears, but cubic.
        ({\sc hint}:  First show that $(\nabla^2)\T\,\nabla^2 u = \bold 0$.)
        \item
        Verify by a program example that the number of iterations
        required with simple constraints is the number of free parameters.
%       \todo{ idoc to reference printout}
\item
        A signal on a uniform mesh has missing values.
        How should we estimate the mean?
\item
        It is desired to find a compromise between
        the Laplacian roughener
        and the gradient roughener.
        What is the size of the residual space?
        \sx{roughener ! Laplacian}
        \sx{roughener ! gradient}
\item
        Like the seismic prospecting industry,
        you have solved a huge problem using binning.
        You have computer power left over
        to do a few iterations with linear interpolation.
        How much does the cost per iteration increase?
        Should you refine your model mesh,
        or can you use the same model mesh
        that you used when binning?
\item
	Nuclear energy,
	having finally reached its potential,
	has dried up the prospecting industries so you find yourself
	doing
	\bx{medical imaging} (or \bx{earthquake seismology}).
	You probe the human body from all sides
	on a dense regular mesh in cylindrical coordinates.
	Unfortunately,
	you need to represent your data in Fourier space.
	There is no such thing as a fast Fourier transform in cylindrical coordinates,
	while slow Fourier transforms are pitifully slow.
	Your only hope to keep up with your competitors
	is to somehow do your FTs in cartesian coordinates.
	Write down the sequence of steps
	to achieve your goals
	using the methods of this chapter.
\end{exer}


\clearpage

