% copyright (c) 2005 Jon Claerbout

\title{Model fitting by least squares}
\author{Jon Claerbout}

\maketitle

\label{paper:lsq}

%\def\figdir{Fig}
%\def\sx#1{}
%\def\bx{}
%\def\bxbx#1#2{#1}
%\def\eq{\quad =\quad}
\def\ff{{\bf f}}
\def\dd{{\bf d}}

\sx{least squares}
%\begin{notforlecture}
\par
The first level of computer use in science and engineering is \bx{modeling}.
Beginning from physical principles and design ideas,
the computer mimics nature.
Then the worker looks at the result, thinks a while,
alters the modeling program, and tries again.
The next, deeper level of computer use is that the computer 
examines the results of modeling and reruns the modeling job.
This deeper level
is variously called
``\bx{fitting},'' 
``\bx{estimation},'' or
``\bx{inversion}.''
We inspect the \bx{conjugate-direction method} of fitting
and write a subroutine for it that is used in most of
the examples in this book.
%\end{notforlecture}

\section{UNIVARIATE LEAST SQUARES}
%\begin{notforlecture}
A single parameter fitting problem arises in Fourier analysis,
where we seek a ``best answer'' at each frequency,
then combine all the frequencies to get a best signal.
Thus, emerges a wide family of interesting and useful applications.
However, Fourier analysis first requires us to introduce complex numbers
into statistical estimation.
\par
Multiplication in the Fourier domain is \bx{convolution} in the time domain.
Fourier-domain division is time-domain \bx{deconvolution}.
This division is challenging when $F$ has observational error.
Failure erupts if zero division occurs.
More insidious are the poor results we obtain
when zero division is avoided by a near miss.

\subsection{Dividing by zero smoothly}
\sx{divide by zero}
\sx{zero divide}
Think of any real numbers $x$, $y$, and $f$ and any
program containing $x=y/f$.
How can we change the program so that it never divides by zero?
A popular answer is to change $x=y/f$
to $x=yf/(f^2+\epsilon^2)$, where $\epsilon$ is any tiny value.
When $|f| >> |\epsilon|$,
then $x$ is approximately $y/f$ as expected.
But when the divisor $f$ vanishes,
the result is safely zero instead of infinity.
The transition is smooth,
but some criterion is needed to choose the value of $\epsilon$.
This method may not be the only way or the best way
to cope with
\bxbx{zero division}{zero divide},
but it is a good method,
and permeates the subject of signal analysis.
\par
To apply this method in the Fourier domain,
suppose that $X$, $Y$, and $F$ are complex numbers.
What do we do then with $X=Y/F$?
We multiply the
top and bottom by the complex conjugate $\overline{F}$,
and again add $\epsilon^2$ to the denominator.
Thus,
\begin{equation}
X(\omega) \eq
\frac{ \overline{F(\omega)} \ Y(\omega)  }{ \overline{F(\omega)} F(\omega) \ +\ \epsilon^2}
\label{eqn:z1}
\end{equation}
Now, 
the denominator must always be a positive number greater than zero,
so division is always safe.
Equation~(\ref{eqn:z1}) ranges continuously from
\bx{inverse filter}ing, with
\sx{filter ! inverse}
$X=Y/F$, to filtering with $X=\overline{F}Y$,
which is called ``\bx{matched filter}ing.''
\sx{filter ! matched}
Notice that for any complex number $F$,
the phase of $1/F$ equals the phase of $\overline{F}$,
so the filters 
have the same phase.
%
\subsection{Damped solution}
Another way to say $x=y/f$ is to say $fx-y$ is small, or $(fx-y)^2$ is small.
This does not solve the problem of $f$ going to zero,
so we need the idea that $x^2$ does not get too big.
To find $x$, we minimize the quadratic function in $x$.
\begin{equation}
Q(x) \eq (f x-y)^2 + \epsilon^2 x^2
\label{eqn:z2}
\end{equation}
The second term is called a ``\bx{damping} factor,''
because it prevents $x$ from going to $\pm \infty$ when $f\rightarrow 0$.
Set $dQ/dx=0$, which gives:
\begin{equation}
0 \eq f(f x-y) + \epsilon^2 x
\label{eqn:z3}
\end{equation}
Equation (\ref{eqn:z3}) yields our earlier common-sense guess $x=fy/(f^2+\epsilon^2)$.
It also leads us to wider areas of application in which the elements are complex
vectors and matrices.

\par
With Fourier transforms,
the signal $X$ is a complex number at each frequency $\omega$.
Therefore we generalize equation~(\ref{eqn:z2}) to:
\begin{equation}
Q(\bar X, X) \eq
(\overline{FX-Y})        (FX-Y) + \epsilon^2 \bar X X \eq
(\bar X \bar F - \bar Y) (FX-Y) + \epsilon^2 \bar X X
\label{eqn:z4}
\end{equation}
To minimize $Q$, we could use a real-values approach,
where we express
$X=u+iv$ in terms of two real values $u$ and $v$,
and then set $\partial Q/\partial u=0$ and $\partial Q/\partial v=0$.
The approach we take, however,
is to use complex values,
where we set
$\partial Q/\partial X=0$ and $\partial Q/\partial \bar X=0$.
Let us examine $\partial Q/\partial \bar X$:
\begin{equation}
{\partial Q(\bar X, X)\over \partial  \bar X}  \eq
\bar F (FX-Y) + \epsilon^2  X  \eq 0
\label{eqn:z5}
\end{equation}
The derivative $\partial Q/\partial X$ is
the complex conjugate of $\partial Q/\partial \bar X$.
Therefore, if either is zero, the other is also zero.
Thus, we do not need to specify both
$\partial Q/\partial X=0$ and $\partial Q/\partial \bar X=0$.
I usually set
$\partial Q/\partial \bar X$ equal to zero.
Solving equation~(\ref{eqn:z5}) for $X$
gives equation~(\ref{eqn:z1}).

\par
Equation~(\ref{eqn:z1}) solves $Y=XF$ for $X$,
giving the solution for what is called
``the \bx{deconvolution} problem with a known wavelet $F$.''
Analogously, we can use $Y=XF$ when the filter $F$ is unknown,
but the input $X$ and output $Y$ are given.
Simply interchange $X$ and $F$ in the derivation and result.

\subsection{Smoothing the denominator spectrum}

\par
Equation~(\ref{eqn:z1}) gives us one way to divide by zero.
Another way is stated by the equation
\begin{equation}
X(\omega) = \frac{ \overline{F}(\omega) Y(\omega) }
{\left<
\overline{F}(\omega)  F(\omega)
\right> }
\label{eqn:z6}
\end{equation}
where the strange notation in the denominator means
that the spectrum there should be smoothed a little.
Such smoothing fills in the holes in the spectrum
where zero-division is a danger,
filling not with an arbitrary numerical value $\epsilon$
but with an average of nearby spectral values.
Additionally, if the denominator spectrum
$\overline{F}(\omega) F(\omega)$ is rough,
the smoothing creates a shorter autocorrelation function.
\par
Both divisions,
equation~(\ref{eqn:z1}) and
equation~(\ref{eqn:z6}),
irritate us by requiring us to specify a parameter,
but for the latter, the parameter has a clear meaning.
In the latter case we smooth a spectrum with a smoothing
window of width, say $\Delta\omega$
which this corresponds inversely to a time interval over which we smooth.
Choosing a numerical value for  $\epsilon$ has not such a simple interpretation.

\inputdir{antoine}

\par
We jump from simple mathematical theorizing
towards a genuine practical application when I grab some real data,
a function of time and space from another textbook.
Let us call this data $f(t,x)$ and its 2-D Fourier transform
$F(\omega, k_x)$.
The data and its autocorrelation are in Figure~\ref{fig:antoine10}.
\par
The autocorrelation $a(t,x)$ of $f(t,x)$ is
the inverse 2-D Fourier Transform  of 
$\overline{F}(\omega, k_x) F(\omega, k_x)$.
Autocorrelations $a(x,y)$
satisfy the symmetry relation
$a(x,y)=a(-x,-y)$.
Figure~\ref{fig:antoine11}
shows only the interesting quadrant of the two independent quadrants.
We see the autocorrelation of a 2-D function has some
resemblance to the function itself but differs in important ways.

\par
Instead of messing with two different functions $X$ and $Y$ to divide,
let us divide $F$ by itself.
This sounds like $1=F/F$ but we will
watch what happens when we do the division carefully
avoiding zero division in the ways we usually do.
\par
Figure~\ref{fig:antoine11} shows
what happens with
\begin{equation}
\label{eqn:z7}
1 = F/F \quad \approx \quad \frac{\overline{F}F}{\overline{F}F+\epsilon^2}
\end{equation}
and with
\begin{equation}
\label{eqn:z8}
1 = F/F \quad \approx\quad  \frac{\overline{F}F}{\left< \overline{F}F \right>}
\end{equation}
%
\plot{antoine10}{width=\textwidth,height=1.6\textwidth}{
  2-D data (right) and a quadrant of its autocorrelation (left).
  Notice the longest nonzero time lag on the data is about 5.5 sec
  which is the latest nonzero signal on the autocorrelation.
}
%
From Figure~\ref{fig:antoine11} we notice that both methods of
avoiding zero division give similar results.
By playing with the $\epsilon$ and the smoothing width
the pictures could be made even more similar.
My preference, however, is the smoothing.
It is difficult to make physical sense of choosing a numerical value
for $\epsilon$.
It is much easier to make physical sense of choosing a smoothing window.
The smoothing window is in $(\omega,k_x)$ space,
but Fourier transformation tells us its effect in $(t,x)$ space.

\plot{antoine11}{width=\textwidth,height=1.16\textwidth}{
  Equation~\ref{eqn:z7} (left) and
  equation~\ref{eqn:z8} (right).
  Both ways of dividing by zero give similar results.
}

\subsection{Imaging}
The example of dividing a function by itself $(1=F/F)$ might not
seem to make much sense, but it is very closely related to estimation
often encountered in imaging applications.
It's not my purpose here to give a lecture on imaging theory, but
here is an over-brief explanation.
\par
Imagine a downgoing wavefield $D(\omega,x,z)$. Propagating against irregularities in  the medium $D(\omega, x , z )$
creates by scattering  an upgoing wavefield $U(\omega,x,z)$.
Given $U$ and $D$, if there is a strong temporal correlation between them
at any $(x,z)$ it likely means there is a reflector nearby that is
manufacturing $U$ from $D$.
This reflectivity could be quantified by $U/D$.
At the Earth's surface the surface boundary condition says something like
$U=D$ or $U=-D$.   Thus at the surface we have something like $F/F$.
As we go down in the Earth, the main difference is that $U$ and $D$ get
time-shifted in opposite directions, so $U$ and $D$ are similar but
for that time difference.  Thus, a study of how we handle $F/F$ is worthwhile.

\subsection{Formal path to the low-cut filter}

This book defines many geophysical estimation applications.
Many applications amount to fitting two goals.
The first goal is a data-fitting goal,
the goal that the model should imply some observed data.
The second goal is that the model be not too big nor too wiggly.
We state these goals as two residuals, each of which is ideally zero.
A very simple data fitting goal would be that
the model $m$ equals the data $d$,
thus the difference should vanish, say $0\approx  m- d$.
A more interesting goal is that the model should match the data
especially at high frequencies but not necessarily at low frequencies.
\begin{equation}
0 \quad\approx\quad  -i\omega(m - d)
\end{equation}
A danger of this goal is that the model could have a zero-frequency component
of infinite magnitude as well as large amplitudes for low frequencies.
To suppress such bad behavior we need the second goal, a model residual
to be minimized.  We need a small number $\epsilon$.
The model goal is:
\begin{equation}
0 \quad\approx\quad \epsilon \ m
\end{equation}
To see the consequence of these two goals,
we add the squares of the residuals:
\begin{equation}
 Q(m) \eq \omega^2 (m-d)^2 + \epsilon^2  m^2
\end{equation}
and then, we minimize $Q(m)$ by setting its derivative to zero:
\begin{equation}
0\eq {dQ\over dm} \eq 2 \omega^2 (m-d) + 2\epsilon^2  m
\end{equation}
or
\begin{equation}
m \eq  {\omega^2 \over \omega^2+ \epsilon^2}\  d
\label{eqn:lowcut}
\end{equation}
Let us rename $\epsilon$ to give it physical units of frequency $\omega_0 = \epsilon$.
Our expression says
says $m$ matches $d$ except for low frequencies $|m| <|\omega_0|$ where it tends to zero.
Now we recognize we have a low-cut filter with 
``cut-off frequency''  $\omega_0$.

%Our low-pass filter approach in Chapter \ref{ajt/paper:ajt}
%made it quite clear that $\epsilon$ is a filter cutoff
%which might better have been named $\omega_0$.
%We experimented with some objective tests
%for the correct value of $\omega_0$,
%a subject that we will return to later.

%	
%Figure~\ref{fig:antoine11}.
%\activeplot{antoine11}{width=6in,height=8.5in}{ER}{
%        Smoothing the denominator spectrum. Update makefile.
%	}
	
%

\subsection{The plane-wave destructor}
%
We address the question of shifting signals into best alignment. The
most natural approach might seem to be via cross correlations, which
is indeed a good approach when signals are shifted by large amounts.
Here, we assume signals are shifted by small amounts, often less than a
single pixel.  We take an approach closely related to differential
equations. Consider this definition of a residual.
\begin{equation}
0 \quad \approx \quad \hbox{residual}(t,x) \eq \left( \frac{\partial}{\partial x} + p \frac{\partial}{\partial t} \right) u(t,x)
\label{eqn:PWDresidual}
\end{equation}
By taking derivatives we see the residual vanishes when the two-dimensional observation $u(t,x)$ matches the equation of moving waves $u(t-px)$.  The parameter $p$ has units inverse to velocity, the velocity of propagation. 
\par
In practice, $u(t,x)$ might not be a perfect wave but an observed field of many waves that we might wish to fit to the idea of a single wave of a single $p$. We seek the parameter $p$.  First, we need a method of discretization that allows the mesh for $\partial u/\partial t$ to overlay exactly $\partial u /\partial x$.  To this end, I chose to represent the $t$-derivative by averaging a finite difference at $x$ with one at $x+\Delta x$. 
\begin{equation}
\frac{\partial u}{\partial t} \quad \approx \quad \frac{1}{2} 
\left(
\frac{u(t+\Delta t,x) - u(t,x) }{\Delta t}
\right) + \frac{1}{2}
\left(
\frac{u(t+\Delta t,x+\Delta x) - u(t,x+\Delta x) }{\Delta t}
\right)
\end{equation}
Likewise, there is an analogous expression for the $x$-derivative with $t$ and $x$ interchanged.
The function $u(t,x)$ lies on a grid, and the
differencing operator $\delta_x + p\delta_t$ lies atop it and convolves across it.
The operator is a $2\times 2$ convolution filter.
We may represent equation~(\ref{eqn:PWDresidual}) as a matrix operation,
\begin{equation}
{\bf 0} \quad \approx\quad  {\bf r} = {\bf A}{\bf u}
\end{equation}
where the two-dimensional convolution with the difference operator is denoted ${\bf A}$.
\par
%The module {\tt wavekill()}  applies the operator a $\delta_x + b \delta_t$, which can be specialized to the operators we will actually need, namely $\delta_x, \delta_t, \delta_x+p_i\delta_t$. 
%\moddex{wavekill}{wavekill()}
%\par
Now, let us find the numerical value of $p$ that fits a plane wave
$u(t-px)$ to  observations $u (t , x )$. Let $\bold x$ be an abstract
vector having components with values $\partial u /\partial x$ taken
everywhere on a 2-D mesh in $(t,x)$. Likewise, let $\bold t$ contain $\partial u /\partial t$.  Because we want ${\bf x} + p {\bf t} \approx {\bf 0}$, we minimize the quadratic function of $p$, 
\begin{equation}
Q( p) = ({\bf x} + p {\bf t}) \cdot ({\bf x} + p {\bf t}) 
\end{equation}
by setting to zero the derivative by $p$. We get:
\begin{equation}
p \eq - \ \frac{ {\bf x} \cdot {\bf t} }{ {\bf t} \cdot {\bf t} } 
\end{equation}
Because data does not always fit the model very well, it may be helpful to have some way to measure how good the fit is. I suggest:
\begin{equation}
C^2 \eq 1 \ -\  \frac{ ({\bf x} + p {\bf t}) \cdot ({\bf x} + p {\bf t}) }{ {\bf x} \cdot {\bf x}} 
\end{equation}
which, on inserting $p=-({\bf x} \cdot {\bf t})/({\bf t} \cdot {\bf t})$, leads to $C$ , where 
\begin{equation}
C \eq \frac{ {\bf x} \cdot {\bf t} } { \sqrt{ ( {\bf x} \cdot {\bf x} ) ( {\bf t}\cdot{\bf t}) }}
\end{equation} 
is known as the ``{\bf normalized correlation}.''   
%The program for this calculation is straightforward.
%The name {\tt puck2d()}  denotes {\it picking} on a contin{\it uu}m. 

%\moddex{puck2d}{puck2d()}

\inputdir{puck}

To suppress noise, the quadratic functions
$\bold x \cdot \bold x$,
$\bold t \cdot \bold t$, and
$\bold x \cdot \bold t$
were smoothed over time with a triangle filter.

\sideplot{puckin}{width=0.8\textwidth}{
	Input synthetic seismic data includes a low level of noise.}  

\sideplot{residual5}{width=0.8\textwidth}{
	Residuals, i.e., an evaluation of $U_x + pU_t$.}  

\sideplot{puckout}{width=0.8\textwidth}{
	Output values of $p$ are shown by the slope of short line segments.
	}

Subroutine {\tt puck2d} shows the code that generated
Figures~\ref{fig:puckin}--\ref{fig:puckout}.
An example based on synthetic data is shown
in Figures~\ref{fig:puckin} through \ref{fig:puckout}.
The synthetic data in Figure~\ref{fig:puckin} mimics
a reflection seismic field profile,
including one trace that is slightly delayed
as if recorded on a patch of unconsolidated {\bf soil}. 

\par
Figure~\ref{fig:residual5} shows the {\bf residual}.
The residual is small in the central region of the data;
it is large where the signal is not sampled densely enough,
and it is large at the transient onset of the signal.
The residual is rough because of the noise in the signal,
because it is made from derivatives,
and the synthetic data was made by nearest-neighbor interpolation.
Notice that the residual is not particularly large for the delayed trace. 
\par
Figure~\ref{fig:puckout} shows the dips.
The most significant feature of this figure
is the sharp localization of the dips surrounding the delayed trace.
Other methods based on ``beam stacks'' or Fourier concepts
might lead us to conclude that the aperture must be large
to resolve a wide range of angles.
Here, we have a narrow aperture (two traces),
but the dip can change rapidly and widely.
\par
%Finally, an important practical matter.   Taking derivatives
%boosts high frequencies (effectively multiplying by $-i\omega$).
%These high frequencies may have a poorer signal to noise ratio than the raw data.
%To compensate for that, I commonly integrate (or leaky integrate)
%the time axis on the raw data before I begin.

\inputdir{lomask}

\par
Once the stepout $p = d t /d x$ is known between each of the signals, it is a simple matter to integrate to get the total time shift.  A real-life example is shown in Figure~\ref{fig:twod}.
%
\sideplot{twod}{width=0.8\textwidth}{
	A seismic line before and after flattening.}  
%
In this case the flattening was a function of $x$ only.  More interesting (and more complicated) cases arise when the stepout $p = d t /d x$ is a function of both $x$ and $t$.  The code shown here should work well in such cases.
\par
A disadvantage, well known to people who routinely work with finite-difference solutions to partial differential equations, is that for short wavelengths a finite difference operator is not the same as a differential operator; therefore, the numerical value of $p$ is biased.  This problem can be overcome in the following way.  First, estimate the slope $p= d t /d x$ between each trace. Then, shift the traces to flatten arrivals.  Now, there may be a residual $p$ because of the bias in the initial estimate of $p$.  This process can be iterated until the data is flattened.  Everywhere in a plane we have solved a least squares problem for a single value $p$.  
	
\section{MULTIVARIATE LEAST SQUARES}

\subsection{Inside an abstract vector}
In engineering uses,
a vector has three scalar components that
correspond to the three dimensions of the space in which we live.
In least-squares data analysis, a vector is a one-dimensional array
that can contain many different things.
Such an array is an ``\bx{abstract vector}.''
For example, in \bx{earthquake} studies,
the vector might contain the time
an earthquake began, as well as its latitude, longitude, and depth.
Alternatively, the abstract vector
might contain as many components as there are seismometers,
and each component might be the arrival time of an earthquake wave.
Used in signal analysis,
the vector might contain the values of a signal
at successive instants in time or,
alternatively, a collection of signals.
These signals might be ``\bx{multiplex}ed'' (interlaced)
or ``demultiplexed'' (all of each signal preceding the next).
When used in image analysis,
the one-dimensional array might contain an image,
which is an array of signals.
Vectors, including abstract vectors,
are usually denoted by \bx{boldface letters} such as $\bold p$ and $\bold s$.
Like physical vectors,
abstract vectors are \bx{orthogonal}
if a dot product vanishes: $\bold p \cdot \bold s =0$.
Orthogonal vectors are well known in physical space;
we also encounter orthogonal vectors in abstract vector space.
\par
We consider first a hypothetical application
with one data vector $\dd$ and two
fitting vectors $\bold f_1$ and $\bold f_2$.
Each fitting vector is also known as a ``\bx{regressor}.''
Our first task is to approximate the data vector $\dd$
by a scaled combination of the two regressor vectors.
The scale factors $m_1$ and $m_2$
should be chosen so the model matches the data; i.e.,
\begin{equation}
        \dd  \quad \approx \quad \bold f_1 m_1 + \bold f_2 m_2
        \label{eqn:wish1}
\end{equation}
\par
Notice that we could take the partial derivative
of the data in (\ref{eqn:wish1}) with respect to an unknown,
say $m_1$,
and the result is the regressor $\bold f_1$.
The \bx{partial derivative} of all modeled data $d_i$
with respect to any particular model parameter $m_j$
gives a \bx{regressor}.
\par
\boxit{
        A \bx{regressor} is a column in the
        matrix of partial-derivatives, 
        $\partial d_i /\partial m_j$.
        }

\par
The fitting goal (\ref{eqn:wish1}) is often expressed in the more compact
mathematical matrix notation $\dd  \approx \bold F   \bold m$,
but in our derivation here,
we keep track of each component explicitly
and use mathematical matrix notation to summarize the final result.
Fitting the observed data $\bold d = \bold d^{\rm obs}$
to its two theoretical parts
          $\bold f_1m_1$ and $\bold f_2m_2$
can be expressed
as minimizing the length of the residual vector $\bold r$, where:
\begin{eqnarray}
        \bold 0 \quad\approx\quad
        \bold r &=&  \bold d^{\rm theor} -  \bold d^{\rm obs}
        \\
        \bold 0 \quad\approx\quad
        \bold r &=&  \bold f_1 m_1 + \bold f_2 m_2  \ -\ \dd
        \label{eqn:resdef}
\end{eqnarray}

We use a dot product to construct a sum of squares (also called a ``\bx{quadratic form}'')
of the components of the residual vector:
\begin{eqnarray}
Q(m_1,m_2) &=& \bold r \cdot \bold r \\
Q(m_1,m_2) &=&
                   (\ff_1 m_1 + \ff_2 m_2 - \dd )
           \cdot
                   (\ff_1 m_1 + \ff_2 m_2 - \dd )
\label{eqn:allquadratics}
\end{eqnarray}
To find the gradient of the quadratic form $Q(m_1,m_2)$,
you might be tempted to expand out the dot product into all nine terms
and then differentiate.
It is less cluttered, however, to remember the product rule, that:
\begin{equation}
{d\over dx} \bold r \cdot \bold r
\eq
{d\bold r \over dx} \cdot \bold r
+
\bold r
\cdot
{d\bold r \over dx}
\end{equation}
Thus, the gradient of $ Q(m_1,m_2)$  is defined by its two components:

\begin{equation} \label{eqn:Q1comp}
\begin{array}{rcl}
 {\partial Q \over \partial m_1} &= &
                    \ff_1  \cdot (\ff_1 m_1 + \ff_2 m_2 -\dd )  
                         +        (\ff_1 m_1 + \ff_2 m_2 -\dd ) \cdot  \ff_1
 \\
 {\partial Q \over \partial m_2} &= &
                            \ff_2  \cdot (\ff_1 m_1 + \ff_2 m_2 -\dd )  
                         +   (\ff_1 m_1 + \ff_2 m_2 -\dd ) \cdot  \ff_2
\end{array}
\end{equation}
Setting these derivatives to zero and using
$(\ff_1 \cdot \ff_2)=(\ff_2 \cdot \ff_1)$ etc.,
we get
\begin{equation}
\begin{array}{rcl}
(\ff_1 \cdot \dd ) &= & (\ff_1 \cdot \ff_1) m_1 + (\ff_1 \cdot \ff_2)  m_2  \\
(\ff_2 \cdot \dd ) &= & (\ff_2 \cdot \ff_1) m_1 + (\ff_2 \cdot \ff_2)
m_2
\end{array}
\end{equation}
We can use these two equations to solve for
the two unknowns $m_1$ and $m_2$.
Writing this expression in matrix notation, we have:
\begin{equation}
\left[ 
\begin{array}{c}
  (\ff_1 \cdot \dd ) \\ 
  (\ff_2 \cdot \dd ) \end{array} \right] 
\eq \left[ 
\begin{array}{ccc}
  (\ff_1 \cdot \ff_1) & (\ff_1 \cdot \ff_2)  \\
  (\ff_2 \cdot \ff_1) & (\ff_2 \cdot \ff_2)  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  m_1 \\ 
  m_2 \end{array} \right]  \label{eqn:2by2}
\end{equation}
It is customary to use matrix notation without dot products.
For matrix notation
we need some additional definitions.
To clarify these definitions,
we inspect vectors 
$\ff_1$, $\ff_2$, and $\dd$ of three components.
Thus,
\begin{equation}
\bold F \eq [ \ff_1 \quad \ff_2 ] \eq 
\left[ 
\begin{array}{ccc}
  f_{11} & f_{12}  \\
  f_{21} & f_{22}  \\
  f_{31} & f_{32}  \end{array} \right] 
\end{equation}
Likewise, the {\it transposed} matrix $\bold F\T$ is defined by:
\begin{equation}
\bold F\T \eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\end{equation}
Using this matrix ${\bold F\T}$, there is a simple expression
for the gradient calculated in equation~(\ref{eqn:Q1comp}).
It is used in nearly every example in this book.
%
\begin{equation}
\bold g \eq 
\left[
\begin{array}{c}
\frac{\partial Q}{\partial m_1} \\
\frac{\partial Q}{\partial m_2}
\end{array}
\right]
\eq
\left[
\begin{array}{c}
{\bf f}_1 \cdot {\bf r} \\
{\bf f}_2 \cdot {\bf r}
\end{array}
\right]
\eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21}  & f_{31} \\
  f_{12} & f_{22}  & f_{32}
 \end{array} \right] 
\left[
\begin{array}{c}
r_1 \\
r_2 \\
r_3
\end{array}
\right]
\eq
{\bf F}\T\,{\bf r}
\label{eqn:gexample}
\end{equation}
%
In words this expression says, the gradient is found by putting the
residual into the adjoint operator ${\bf g} = {\bf F}\T {\bf
  r}$. Notice, the gradient ${\bf g}$ has the same number of
components as the unknown solution ${\bf m}$, so we can think of the
gradient as a $\Delta {\bf m}$, something we could add to ${\bf m}$
getting ${\bf m}  + \Delta {\bf m}$.  Later, we see how much of
$\Delta {\bf m}$ we want to add to ${\bf m}$.  We reach the best
solution when we find the gradient ${\bf g} = {\bf 0}$ vanishes, which
happens as equation~(\ref{eqn:gexample}) says, when the residual is orthogonal to all the fitting functions (all the rows in the matrix ${\bf F}\T$, the columns in ${\bf F}$, are perpendicular  to ${\bf r}$). 
\par
The matrix in equation (\ref{eqn:2by2})
contains dot products.
Matrix multiplication is an abstract way of representing the dot products:
\begin{equation}
\left[ 
\begin{array}{ccc}
  (\ff_1 \cdot \ff_1) & (\ff_1 \cdot \ff_2)  \\
  (\ff_2 \cdot \ff_1) & (\ff_2 \cdot \ff_2)  \end{array} \right] 
 \eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\left[ 
\begin{array}{ccc}
  f_{11} & f_{12}  \\
  f_{21} & f_{22}  \\
  f_{31} & f_{32}  \end{array} \right] 
\label{eqn:covmat}
\end{equation}
Thus, equation (\ref{eqn:2by2}) without dot products is:
\begin{equation}
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  d_1 \\ 
  d_2 \\ 
  d_3 \end{array} \right]
\eq
\left[ 
\begin{array}{ccc}
  f_{11} & f_{21} & f_{31}  \\
  f_{12} & f_{22} & f_{32}  \end{array} \right] 
\left[ 
\begin{array}{ccc}
  f_{11} & f_{12}  \\
  f_{21} & f_{22}  \\
  f_{31} & f_{32}  \end{array} \right] 
\left[ 
\begin{array}{ccc}
  m_1  \\
  m_2  \end{array} \right] 
\label{eqn:allmats}
\end{equation}
which has the matrix abbreviation:
\begin{equation}
\bold F\T\,\dd \eq ( \bold F\T \; \bold F )  \bold m
\label{eqn:analytic}
\end{equation}
Equation
(\ref{eqn:analytic})
is the classic result of least-squares
fitting of data to a collection of regressors.
Obviously, the same matrix form applies when there are more than
two regressors and each vector has more than three components.
Equation
(\ref{eqn:analytic})
leads to an analytic solution for $\bold m$
using an inverse matrix.
To solve formally for the unknown $\bold m$,
we premultiply by the inverse matrix $( \bold F\T \; \bold F )^{-1}$:

\par
\begin{equation}
\bold m \eq
( \bold F\T \; \bold F )^{-1} \;
\bold F\T\,\dd 
\label{eqn:sln}
\end{equation}
\boxit{
The central result of
\bxbx{least-squares}{least squares, central equation of}
theory is
$\bold m =
( \bold F\T \; \bold F )^{-1} \;
\bold F\T\, \dd $.
We see it everywhere.
}

\par
Let us examine all the second derivatives of $Q(m_1,m_2)$ defined by equation (\ref{eqn:allquadratics}).
Any multiplying $\dd$ does not survive the second derivative, therefore, the terms we are left with are:
\begin{equation}
Q(m_1,m_2) = (\ff_1\cdot\ff_1) m_1^2 +
           2 (\ff_1\cdot\ff_2) m_1 m_2 +
             (\ff_2\cdot\ff_2) m_2^2
\end{equation}
After taking the second derivative, we can organize all these terms in a matrix:
\begin{equation}
\label{eqn:covmat2}
\frac{\partial^2 Q}{\partial m_i\partial m_j} \eq
        \left[
                \begin{array}{cc}
                        (\ff_1\cdot  \ff_1) &  (\ff_1 \cdot \ff_2)   \\
                        (\ff_2\cdot  \ff_1) &  (\ff_2 \cdot \ff_2)  
                \end{array}
        \right]
\end{equation}
Comparing equation (\ref{eqn:covmat2}) to equation (\ref{eqn:covmat}) we conclude
that $\bold F\T\,\bold F$ is a matrix of second derivatives.
This matrix is also known as the
\bxbx{Hessian}{Hessian}.
It often plays an important role in small problems.

\par
Larger problems tend to have insufficient computer memory for the Hessian matrix,
because it is the size of model space squared.
Where model space is a multidimensional Earth image,
we have a large number of values even before squaring that number.
Therefore, this book rarely works with the Hessian,
working instead with gradients.
\par
Rearrange parentheses representing (\ref{eqn:allmats}).
\begin{equation}
\bold F\T\,\dd \eq  \bold F\T \; (\bold F   \bold m )
\label{eqn:comp}
\end{equation}
Equation
(\ref{eqn:analytic})
led to the ``analytic'' solution (\ref{eqn:sln}).
In a later section on conjugate directions,
we see that equation
(\ref{eqn:comp})
expresses better than
equation
(\ref{eqn:sln})
the philosophy of iterative methods.

\par
Notice how equation
(\ref{eqn:comp})
invites us to cancel the matrix
$\bold F\T$
from each side.
We cannot do that of course, because
$\bold F\T$
is not a number, nor is it a square matrix with an inverse.
If you really want to cancel the matrix $\bold F\T$, you may,
but the equation is then only an approximation
that restates our original goal (\ref{eqn:wish1}):
\begin{equation}
\dd  \quad \approx \quad \bold F   \bold m 
\label{eqn:wish2}
\end{equation}

\par
Speedy problem solvers might
ignore the mathematics covering the previous page,
study their application until they
are able to write the statement of goals
(\ref{eqn:wish2}) = (\ref{eqn:wish1}),
\sx{statement of goals}\sx{goals, statement of}
premultiply by $\bold F\T$,
replace $\approx$ by =,
getting~(\ref{eqn:analytic}),
and take
(\ref{eqn:analytic})
to a simultaneous equation-solving program to get $\bold m$.

\par
What I call ``\bx{fitting goals}'' are called
``\bx{regressions}'' by statisticians.
In common language the word regression means
to ``trend toward a more primitive perfect state,''
which vaguely resembles reducing the size of (energy in)
the residual $\bold r = \bold F \bold m - \bold d $.
Formally,
the fitting is often written as:
\begin{equation}
\min_{\bold m} \ \  \| \bold F \bold m - \bold d \| 
\label{eqn:norm}
\end{equation}
The notation with two pairs of vertical lines
looks like double absolute value,
but we can understand it as a reminder to square and sum all the components.
This formal notation is more explicit
about what is constant and what is variable during the fitting.

\subsection{Normal equations}
\par

An important concept is that when energy is minimum,
the residual is orthogonal to the fitting functions.
The fitting functions are the column vectors
$\bold f_1$, $\bold f_2$, and $\bold f_3$.
Let us verify only that the dot product $ \bold r \cdot \bold f_2 $ vanishes;
to do so, we show
that those two vectors are orthogonal.
Energy minimum is found by:
\begin{equation}
0  \quad = \quad {\partial\over \partial m_2}\ \bold r \cdot \bold r
   \quad = \quad 2\; \bold r \cdot {\partial \bold r\over \partial m_2}
   \quad = \quad 2\; \bold r \cdot \bold f_2
\label{eqn:orthogtofit}
\end{equation}
(To compute the derivative, refer to equation~(\ref{eqn:resdef}).)
Equation (\ref{eqn:orthogtofit}) shows that
the residual is orthogonal to a fitting function.
The fitting functions are the column vectors in the fitting matrix.

\par
The basic least-squares equations are often called
the ``normal'' equations. \sx{normal equations}
The word ``normal'' means perpendicular.
We can rewrite equation
(\ref{eqn:comp})
to emphasize the perpendicularity.
Bring both terms to the right,
and recall the definition of the residual $\bold r$
from equation (\ref{eqn:resdef}):
\begin{eqnarray}
\label{eqn:fitorth1}
\bold 0  &=& \bold F\T\,( \bold F   \bold m - \dd)  \\
\bold 0  &=& \bold F\T\,\bold r 
\label{eqn:fitorth2}
\end{eqnarray}
Equation (\ref{eqn:fitorth2}) says that the \bx{residual} vector $\bold r$
is perpendicular to
each row in the $\bold F\T$ matrix.
These rows are the \bx{fitting function}s.
Therefore, the residual, after it has been minimized,
is perpendicular to
{\it all}
the fitting functions.

%\begin{notforlecture}
\subsection{Differentiation by a complex vector}

\sx{differentiate by complex vector}
\sx{complex vector}
\sx{complex operator}
\par
Complex numbers frequently arise in physical applications,
particularly those with Fourier series.
Let us extend the multivariable least-squares theory
to the use of complex-valued unknowns $\bold m$.
First,
recall how complex numbers were handled
with single-variable least squares;
i.e.,~as in the discussion leading up to equation~(\ref{eqn:z5}).
Use an asterisk, such as $\bold m\T$, to denote the complex conjugate
of the transposed vector $\bold m$.
Now, write the positive \bx{quadratic form} as:
\begin{equation}
Q(\bold m\T, \bold m) \eq
(\bold F\bold m - \bold d)\T\,
(\bold F\bold m - \bold d)
\eq
(\bold m\T\,\bold F\T - \bold d\T)
(\bold F\bold m - \bold d)
\label{eqn:6-1-23}
\end{equation}

Recall from equation (\ref{eqn:z4}), where
we minimized a quadratic form $Q(\bar X,X)$
by setting to zero, both
$\partial Q/\partial \bar X$ and $\partial Q/\partial X$.
We noted that only one of
$\partial Q/\partial \bar X$ and $\partial Q/\partial X$
is necessarily zero,
because these terms are conjugates of each other.
Now, take the derivative of $Q$
with respect to the (possibly complex, row) vector $\bold m\T$.
Notice that $\partial Q/\partial  \bold m\T$ is the complex conjugate transpose
of $\partial Q/\partial  \bold m$.
Thus, setting one to zero also sets the other to zero.
Setting $\partial Q/\partial \bold m\T =\bold 0$ gives the normal equations:
\begin{equation}
\bold 0 \eq \frac{\partial Q}{\partial \bold m\T} \eq
\bold F\T\,(\bold F \bold m - \bold d)
\label{eqn:normeq}
\end{equation}
The result is merely the complex form of
our earlier result (\ref{eqn:fitorth1}).
Therefore,
differentiating by a complex vector
is an abstract concept,
but it gives the same set of equations
as differentiating by each scalar component,
and it saves much clutter.
%\end{notforlecture}

%\begin{notforlecture}
\subsection{From the frequency domain to the time domain}
Where data fitting uses
the notation $\bold m \rightarrow \bold d$,
linear algebra and signal analysis often use
the notation $\bold x \rightarrow \bold y$.
Equation~(\ref{eqn:z4}) is a frequency-domain quadratic form
that we minimized by varying a single parameter,
a Fourier coefficient.
Now, we look at the same problem in the time domain.
We see that the time domain offers flexibility with
boundary conditions, constraints, and weighting functions.
The notation is that a filter $f_t$ has input $x_t$ and output $y_t$.
In Fourier space, it is expressed $Y=XF$.
There are two applications to look at,
unknown filter $F$ and unknown input $X$.

\subsubsection{Unknown filter}
When inputs and outputs are given,
the problem of finding an unknown filter appears to be overdetermined,
so we write $\bold y \approx \bold X \bold f$
where the matrix $\bold X$ is a matrix of downshifted columns like
(\ref{eqn:contran2}).
Thus, the quadratic form to be minimized
is a restatement of equation~(\ref{eqn:6-1-23})
with filter definitions:
\begin{equation}
Q(\bold f\T, \bold f) \eq
(\bold X\bold f - \bold y)\T\,
(\bold X\bold f - \bold y)
\end{equation}
The solution $\bold f$ is found just as we found
(\ref{eqn:normeq}),
and it is the set of simultaneous equations
$ \bold 0 = \bold X\T\,(\bold X\bold f - \bold y)$.

\subsubsection{Unknown input: deconvolution with a known filter}
\sx{deconvolution}
For solving the unknown-input problem,
we put the known filter $f_t$ in a matrix of downshifted columns $\bold F$.
Our statement of wishes is now to find $x_t$ so that
$\bold y \approx \bold F \bold x$.
We can expect to have trouble finding unknown inputs $x_t$
when we are dealing with certain kinds of filters,
such as \bx{bandpass filter}s.
If the output is zero in a frequency band,
we are never able to find the input in that band
and need to prevent $x_t$ from diverging there.
We prevent divergence by the statement that we wish
$\bold 0\approx\epsilon\,\bold x$,
where $\epsilon$ is a parameter that is small
with exact size chosen by experimentation.
Putting both wishes into a single, partitioned matrix equation gives:
\begin{equation}
 \left[ 
  \begin{array}{c}
   \bold 0 \\ 
   \bold 0
  \end{array}
 \right] 
\quad\approx\quad
 \left[ 
  \begin{array}{c}
   \bold r_1 \\ 
   \bold r_2
  \end{array}
 \right] 
\eq
 \left[ 
  \begin{array}{c}
   \bold F \\ 
   \epsilon \  \bold I
  \end{array}
 \right] 
 \ 
 \bold x
\quad - \quad
 \left[ 
  \begin{array}{c}
   \bold y \\ 
   \bold 0
  \end{array}
 \right] 
\end{equation}
To minimize the residuals $\bold r_1$ and $\bold r_2$,
we can minimize the scalar
$\bold r\T\, \bold r = \bold r_1\T\, \bold r_1 + \bold r_2\T\, \bold r_2$.
Expanding:
\begin{eqnarray}
Q(\bold x\T, \bold x) &=& (\bold F  \bold x  - \bold y)\T\,(\bold F\bold x-\bold y)
                                        + \epsilon^2 \bold x\T\,\bold x
                                                                \nonumber \\
                     &=& (\bold x\T\,\bold F\T - \bold y\T) (\bold F\bold x-\bold y)
                                        + \epsilon^2 \bold x\T\,\bold x
\label{eqn:knownfilt}
\end{eqnarray}
We solved this minimization
in the frequency domain
(beginning from equation~(\ref{eqn:z4})).
\par
Formally the solution is found just as with equation (\ref{eqn:normeq}),
but this solution looks unappealing in practice
because there are so many unknowns and 
the problem can be solved much more quickly
in the Fourier domain.
To motivate ourselves to solve this problem in the time domain,
we need either to find an approximate solution method that is
much faster, or find ourselves with an application 
that needs boundaries,
or needs time-variable weighting functions.

%\item
%Try other lags in~\EQN{2conv} such as
%$(0,1,0)'$ and 
%$(0,0,1)'$.
%Which works best?
%Why?


\section{KRYLOV SUBSPACE ITERATIVE METHODS}
The \bx{solution time} for simultaneous \bx{linear equations}
grows cubically with the number of unknowns.
There are three regimes for solution;
which regime is applicable
depends on the number of unknowns in $m$.
For $m$ three or less, we use analytical methods.
We also sometimes use analytical methods on matrices of size $4\times 4$
if the matrix contains many zeros.
My 1988 desktop workstation solved a $100 \times 100$
system in a minute.
Ten years later, it would do a $600\times 600$ system in roughly a minute.
A nearby more powerful computer would do
1,000 $\times$ 1,000 in a minute.
Because the computing effort increases with the third power of the size,
and because $4^3=64\approx 60$,
an hour's work solves a four times larger matrix,
namely 4,000 $\times$ 4,000 on the more powerful machine.
For significantly larger values of $m$,
exact numerical methods must be abandoned
and \bx{iterative method}s must be used.
\par
The compute time for a rectangular matrix is slightly more pessimistic.
It is the product of the number of data points $n$
times the number of model points squared $m^2$ which is also the cost of computing the matrix
$\bold F\T\,\bold F$ from $\bold F$.
Because the number of data points generally exceeds the number of model
points $n>m$ by a substantial factor
(to allow averaging of noises),
it leaves us with significantly fewer than 4,000 points in model space.
\par
A square image packed into a 4,096-point vector is a $64\times 64$ array.
The computer power for linear algebra to give us solutions that
fit in a $k\times k$ image is thus proportional
to $k^6$, which means that even though computer power grows rapidly,
imaging resolution using ``exact numerical methods'' hardly
grows at all from our $64\times 64$ current practical limit.

\par
The retina in our eyes captures an image of size roughly 1,000 $\times$ 1,000
which is a lot bigger than $64\times 64$.
Life offers us many occasions in which final images exceed the 4,000
points of a $64\times 64$ array.
To make linear algebra (and inverse theory) relevant to such applications,
we investigate special techniques.
A numerical technique known as the
``\bx{conjugate-direction method}''
works well for all values of $m$ and is our subject here.
As with most simultaneous equation solvers,
an exact answer (assuming exact arithmetic)
is attained in a finite number of steps.
And, if $n$ and $m$ are too large to allow enough iterations,
the iterative methods can be interrupted at any stage,
the partial result often proving useful.
Whether or not a partial result actually is useful
is the subject of much research;
naturally, the results vary from one application to the next.

\subsection{Sign convention}
On the last day of the survey, a storm blew up,
the sea got rough, and the receivers drifted further downwind.
The data recorded that day
had a larger than usual difference
from that predicted by the final model.
We could call
$(\bold d-\bold F\bold m)$
the {\it \bx{experimental error}}.
(Here,
$\bold d$ is data,
$\bold m$ is model parameters, and
$\bold F$ is their linear relation.)

\par
The alternate view is that our theory was too simple.
It lacked model parameters for the waves and the drifting cables.
Because of this model oversimplification,
we had a {\it \bx{modeling error}} of the opposite polarity
$(\bold F\bold m-\bold d)$.
\par
Strong experimentalists prefer to think of the error
as experimental error, something for them to work out.
Likewise, a strong analyst likes to think
of the error as a theoretical problem.
(Weaker investigators might be inclined to take the opposite view.)

\par
Opposite to common practice,
I define the \bx{sign convention} for the error (or residual) as
$(\bold F\bold m-\bold d)$.
Here is why.
Minus signs are a source of confusion and errors.
Putting the minus sign on the field data limits it to one location,
while putting it in model space would spread it into as many parts
as model space has parts.

\begin{comment}
So in this book
we see positive signs on operators
and we see residuals initialized by the negative of the data,
often with subroutine \texttt{negcopy()}.
\progdex{negcopy}{copy and negate}
\end{comment}
\par
\boxit{
        Beginners often feel disappointment
        when the data does not fit the model very well.
        They see it as a defect in the data
        instead of an opportunity
        to discover what our data contains
	that our theory does not.
        }
\par

\subsection{Method of random directions and steepest descent}
\sx{random directions} \sx{steepest descent}
\par
Let us minimize the sum of the squares of the components
of the \bx{residual} vector given by:
\begin{eqnarray}
{\rm residual}
&=& \quad\quad
{\rm 
\ \ \hbox{transform} \ \ \  \ \quad \hbox{model space} 
\ \ \ -\ \
\ \hbox{data space}
}
        \\                                                                               
\left[
%\matrix { \matrix { \cr \cr \bold r  \cr \cr \cr } }
\begin{array}{c}
\, \\
\, \\
\bold r\\
\, \\
\, \\
\,
\end{array}
\right]
       &=&
       \ \ 
\left[
\begin{array}{cccccc}
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \bold F \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \, \\
\, & \,& \,&     \,& \,& \,
%  \Matrix {
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr \bold F \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    \matrix { \cr \cr         \cr \cr \cr }
%    }
\end{array}
  \right]
 \ \ \ \
\left[
\begin{array}{c}
\, \\
\, \\
\bold x\\
\, \\
\, \\
\,
\end{array}
%  \matrix {
%    \matrix {  \cr \bold x \cr \cr }
%    }
  \right]
  \ \ \ 
\ \ \ \ \ -\ \ \ \ \ 
\left[
\begin{array}{c}
\, \\
\, \\
\bold d\\
\, \\
\, \\
\,
\end{array}
%  \matrix { 
%    \matrix {  \cr  \cr \bold d \cr  \cr  \cr}
%    }
  \right]
\label{eqn:cg1b}
\end{eqnarray}
\par
A contour plot is based on an altitude function of space.
The altitude is the \bx{dot product}  $\bold r \cdot \bold r$.
By finding the lowest altitude,
we are driving the residual vector  $\bold r$  as close as we can to zero.
If the residual vector  $\bold r$  reaches zero, then we have solved
the simultaneous equations  $\bold d= \bold F \bold x$.
In a two-dimensional world, the vector  $\bold x$  has two components,
$(x_1,x_2)$.
A contour is a curve of constant
$\bold r \cdot \bold r$  in $(x_1 , x_2 )$-space.
These contours have a statistical interpretation as contours
of uncertainty in $(x_1 , x_2 )$, with measurement errors in $\bold d$.
\par
Let us see how a random search-direction
can be used to reduce the residual
$\bold 0\approx \bold r= \bold F \bold x - \bold d$.
Let $\Delta \bold x$ be an abstract vector
with the same number of components as the solution $\bold x$,
and let $\Delta \bold x$ contain arbitrary or random numbers.
We add an unknown quantity $\alpha$
of vector $\Delta \bold x$ to the vector $\bold x$,
and thereby create $\bold x_{\rm new}$:
\begin{equation}
\bold x_{\rm new} \eq \bold x+\alpha \Delta \bold x
\label{eqn:oldx}
\end{equation}
The new $\bold x$ gives a new residual:
\begin{eqnarray}
\bold r_{\rm new} &=& \bold F\ \bold x_{\rm new}           -\bold d \\
\bold r_{\rm new} &=& \bold F( \bold x+\alpha\Delta\bold x)-\bold d\\
\bold r_{\rm new} \eq
\bold r+\alpha \Delta\bold r
                  &=& (\bold F \bold x-\bold d)
                                                +\alpha\bold F\Delta\bold x 
\label{eqn:resupdatelin}
\end{eqnarray}
which defines $\Delta \bold r = \bold F \Delta \bold x$.

\par
Next, we adjust $\alpha$ to minimize the dot product:
$ \bold r_{\rm new} \cdot \bold r_{\rm new} $
\begin{equation}
(\bold r+\alpha\Delta \bold r)\cdot (\bold r+\alpha\Delta \bold r) \eq
\bold r\cdot \bold r + 2\alpha (\bold r \cdot \Delta \bold r) \ +\ 
\alpha^2 \Delta \bold r \cdot \Delta \bold r
\label{eqn:mindot}
\end{equation}
Set to zero its derivative with respect to  $\alpha$:
\begin{equation}
0\eq 
2 \bold r \cdot \Delta \bold r  + 2\alpha \Delta \bold r \cdot \Delta \bold r
\label{eqn:newresperp}
\end{equation}
which says that
the new residual $\bold r_{\rm new} = \bold r + \alpha \Delta \bold r$ is
perpendicular to the ``fitting function'' $\Delta \bold r$.
Solving gives the required value of $\alpha$.
\begin{equation}
\alpha \eq - \ { (\bold r \cdot \Delta \bold r ) \over
( \Delta \bold r \cdot \Delta \bold r ) }
\label{eqn:alfa}
\end{equation}

\par
A ``computation \bx{template}'' for the method of random directions is:
\def\padarrow{\quad\longleftarrow\quad}
\label{lsq/'randtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow {\rm random\ numbers}$          \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \  \Delta \bold x$      \\
\>      \> $\alpha \padarrow
                -(       \bold r \cdot \Delta\bold r )/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\Delta \bold x$       \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}
A nice thing about the method of random directions is that you
do not need to know the adjoint operator $\bold F\T$.

\par
In practice, random directions are rarely used.
It is more common to use the \bx{gradient} direction than a random direction.
Notice that a vector of the size of $\Delta \bold x$ is:
\begin{equation}
\bold g \eq  \bold F\T\,\bold r
\end{equation}
Recall this vector can be found by taking the gradient
of the size of the residuals:
\begin{equation}
{\partial \over  \partial \bold x\T }  \ \bold r \cdot \bold r
\eq
{\partial \over  \partial \bold x\T }  \ 
( \bold x\T \, \bold F\T  \ -\  \bold d\T) \,
( \bold F  \, \bold x   \ -\  \bold d)
\eq
\bold F\T \  \bold r
\end{equation}
Choosing $\Delta \bold x$ to be the gradient vector
$\Delta\bold x = \bold g = \bold F\T\,\bold r$
is called ``the method of \bx{steepest descent}.''

\par
Starting from a model $\bold x = \bold m$ (which may be zero),
the following is a \bx{template} of pseudocode for minimizing the residual
$\bold 0\approx \bold r = \bold F \bold x - \bold d$
by the steepest-descent method:
\label{lsq/'sdtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow \bold F\T\         \bold r$      \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \  \Delta \bold x$      \\
\>      \> $\alpha \padarrow
                -(       \bold r \cdot \Delta\bold r )/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\Delta \bold x$       \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}


\par
Good science and engineering is finding something unexpected.
Look for the unexpected both in data space and in model space.
In data space, you look at the residual $\bold r$.
In model space, you look at the residual projected there $\bold F\T\,\bold r$.
What does it mean?
It is simply $\Delta m$,
the changes you need to make to your model.
It means more in later chapters,
where the operator $\bold F$ is a column vector of operators
that are fighting with one another to grab the data.



\subsection{Why steepest descent is so slow}
Before we can understand why the \bx{conjugate-direction method} is so fast,
we need to see why the
\bxbx{steepest-descent method}{steepest descent}
is so slow.
The process of selecting $\alpha$ is called ``\bx{line search},''
but for a linear problem like the one we have chosen here,
we hardly recognize choosing $\alpha$ as searching a line.
A more graphic understanding of the whole process is possible
from considering a two-dimensional space,
where the vector of unknowns $\bold x$
has just two components, $x_1$ and $x_2$.
Then, the size of the residual vector $\bold r \cdot \bold r$ can be
displayed with a contour plot in the plane of $(x_1,x_2)$.
Figure \ref{fig:yunyue}
shows a contour plot of the penalty function
of $(x_1,x_2)=(m_1,m_2)$.
The gradient is perpendicular to the contours.
Contours and gradients are {\it curved lines}.
When we use the steepest-descent method, we start at a point
and compute the gradient direction at that point.
Then, we begin a %
\it straight-line %
\rm descent in that direction.
The gradient direction curves away from our direction of travel,
but we continue on our straight line
until we have stopped descending and are about to ascend.
There we stop, compute another gradient vector,
turn in that direction, and descend along a new straight line.
The process repeats until we get to the bottom
or until we get tired.

\inputdir{Matlab}
\sideplot{yunyue}{width=0.9\textwidth}{
	Route of steepest descent (black)
	and route of conjugate direction (light grey or red).
	}

What could be wrong with such a direct strategy?
The difficulty is at the stopping locations.
These locations occur where the descent direction
becomes %
\it parallel %
\rm to the contour lines.
(There the path becomes level.)
So, after each stop, we turn $90^\circ$
from parallel to perpendicular to the local contour line
for the next descent.
What if the final goal is at a $45^\circ$ angle to our path?
A $45^\circ$ turn cannot be made.
Instead of moving like a rain drop down the centerline of a rain gutter,
we move along a fine-toothed zigzag path,
crossing and recrossing the centerline.
The gentler the slope of the rain gutter,
the finer the teeth on the zigzag path.
%\todo{Missing figure (ls-sawtooth) A search path for steepest
%  descent.}

\par

\subsection{Null space and iterative methods}
In applications where we fit
$\bold d \approx\bold F \bold x$,
there might exist a vector (or a family of vectors)
defined by the condition $\bold 0 =\bold F \bold x_{\rm null}$.
This family is called a \bx{null space}.
For example, if the operator $\bold F$ is a time derivative,
then the null space is the constant function;
if the operator is a second derivative,
then the null space has two components, a constant function
and a linear function, or combinations of both.
The null space is a family of model components that have no effect on the data.
\par
When we use the steepest-descent method,
we iteratively find solutions by this updating:
\begin{eqnarray}
\bold x_{i+1} &=& \bold x_i + \alpha \Delta \bold x                     \\
\bold x_{i+1} &=& \bold x_i + \alpha \bold F\T\,\bold r                   \\
\bold x_{i+1} &=& \bold x_i + \alpha \bold F\T\,(\bold F\bold x -\bold d)
\end{eqnarray}
After we have iterated to convergence,
the gradient
$ \Delta \bold x=\bold F\T\, \bold r$
vanishes.
Adding any $\bold x_{\rm null}$ to $\bold x$
does not change the residual
$\bold r=\bold F\bold x -\bold d$.
Because $\bold r$ is unchanged,
$ \Delta \bold x=\bold F\T\, \bold r$
remains zero and $\bold x_{i+1} =\bold x_i$.
Thus, we conclude that any null space in the initial guess $\bold x_0$
remains there unaffected by the gradient-descent process.
So, in the presense of null space,
the answer we get from an iterative method
depends on the starting guess.
Oops!
The analytic solution does not do any better.
It needs to deal with a singular matrix.
Existence of a null space destroys the uniqueness of any resulting model.
\par
Linear algebra theory enables us to dig up the entire null space
should we so desire.
On the other hand, the computer demands might be vast.
Even the memory for holding the many $\bold x$ vectors could be prohibitive.
A much simpler and more practical goal
is to find out if the null space has any members,
and if so, to view some members.
To try to see a member of the null space,
we take two starting guesses,
and we run our iterative solver for each.
If the two solutions,
$\bold x_1$ and $\bold x_2$,
are the same, there is no null space.
If the solutions differ, the difference
is a member of the null space.
Let us see why:
Suppose after iterating to minimum residual we find:
\begin{eqnarray}
\bold r_1 &=& \bold F\bold x_1 - \bold d
\\
\bold r_2 &=& \bold F\bold x_2 - \bold d 
\end{eqnarray}
We know that the residual squared is a convex quadratic function
of the unknown $\bold x$.
Mathematically that means the minimum value is unique,
so $\bold r_1 =\bold r_2$.
Subtracting,
we find
$0=\bold r_1-\bold r_2 =\bold F(\bold x_1-\bold x_2)$
proving that $\bold x_1-\bold x_2$ is a model in the null space.
Adding $\bold x_1-\bold x_2$ to any to any model $\bold x$
does not change the modeled data.
%Are you having trouble visualizing $\bold r$ being unique,
%but $\bold x$ not being unique?  Imagine that $\bold r$
%happens to be independent of one of the components of $\bold x$.
%That component is non-unique.
%More generally, it is some linear combination of components of $\bold x$
%that $\bold r$ is independent of.

\par
\boxit{
        A practical way to learn about the existence of null spaces
        and see samples is to try
        gradient-descent methods
        beginning from various different starting guesses.
        }
\par
``Did I fail to run my iterative solver long enough?'' is
a question you might have.
If two residuals from two starting solutions are not equal,
$\bold r_1 \ne \bold r_2$,
then you should be running your solver through more iterations.
\par
\boxit{
        If two different starting solutions
        produce two different residuals,
        then you did not run your solver through enough iterations.
        }

\subsection{The magical property of the conjugate direction method}
In the \bx{conjugate-direction method}, not a line, but rather a plane,
is searched.
A plane is made from an arbitrary linear combination of two vectors.
One vector is chosen to be the gradient vector, say  $\bold g$.
The other vector is chosen to be the previous descent step vector,
say  $\bold s = \bold x_j - \bold x_{j-1}$.
Instead of  $\alpha \, \bold g$,  we need a linear combination,
say  $\alpha \bold g + \beta  \bold s$.
For minimizing quadratic functions the \bx{plane search} requires
only the solution of a two-by-two set of linear equations
for  $\alpha$  and  $\beta$.
\par
The conjugate-direction (CD) method described in this book
has a magical property shared by the more famous conjugate-gradient method.
This magical property is not proven in this book,
but it may be found in many sources.
Although both methods are iterative methods,
both converge on the exact answer (assuming perfect numerical precision)
in a fixed number of steps.
That number is the number of components in model space $\bold x$.

%\begin{comment}
%\subsection{Magic (abandoned)}
%Some properties of the conjugate-gradient and the conjugate-direction approach
%are well known but hard to explain.
%D. G. \bx{Luenberger}'s book,
%{\it Introduction to Linear and Nonlinear Programming},
%is a good place to find formal explanations of this magic.
%(His book also provides other forms of these algorithms.)
%Another helpful book is \bx{Strang}'s {\it Introduction to Applied Mathematics}.
%Known properties follow:
%\begin{itemize}
%\item[{1.}]
%The \bx{conjugate-gradient method} and the
%\bx{conjugate-direction method}
%get the exact answer
%(assuming exact arithmetic) in  $n$  descent steps (or less),
%where  $n$  is the number of unknowns.
%\item[{2.}]
%It is helpful to use the previous step,
%so you might wonder why not use the previous two steps,
%because it is not hard to solve a three-by-three set
%of simultaneous linear equations.
%It turns out that the third direction does not help:
%the distance moved in the extra direction is zero.
%\end{itemize}
%\end{comment}

\par
Where we benefit from iterative methods is where convergence is more rapid than
the theoretical requirement.
%$m$ iterations.
Whether or not that happens, depends on the problem at hand.
Reflection seismology has many problems so massive they are said
to be solved simply by one application of the adjoint operator.
The idea that such solutions might be improved
by a small number of iterations is very appealing.


\subsection{Conjugate-direction theory for programmers}
Fourier-transformed variables are often capitalized.
This convention is helpful here,
so in this subsection only,
we capitalize vectors transformed by the  $\bold F$  matrix.
As everywhere, a matrix, such as $\bold F$,
is printed in {\bf boldface} type
but in this subsection,
vectors are {\it not} printed in boldface.
Thus, we define the solution, the solution step
(from one iteration to the next),
and the gradient by:
\begin{eqnarray}
X   &=& \bold F \  x\   \quad\quad\quad  {\rm modeled\ data\ } = \bold F\ {\rm model}
\\
S_j &=& \bold F \  s_j  \quad\quad\quad  {\rm solution\ change}    \\
G_j &=& \bold F \  g_j  \quad\quad\quad  \Delta\bold r = \bold F\Delta\bold m
\end{eqnarray}
A linear combination in solution space,
say  $s+g$,  corresponds to  $S+G$  in the conjugate space, the data space,
because $S+G = \bold F s + \bold F g = \bold F(s+g)$.
According to equation 
(\ref{eqn:cg1b}),
the residual is the modeled data minus the observed data.
\begin{equation}
R \eq \bold F  x \ -\ D
  \eq          X \ -\ D
\end{equation}
The solution  $x$  is obtained by a succession of steps  $s_j$, say:
\begin{equation}
x \eq s_1 \ +\  s_2 \ +\  s_3 \ +\  \cdots
\end{equation}
The last stage of each iteration is to update the solution and the residual:
\begin{eqnarray}
\label{eqn:xupdate}
{\rm solution\ update:} \quad\quad\quad  & x \ \leftarrow  x&  +\  s\\
\label{eqn:Rupdate}
{\rm residual\ update:} \quad\quad\quad  & R \ \leftarrow  R&  +\  S
\end{eqnarray}

\par
The {\it gradient} vector $g$ is a vector with the same number
of components as the solution vector $x$.
A vector with this number of components is:
\begin{eqnarray}
g &=& \bold F\T \  R \eq \hbox{gradient}                 \label{eqn:g} \\
G &=& \bold F  \  g \eq \hbox{conjugate gradient}   \ =\ \Delta r     \label{eqn:G}
\end{eqnarray}
The gradient $g$ in the transformed space is $G$,
also known as the \bx{conjugate gradient}.
\par
What is our solution update $\Delta \bold x=\bold s$?
It is some unknown amount $\alpha$ of the gradient $\bold g$ plus
another unknown amount         $\beta$  of the previous step $\bold s$.
Likewise in residual space.
\begin{eqnarray}
\Delta \bold x &=& \alpha \bold g + \beta \bold s \quad\quad \hbox{model space}
\\
\Delta \bold r &=& \alpha \bold G + \beta \bold S \quad\quad \hbox{data space}
\end{eqnarray}

\par
The minimization (\ref{eqn:mindot}) is now generalized
to scan not only in a line with $\alpha$,
but simultaneously another line with $\beta$.
The combination of the two lines is a plane.
We now set out to find the location in this plane that minimizes the quadratic $Q$.
\begin{equation}
Q(\alpha ,\beta ) \eq
( R + \alpha G + \beta S) \ \cdot\  (R + \alpha G + \beta S )
\label{eqn:cgqf}
\end{equation}
The minimum is found at  $\partial Q / \partial \alpha \,=\,0$  and
$\partial Q / \partial \beta \,=\,0$, namely,
\begin{eqnarray}
0 &=& G \ \cdot\  ( R + \alpha G + \beta S )
\\
0 &=& S \ \cdot\  ( R + \alpha G + \beta S )
\end{eqnarray}
\par
%XXX NEW EQUATION BEGIN
\begin{equation}
\label{eqn:twobytwosln}
-\ \ 
\left[ 
\begin{array}{c}
  (G\cdot R) \\
  (S\cdot R) \end{array} \right]
\quad = \quad
\left[ 
\begin{array}{rr}
   (G \cdot G) & (S \cdot G)  \\
   (G \cdot S) & (S \cdot S)  \end{array} \right] 
\;
\left[ 
\begin{array}{c}
  \alpha \\ 
  \beta \end{array} \right] 
\end{equation}
Equation
(\ref{eqn:twobytwosln})
is a set of two equations for $\alpha$ and $\beta$.
Recall the inverse of a $2\times 2$ matrix, equation
(\ref{eqn:twobytwoinv})
and get:
\begin{equation}
\label{eqn:twobytwosln}
\left[ 
\begin{array}{c}
  \alpha \\ 
  \beta \end{array} \right] 
\ = \ 
        {-1 \over (G\cdot G)(S\cdot S)-(G\cdot S)^2}
\left[ 
\begin{array}{rr}
  (S \cdot S) & -(S \cdot G)  \\
  -(G \cdot S) & (G \cdot G)  \end{array} \right] 
\; \left[ 
\begin{array}{c}
  (G\cdot R) \\
  (S\cdot R) \end{array} \right]
\end{equation}
\par
The many applications in this book all need to
find $\alpha$ and $\beta$ with (\ref{eqn:twobytwosln}), and then
update the solution with (\ref{eqn:xupdate}) and
update the residual with (\ref{eqn:Rupdate}).
Thus, we package these activities in a subroutine
named \texttt{cgstep()}.
To use that subroutine, we have a computation \bx{template}
%like we had for steepest descents, except that we
with
repetitive work done by subroutine {\tt cgstep()}.
This template (or pseudocode) for minimizing the residual
$\bold 0\approx \bold r = \bold F \bold x - \bold d$
by the conjugate-direction method is:
\label{lsq/'cgtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$                \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow \bold F\T\         \bold r$      \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \  \Delta \bold x$      \\
\>      \>  $(\bold x,\bold r) \padarrow {\rm cgstep}
             (\bold x,\bold r, \Delta\bold x,\Delta\bold r )$
        \\
\>      \> \}                                           
\end{tabbing}
where
the subroutine {\tt cgstep()}
remembers the previous iteration and
works out the step size and adds in
the proper proportion of the $\Delta \bold x$ of
the previous step.

%\par
%Most of the least-squares solver subroutines in this book
%follow the above \bx{template}.
%They may look less complicated when they start from $\bold x=\bold 0$
%or more complicated when $\bold F$ has several parts.
%$\bold F$ is often a partitioned matrix of operators
%and the code for applying it
%will have subtle differences from the code
%for applying its adjoint $\bold F\T$.
%Often we fit two expressions simultaneously,
%$\bold 0\approx \bold L\bold x-\bold d$ and
%$\bold 0\approx \bold A\bold x$, and then
%$\bold F$
%is the column matrix
%\begin{equation}
%\bold F \eq
%        \left[
%        \begin{array}{c}
%        \bold L \\
%        \bold A
%        \end{array}
%        \right]
%\end{equation}

%\begin{notforlecture}
\subsection{Routine for one step of conjugate-direction descent}
\par
\begin{comment}
The \bx{conjugate-direction program}
can be divided into two parts:
an inner part that is used almost without change
over a wide variety of applications,
and an outer part containing memory allocations,
operator invocations, and initializations.

Because \bx{Fortran} does not recognize the difference between upper- and
lower-case letters,
\end{comment}
The conjugate vectors $G$ and $S$ in the program are denoted by
{\tt gg} and {\tt ss}.
The inner part of the conjugate-direction task is in
function {\tt cgstep()}.%
%\moddex[f90]{cgstep}{one step of CD}
\moddex{cgstep}{one step of CD}{51}{86}{api/c}
\par
Observe the \texttt{cgstep()} function has a logical parameter
called \texttt{forget}.
This parameter does not need to be input.
In the normal course of things, \texttt{forget} is true
on the first iteration and false on subsequent iterations.
On the first iteration
there is no previous step,
so the conjugate direction method
is reduced to the steepest descent method.
At any iteration, however, you have the option to set
\texttt{forget=true},
which amounts to restarting the calculation
from the current location,
something we rarely find reason to do.


\subsection{A basic solver program}
There are many different methods for iterative least-square estimation
some of which are discussed later in this book.
The conjugate-gradient (CG) family
(including the first order conjugate-direction method previously described)
share the property that theoretically they achieve the solution
in $n$ iterations, where $n$ is the number of unknowns.
The various CG methods differ
in their numerical errors,
memory required,
adaptability to nonlinear optimization,
and their requirements on accuracy of the adjoint.
What we do in this section is to show you the generic interface.
\par
None of us is an expert in both geophysics and in optimization theory (OT),
yet we need to handle both.
We would like to have each group write its own code with
a relatively easy interface.
The problem is that the OT codes must invoke the physical operators
yet the OT codes should not
need to deal with all the data and parameters needed by the physical operators.
\par
In other words,
if a practitioner decides to
swap one solver for another,
the only thing needed is the name of the new solver.
\par
The operator entrance is for the geophysicist,
who formulates the estimation application.
The solver entrance is for the specialist in numerical algebra,
who designs a new optimization method.
The C programming language allows us
to achieve this design goal by means of generic function interfaces.
\par
A generic solver subroutine is \texttt{tinysolver}. It is simplified substantially from the library version,
which has a much longer list of optional arguments. (The \texttt{forget} parameter
is not needed by the solvers we discuss first.)

\moddex{tinysolver}{tiny solver}{23}{58}{api/c}

\par
The two most important arguments in \texttt{tinysolver()}
are the operator function \texttt{Fop},
which is defined by the interface from Chapter \ref{paper:ajt},
and the solver function \texttt{stepper},
which implements one step of an iterative estimation.
For example, a practitioner who choses to use our new
\texttt{cgstep()} \vpageref{lst:cgstep}
for iterative solving the operator
\texttt{matmult} \vpageref{lst:matmult}
would write the call
\par
\texttt{tinysolver ( matmult\_lop, cgstep, ...}
\par
The other required parameters to \texttt{tinysolver()} 
are \texttt{dat} (the data we want to fit),
\texttt{x} (the model we want to estimate),
and \texttt{niter} (the maximum number of iterations).
There are also a couple of optional arguments.
For example, \texttt{x0} is the starting guess for the model.
If this parameter is omitted, the model is initialized to zero.
To output the final residual vector,
we include a parameter called \texttt{res},
which is optional as well.
We will watch how the list of optional parameters
to the generic solver routine grows
as we attack more and more complex applications in later chapters.

\subsection{Fitting success and solver success}
Every time we run a data modeling program,
we have access to two publishable numbers
$1-|\bold r|/|\bold d|$ and
$1-|\bold F\T\,\bold r|/|\bold F\T\,\bold d|$.
The first says how well the model fits the data.
The second says how well we did the job of finding out.
\par
Define the residual $\bold r=\bold F\bold m-\bold d$ and
the ``size'' of any vector, such as the data vector,
as $|\bold d|=\sqrt{\bold d \cdot \bold d}$.
The number $1-|\bold r|/|\bold d|$ is called the
``success at fitting data.''
(Any data-space weighting function
should have been incorporated in both $\bold F$ and $\bold d$.)
\par
While the data fitting success is of interest to everyone,
the second number
$1-|\bold F\T\,\bold r|/|\bold F\T\,\bold d|$
is of interest in QA (quality analysis).
In giant problems,
especially those arising in seismology,
running iterations to completion is impractical.
A question always of interest is whether or not enough iterations have been run.
This number gives us guidance to where more effort could be worthwhile.
\par
\boxit{
$ 0 \quad \le \quad {\rm Success} \quad \le \quad 1$
\par
Fitting success: \quad $1-|\bold r|/|\bold d|$
\par
Numerical success: \quad $1-|\bold F\T\,\bold r|/|\bold F\T\,\bold d|$
}



\subsection{Roundoff}

Surprisingly, as a matter of practice, the simple conjugate-direction method
defined in this book is more reliable than the conjugate-gradient method
defined in the formal professional literature.
I know this sounds unlikely, but I can tell you why.
\par
In large applications, 
numerical roundoff can be a problem. 
Calculations need to be done in higher precision.
The conjugate gradient method depends on you to supply an operator
with the adjoint correctly computed.
Any roundoff in computing the operator should somehow be matched
by the roundoff in the adjoint.   But that is unrealistic.
Thus, optimization may diverge while theoretically converging.
The conjugate direction method does not mind the roundoff;
it simply takes longer to converge.
\par
Let us see an example of a situation in which roundoff becomes a problem.
Suppose we add 100 million 1.0s.  You expect the sum to be 100 million.
I got a sum of 16.7 million.
Why?
After the sum gets to 16.7 million, adding a one to it adds nothing.
The extra 1.0 disappears in single precision roundoff.
\par
\begin{verbatim}
        real function one(sum);  one=1.;  return; end
        integer i;   real sum
        do i=1, 100000000
                sum = sum + one(sum)
        write (0,*) sum;    stop;  end
	1.6777216E+07
\end{verbatim}
\par
The previous code must be a little more complicated than I had hoped
because modern compilers are so clever.
When told to add all the values in a vector the compiler wisely
adds the numbers in groups, and then adds the groups.
Thus, I had to hide the fact I was adding ones by getting those ones
from a subroutine that seems to depend upon the sum (but really
does not).

\subsection{Test case: solving some simultaneous equations}
\par
Now we assemble a module \texttt{cgtest} for solving simultaneous equations.
Starting with the conjugate-direction module {\tt cgstep} 
\vpageref{lst:cgstep}
we insert the module \texttt{matmult} \vpageref{lst:matmult} as the linear operator.
\moddex{cgtest}{demonstrate CD}{23}{31}{user/pwd}

\par
The following shows the solution to a $5 \times 4$ set of simultaneous equations.
Observe that the ``exact'' solution is obtained in the last step.
Because the data and answers are integers,
it is quick to check the result manually.
%\newpage
\par
\noindent
\footnotesize
\begin{verbatim}
d transpose
      3.00      3.00      5.00      7.00      9.00

F transpose
      1.00      1.00      1.00      1.00      1.00
      1.00      2.00      3.00      4.00      5.00
      1.00      0.00      1.00      0.00      1.00
      0.00      0.00      0.00      1.00      1.00

for iter = 0, 4
x    0.43457383  1.56124675  0.27362058  0.25752524
res -0.73055887  0.55706739  0.39193487 -0.06291389 -0.22804642
x    0.51313990  1.38677299  0.87905121  0.56870615
res -0.22103602  0.28668585  0.55251014 -0.37106210 -0.10523783
x    0.39144871  1.24044561  1.08974111  1.46199656
res -0.27836466 -0.12766013  0.20252672 -0.18477242  0.14541438
x    1.00001287  1.00004792  1.00000811  2.00000739
res  0.00006878  0.00010860  0.00016473  0.00021179  0.00026788
x    1.00000024  0.99999994  0.99999994  2.00000024
res -0.00000001 -0.00000001  0.00000001  0.00000002 -0.00000001
\end{verbatim}
\normalsize


\section{INVERSE NMO STACK}
\inputdir{invstack}

\sx{NMO stack}
To illustrate an example of solving a huge set of simultaneous
equations without ever writing down the matrix of coefficients,
we consider how
{\it \bx{back projection}} can be upgraded toward
{\it \bx{inversion}} in the application called \bx{moveout and stack}.
\sideplot{invstack}{width=.8\textwidth,height=.8\textwidth}{
  Top is a model trace $\bold m$.
  Next, are the synthetic data traces, $\bold d = \bold F \bold m$.
  Then, labeled {\tt niter=0} is the \protect\bx{stack},
  a result of processing by adjoint modeling.
  Increasing values of {\tt niter} show $\bold m$
  as a function of iteration count in the fitting goal
  $\bold d \approx \bold F \bold m$.
  (Carlos Cunha-Filho)
}
\par
The seismograms at the bottom of Figure~\ref{fig:invstack}
show the first four iterations of conjugate-direction inversion.
You see the original rectangle-shaped waveform returning
as the iterations proceed.
Notice also on the \bx{stack}
that the early and late events have unequal amplitudes,
but after enough iterations match the original model.
Mathematically,
we can denote the top trace as the model $\bold m$,
the synthetic data signals as $\bold d = \bold F \bold m$,
and the stack as $\bold F\T\,\bold d$.
The conjugate-gradient algorithm optimizes the fitting goal
$\bold d \approx \bold F \bold x$ by variation of $\bold x$,
and the figure shows $\bold x$ converging to $\bold m$.
Because there are 256 unknowns in $\bold m$,
it is gratifying to see good convergence occurring
after the first four iterations.
The fitting is done by module {\tt invstack},
which is just like
\texttt{cgtest} \vpageref{lst:cgtest} except that the matrix-multiplication operator
\texttt{matmult} \vpageref{lst:matmult} has been replaced by
\texttt{imospray}. % \vpageref{lst:imospray}.
Studying the program,
you can deduce that,
except for a scale factor,
the output at {\tt niter=0} is identical to the stack $\bold M\T \bold d$.
All the signals in Figure~\ref{fig:invstack} are intrinsically the same scale.%
\moddex{invstack}{inversion stacking}{24}{34}{user/gee}
\par
This simple inversion is inexpensive.
Has anything been gained over conventional stack?
First,
though we used nearest-neighbor interpolation,
we managed to preserve the spectrum of the input,
apparently all the way to the Nyquist frequency.
Second, we preserved the true amplitude scale
without ever bothering to think about
(1) dividing by the number of contributing traces,
(2) the amplitude effect of NMO stretch, or
(3) event truncation.
\par
With depth-dependent velocity,
wave fields become much more complex at wide offset.
NMO soon fails,
but wave-equation forward modeling
offers interesting opportunities for inversion.

\section{FLATTENING 3-D SEISMIC DATA}
\inputdir{lomask}

Here is an expression that on first sight seems to say nothing:
\begin{equation}
\nabla \tau \eq 
\left[
\begin{array}{c}
\frac{\partial \tau}{\partial x} \\
\\
\frac{\partial \tau}{\partial y}
\end{array}
\right] \label{eqn:TauFields}
\end{equation}
Equation~(\ref{eqn:TauFields}) looks like a tautology,
a restatement of basic mathematical notation. 
But it is a tautology
only if $\tau (x , y )$ is known and the derivatives come from it.
When $\tau (x , y )$ is not known but the partial derivatives are observed,
then, we have two measurements at each $(x , y )$ location
for the one unknown $\tau$ at that location. 
In Figure~\ref{fig:TwoD}, we have seen how to flatten 2-D seismic data.
The 3-D process $(x,y,\tau)$ is much more interesting because
of the possibility encountering a vector field that
cannot be derived from a scalar field.
\par
The easy case is when you can move around the $(x,y)$ plane
adding up $\tau$ by steps of $d\tau/dx$ and $d\tau/dy$ and find
upon returning to your starting location that the total time change $\tau$ is zero.
When  $d\tau/dx$ and $d\tau/dy$ are derived from noisy data,
such sums around a path often are not zero.
Old time seismologists would say, ``The survey lines don't tie.''
\sx{seismology}
Mathematically,
it is like an electric field vector that may be derived
from a potential field
unless the loop encloses a changing \bx{magnetic} field.
\par
%As we push to the limits of our knowledge (which we normally do)
%this problem always arises.
We would like a solution for $\tau$
that gives the best fit of all the data
(the stepouts  $d\tau/dx$ and $d\tau/dy$)
in a volume.
Given a volume of data $d(t , x , y )$, we seek
the best $\tau (x , y )$ such that
$w(t , x , y )= d (t-\tau (x , y ), x , y )$
is  flattened. Let us get it.
\par
We write a regression, a residual ${\bf r}$ that
we minimize to find a best fitting
$\tau (x , y )$ or maybe $\tau (x , y , t )$.
Let $d$ be the measurements in the vector in equation~(\ref{eqn:TauFields}),
the measurements throughout the $(t , x , y )$-volume.
Expressed as a regression, equation~(\ref{eqn:TauFields}) becomes:
\begin{equation}
{\bf 0} \quad \approx\quad  {\bf r} \eq \nabla \tau \ -\  {\bf d}
\end{equation}
Figure~\ref{fig:chev} shows slices through a cube of seismic data.
A paper book is inadequate to display all the images required
to compare before and after
(one image of output is blended over multiple images of input),
therefore, we move on to a radar application of much the same idea,
but in 2-D instead of 3-D.

\plot{chev}{width=\textwidth}{
        [Jesse Lomask]
	Chevron data cube from the Gulf of Mexico.
	Shown are three planes within the cube.
	A salt dome (lower right corner in the top plane) has pushed upward,
	dragging bedding planes (seen in the 
	bottom two orthogonal planes) along with it.}
Let us see how the coming 3-D illustrations were created.
First we need code for vector gradient with its adjoint,
negative vector divergence. Here it is: 
\moddex{igrad2}{igrad} {48}{60}{api/c}
In a kind of magic, all we need to fit our regression~(\ref{eqn:TauFields})
is to pass the {\tt igrad2} 
module to the Krylov subspace solver,
simple solver using {\tt cgstep},
but first we need 
to compute {\bf d} by calculating $dt/dx$ and $dt/dy$ between
all the mesh points. 
\begin{verbatim}
do iy=1,ny { # Calculate x-direction dips: px 
             call puck2d(dat(:,:,iy),coh_x,px,res_x,boxsz,nt,nx) 
           } 
do ix=1,nx { # Calculate y-direction dips: py 
             call puck2d(dat(:,ix,:),coh_y,py,res_y,boxsz,nt,ny) 
           } 
do it=1,nt { # Integrate dips: tau 
             call dipinteg(px(it,:,:),py(it,:,:),tau,niter,verb,nx,ny) 
           } 
\end{verbatim}
The code says first to initialize the gradient operator.
Convert the 2-D plane of $dt/dx$ to a vector.
Likewise $dt/dy$.
Concatenate these two vectors
into a single column vector {\bf d} like in
equation~(\ref{eqn:TauFields}).
Tell the simple solver to make its steps
with to use {\tt cgstep} with the linear operator {\tt igrad2}. 
%\moddex{dipinteg}{dipinteg}

\subsection{Gulf of Mexico Salt Piercement Example (Jesse Lomask) }
Figure~\ref{fig:chev} shows a 3-D seismic data cube from the Gulf of Mexico provided by 
Chevron. A volume of data cannot be displayed on the page of a book. The display 
here consists of three slices from the volume. Top is a $(t_0 , x , y )$ slice, also called a 
``time slice.'' Beneath it is a $(t , x , y_0 )$ slice; aside that is a $(t , x_0 , y )$ slice, depth slices 
in orthogonal directions. Intersections of the slices within the cube are shown by 
the heavy black lines on the faces of the cube. The circle in the lower right corner 
of the top slice is an eruption of salt (which, like ice, under high pressure will flow 
like a liquid). Inside the salt there are no reflections so the data should be ignored 
there. Outside the salt we see layers, simple horizons of sedimentary rock. As the 
salt has pushed upward it has dragged bedding planes upward with it.
Presuming the bedding to contain permeable sandstones and impermeable shales,
the pushed up bedding around the salt is a prospective oil trap.
The time slice in the top panel shows ancient river channels,
some large, some small, that are now deeply buried. 
These may also contain sand.
Being natural underground ``oil pipes'' they are of great interest.
To see these pipes as they approach the salt dome
we need a picture not at a constant $t$,
but at a constant $t -\tau (x , y )$. 

\par
Figure~\ref{fig:slicecomp22} shows a time slice of the original cube
and the flattened cube of Figure~\ref{fig:chev}.
The first thing to notice on the plane before flattening is that the panel 
drifts from dark to light in place to place.
This is because the horizontal layers are not fully horizontal.
Approaching the dome the change from dark to light and 
back again happens so rapidly that the dome appears surrounded by rings.
After flattening, the drift and rings disappear.
The reflection horizons are no longer cutting across the image.
Channels no longer drift off (above or below) the viewable time slice.
Carefully viewing the salt dome it seems smaller after flattening because the 
rings are replace by a bedding plane.
%

%
\sideplot{slicecomp22}{width=0.8\textwidth}{
	Slices of constant time before and after flattening.
	Notice the rings surrounding the dome are gone giving
	the dome a reduced diameter. 
	(Ignore the inside of the dome.)}
%

\section{VESUVIUS PHASE UNWRAPPING}
\inputdir{vesuvio}
\sx{Vesuvius}
\sx{phase unwrapping}
\sx{phase}
Figure \ref{fig:vesuvio} shows
radar images of
Mt.~Vesuvius\footnote{
        A web search engine quickly finds you other views.
        }
in Italy.
These images are made from backscatter
signals $s_1(t)$ and $s_2(t)$,
recorded along two \bx{satellite orbit}s 800-km high and 54-m apart.
The signals are very high frequency
(the radar wavelength being 2.7 cm).
The signals were Fourier transformed
and one multiplied by the complex conjugate of the other,
getting the product $Z=S_1(\omega) \bar S_2(\omega)$.
The product's amplitude and phase are shown in Figure \ref{fig:vesuvio}.
Examining the data,
you can notice that where the signals are strongest (darkest on the left),
the phase (on the right)
is the most spatially consistent.
Pixel by pixel evaluation with the two frames in a movie program
shows that there are a few somewhat large local amplitudes
(clipped in Figure \ref{fig:vesuvio})
but because these generally have spatially consistent phase,
I would not describe the data as containing noise bursts.
\plot{vesuvio}{width=\textwidth,height=.5\textwidth}{
  Radar image of Mt. Vesuvius.
  Left is the amplitude $|Z(x,y)|$.
  Nonreflecting ocean in upper-left corner.
  Right is the phase $\arctan( \Re Z(x,y), \Im Z(x,y)\;)$.
  (European Space Agency via Umberto Spagnolini)
}

\par
To reduce the time needed for analysis and printing,
I reduced the data size two different ways,
by decimation and local averaging,
as shown in Figure \ref{fig:squeeze}.
The decimation was to about 1 part in 9 on each axis,
and the local averaging was done in $9\times 9$ windows
giving the same spatial resolution in each case.
The local averaging was done independently in the
plane of the real part and the plane of the imaginary part.
On the smoothed data the phase is less noisy.
\plot{squeeze}{width=\textwidth,height=.5\textwidth}{
  Phase based on decimated data (left)
  and smoothed data (right).
}

\par
From Figures \ref{fig:vesuvio} and \ref{fig:squeeze},
we see that contours of constant phase
appear to be contours of constant altitude;
this conclusion leads us to suppose that a study of radar theory
would lead us to a relation like $Z(x,y)=e^{ih(x,y)}$,
where $h(x,y)$ is altitude.
We nonradar specialists often think of phase in
$e^{i\phi} = e^{i\omega t_0(x,y)}$
as being caused by some time delay and
being defined for some constant frequency $\omega$.
Knowledge of this $\omega$ (as well as some angle parameters)
would define the physical units of $h(x,y)$.

Because the flat land away from the mountain is all at the same phase
(as is the altitude),
the distance as revealed by the phase does not represent
the distance from the ground to the satellite viewer.
We are accustomed to measuring altitude along a vertical line to a datum;
but here, the distance seems to be measured
from the ground along a $23^\circ$ angle from the vertical
to a datum at the satellite height.

\par
Phase is a troublesome measurement, because
we generally see it modulo $2\pi$.
Marching up the mountain, we see the phase getting lighter and lighter
until it suddenly jumps to black, which then continues to lighten
as we continue up the mountain to the next jump.
Let us undertake to compute the phase, including
all its jumps of $2\pi$.
Begin with a complex number $Z$ representing
the complex-valued image at any location
in the $(x,y)$-plane.
\begin{eqnarray}
r e^{i \phi}   &=& Z \\
\ln |r| + i \phi &=& \ln Z \\
\phi(x,y)            &=&  \Im \ln Z(x,y) ~+~  2\pi N(x,y)
\end{eqnarray}
Computers find the imaginary part of the logarithm
with the arctan function of two arguments, {\tt atan2(y,x)},
which puts the phase in the range $-\pi < \phi \le \pi$,
although any multiple of $2\pi$ could be added.
We seem to escape the $2\pi N$ phase ambiguity by differentiating:
\begin{eqnarray}
{\partial\phi \over \partial x} \eq \Im {1 \over Z}{\partial Z \over \partial x} \eq
                                   {\Im  \bar Z {\partial Z \over \partial x} \over \bar Z Z }
\label{eqn:integrate1D}
\end{eqnarray}
For every point on the $y$-axis, equation (\ref{eqn:integrate1D})
is a differential equation on the $x$-axis.
We could integrate them all to find $\phi(x,y)$.
That sounds easy.
On the other hand,
the same equations are valid when $x$ and $y$ are interchanged,
therefore we get twice as many equations as unknowns.
Ideally either of these sets of equations
is equivalent to the other;
but for real data, we expect to be fitting this fitting goal:
\begin{equation}
\nabla \phi \quad \approx \quad {\Im  \bar Z \nabla Z \over \bar Z Z}
\label{eqn:integrateme}
\end{equation}
where
$\nabla = ({\partial \over \partial x}, {\partial \over \partial y} ) $.
Mathematically, computing phase this way
is like our previous seismic flattening with
$\nabla \tau \approx {\bf d}$.
Taking measurements to be phase differences 
between neighboring mesh points,
it is more correct to interpret Equation~(\ref{eqn:integrateme}) as 
a difference equation than a differential equation.
Because we measure phase differences only over tiny distances (one pixel),
we hope not to worry about phases greater than $2\pi$.
But, if such jumps do occur, the jumps contribute to overall error.
\par
Let us consider a typical location in the $(x , y )$ plane where the complex numbers
$Z_{i,j}$ are given. Define a shorthand $a , b, c$, and $d$ as follows: 

\par
%We will be handling the differential equation as a difference equation
%using an exact representation on the data mesh.
%By working with the phase difference of neighboring data values,
%we do not have to worry about phases greater than $2\pi$
%(except where phase jumps that much between mesh points).
%Thus we solve (\ref{eqn:integrateme})
%with finite differences instead of differentials.
%Module \texttt{igrad2} is a linear operator for the difference
%representation of the operator representing
%the gradient of a potential field.
%Its adjoint is known as the divergence of a vector field.
%\opdex{igrad2}{gradient in 2-D}
%To do the least-squares fitting
%(\ref{eqn:integrateme})
%we pass the \texttt{igrad2} module to the Krylov subspace solver.
%(Other people might prepare a matrix and give it to Matlab.)
%\par
%The difference equation representation of the fitting goal
%(\ref{eqn:integrateme}) is:
%\begin{equation}
%        \begin{array}{rcl}
%                \phi_{i+1,j} -\phi_{i,j} &\approx& \Delta\phi_{ac} \\
%                \phi_{i,j+1} -\phi_{i,j} &\approx& \Delta\phi_{ab}
%        \end{array}
%\label{eqn:diffgrad}
%\end{equation}
%where we still need to define the right-hand side.
%Define the parameters
%$a$,
%$b$,
%$c$, and
%$d$ as follows:
\begin{equation}
        \left[
                \begin{array}{ll}
                a & b \\
                c & d
                \end{array}
        \right]
        \eq
        \left[
                \begin{array}{ll}
                Z_{i,j}   & Z_{i,j+1} \\
                Z_{i+1,j} & Z_{i+1,j+1}
                \end{array}
        \right]
\end{equation}
With this shorthand, the difference equation representation of the fitting goal (\ref{eqn:integrateme}) is: 
\begin{equation}
        \begin{array}{rcl}
                \phi_{i+1,j} -\phi_{i,j} &\approx& \Delta\phi_{ac} \\
                \phi_{i,j+1} -\phi_{i,j} &\approx& \Delta\phi_{ab}
        \end{array}
\label{eqn:diffgrad}
\end{equation}
Now, let us find the phase jumps between the various locations. Complex numbers $a$ and $b$ may be expressed in polar form, say $a=r_ae^{i\phi_a}$ and $b=r_be^{i\phi_b}$.
The complex number 
$\bar a b = r_a r_b e^{i(\phi_b-\phi_a)}$ has the desired phase
$\Delta \phi_{ab}$.
To obtain it we take the imaginary part of the complex logarithm
$\ln |r_a r_b| + i\Delta \phi_{ab}$:
\begin{equation}
  \begin{array}{lllll}
        \phi_b-\phi_a &=& \Delta \phi_{ab} &=& \Im \ln  \bar a b\\
        \phi_d-\phi_c &=& \Delta \phi_{cd} &=& \Im \ln  \bar c d\\
        \phi_c-\phi_a &=& \Delta \phi_{ac} &=& \Im \ln  \bar a c\\
        \phi_d-\phi_b &=& \Delta \phi_{bd} &=& \Im \ln  \bar b d
  \end{array}
\label{eqn:thedeltas}
\end{equation}
which gives the information needed to fill in the right side of
(\ref{eqn:diffgrad}), as done by subroutine \texttt{igrad2init()} from
module \texttt{unwrap} \vpageref{lst:unwrap}.
% \progdex{igrad2init}{gradient 2-D init.}

The operator needed is \texttt{igrad2},
gradient with its adjoint, the divergence. \sx{divergence, vector}
%\opdex{igrad2}{gradient 2-D.}

\subsection{Estimating the inverse gradient}
To optimize the fitting goal (\ref{eqn:diffgrad}),
module \texttt{unwrap()} uses the conjugate-direction method
like the modules \texttt{cgmeth()}
%\vpageref{/prog:cgmeth}
and 
\texttt{invstack()}:
%\vpageref{/prog:invstack}.
\moddex{unwrap}{Inverse 2-D gradient}{65}{67}{user/gee}
An open question is whether or not the required number of iterations is reasonable
or if we need to uncover a preconditioner
or more rapid solution method.
I adjusted the frame size 
(by the amount of smoothing in Figure \ref{fig:squeeze})
so that I would get the solution in about ten seconds with 400 iterations.
Results are shown in Figure~\ref{fig:veshigh}.
\plot{veshigh}{width=\textwidth,height=.5\textwidth}{
        Estimated altitude.}

\begin{comment}
When we are working with figures like Figure~\ref{fig:veshigh},
the number of iterations often exceeds the number of intermediate
output frames that we care to deal with.
The computer function \texttt{klick()} is a simple tool
to detect logarithmically spaced intervals for taking snapshots
of iterative descent.
\progdex{klick}{Logarithmic increment detect}
\end{comment}

Revise \texttt{igrad2} to make a module called {\tt tgrad2}
which has transient boundaries.

\plot{synmod}{width=\textwidth,height=.25\textwidth}{ 
  Synthetic mountain with hidden backside.
  For your estimation enjoyment.
}

\subsection{Analytical solutions}
%
We have found a numerical solution to fitting applications, such as:
\begin{equation}
{\bf 0} \quad \approx \quad \nabla \tau \ - \ {\bf d}
\label{eqn:analyticsolution}
\end{equation}
An analytical solution is much faster.
From any regression, we get the least 
squares solution when we multiply by the transpose of the operator. Thus,
\begin{equation}
{\bf 0} \eq \nabla\T\;\nabla \tau \ -\ \nabla\T\,{\bf d}
\end{equation} 
We need to understand what is the transpose of the gradient operator.
Recall the finite difference representation of a derivative in Chapter~1.
Ignoring end effects,
the transpose of a derivative is the negative of a derivative.
Because the transpose of a column vector is a row vector,
the adjoint of a gradient $\nabla$, namely,
$\nabla\T$ is more commonly known as the vector divergence 
($\nabla \cdot$).
Likewise, $\nabla\T\,\nabla$ is a positive definite matrix,
the negative of the Laplacian $\nabla^2$.
Thus, in more conventional mathematical notation,
the solution $\tau$ is that of Poisson's equation. 
\begin{equation}
\nabla^2 \tau \eq - \ \nabla \cdot {\bf d} 
\label{eqn:PoissonEQ}
\end{equation} 
In the Fourier domain, we can have an analytic solution.
There, $-\nabla^2 = k_x^2 + k_y^2$ 
where $(k_x , k_y)$ are the Fourier frequencies on the $(x , y )$ axes.
Instead of thinking 
of equation~(\ref{eqn:PoissonEQ}) as a convolution in physical space,
think of it as a product in Fourier space.
Thus, the analytic solution is:
\begin{equation}
\tau(x,y) \eq
{\bf FT}^{-1}\ \frac{ {\bf FT} \ \ \nabla \cdot {\bf d} } { k_x^2+k_y^2}
\end{equation}
where {\bf FT} denotes two-dimensional Fourier transform over $x$ and $y$ . 
Here is a trick from numerical analysis that gives better results: Instead of 
representing the denominator $k_x^2+k_y^2$ in the most obvious way, let us represent it 
in a manner consistent with the finite-difference way we expressed the numerator 
$\nabla \cdot {\bf d}$. Recall that $- i\omega \Delta t \approx i
\hat{\omega} \Delta t = 1-Z = 1-{\rm exp}(-i \omega \Delta t)$, 
which is a Fourier 
domain way of saying that difference equations tend to differential equations at low 
frequencies. Likewise, a symmetric second time derivative has a finite-difference 
representation proportional to $(-2+Z+1/Z)$ and in a two-dimensional space, a 
finite-difference representation of the Laplacian operator is proportional to $(-4+X+1/X+Y+1/Y)$, where $X = {\rm exp}(i k_x \Delta x )$ and $Y = {\rm exp}(i k_y \Delta y )$. 
Fourier solutions have peculiarities (periodic boundary conditions) 
that are not always appropriate in practice, but having these solutions available 
is often a nice place to start from when solving an application that cannot be solved 
in Fourier space.

\par
For example, suppose we feel some data values are bad, and we
would like to throw out the regression equations involving the bad data points. At 
Vesuvius, we might consider the strength of the radar return (which we have
previously ignored) and use it as a weighting function ${\bf W}$.
Now, our regression~(\ref{eqn:analyticsolution}) becomes:
\begin{equation}
{\bf 0} \quad \approx\quad
{\bf W} \ (\nabla \phi -{\bf d})
\eq ({\bf W} \nabla) \phi \ -\  {\bf Wd}
\end{equation}
which is a regression with an operator ${\bf W \nabla}$  and 
data ${\bf Wd}$. The weighted problem is not solvable in the Fourier domain, because 
the operator $({\bf W} \nabla )\T\; {\bf W} \nabla$ has no simple
expression in the Fourier domain. 
Thus, 
we would use the analytic solution to the unweighted problem as a starting guess for 
the iterative solution to the real problem. 
\par
With the Vesuvius data, we could construct a weight ${\bf W}$ from the signal strength.
We also have available the curl, which should vanish.
Vanishing is an indicator of questionable data
that should be weighted down relative to other data.


%\begin{notforlecture}
\section{OPERATOR SCALING (BINORMALIZATION)}
%
We can accept model $\bold m$ and data $\bold d$ as they arise from the geometry
or geophysics of an application,
or we can transform them to forms that are computationally convenient,
work with them, and finally transform back.
Say we transform them to new variables, $\bold u$ and $\bold v$.
\begin{eqnarray}
\bold u &=& \bold M \bold m
\\
\bold v &=& \bold D \bold d
\end{eqnarray}
These transformations change the $\bold F$ operator to $ \bold D \bold F \bold M^{-1} $ because
\begin{eqnarray}
\bold d &=& \bold F \bold m
\\
\bold D \bold d &=& \bold D \bold F \bold M^{-1} \bold M \bold m
\\
\bold v &=& \bold D \bold F \bold M^{-1} \bold u
\end{eqnarray}
The game is looking for the best
$\bold M$ and
$\bold D$.
\par
If I were able and willing to handle linear algebra in a modern way,
I would show you this matrix iteration
\begin{equation}
\lambda\ 
\left[
	\begin{array}{c}
	\bold d   \\
	\bold m 
	\end{array}
\right]_{i+1}
\eq
\left[
	\begin{array}{cc}
	\bold 0 & \bold F  \\
	\bold F\T & \bold 0
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold d   \\
	\bold m 
	\end{array}
\right]_i
\end{equation}
What is $\lambda$?  
It is a scale factor that you use to keep the vectors normalized
as the iteration proceeds.
You may recognize a path leading to eigenvectors, eigenvalues, Hermitian matrices,
and singular-value decomposition.
You'll need to find other sources to go further on that path because
it has not led me to a solution to the problem at hand
which is how to choose the best $\bold M$ and $\bold D$.
\par
If you have an operator that you are using millions of times 
it is worth seeking good choices.
Good choices are those
that make the adjoint of your new operator
$ \bold D \bold F \bold M^{-1}$
a good approximation to its inverse.
These are the two conditions we seek:
\begin{eqnarray}
\bold I &\approx &
(\bold D \bold F \bold M^{-1})\T \ 
(\bold D \bold F \bold M^{-1})
\\
\bold I &\approx &
(\bold D \bold F \bold M^{-1}) \ 
(\bold D \bold F \bold M^{-1})\T
\end{eqnarray}
If these were true, we could probe with any test vectors we wished
\begin{eqnarray}
\hat {\bold t}_m
&=&
(\bold M^{-1})\T \ (\rm something)\ \bold M^{-1}\ \bold t_m
\\
\hat {\bold t}_d
&=&
\bold D \ {(\rm something\ else)} \
\bold D\T \ \bold t_d
\end{eqnarray}
and find both $\hat {\bold t}_m\approx \bold t_m$
and  $\hat {\bold t}_d\approx \bold t_d$.
So, the game is to play with
$\bold M$ and $\bold D$
to try to get this to happen.
\par
About the only trick I know is to try $\bold M$ and $\bold D$ as diagonal matrices.
For test functions $\bold t$, I generally use a pattern of moderately spaced impulses.
In physical space we may see places where
$\hat {\bold t}$ is smaller than $\bold t$.
Those are the places to boost the corresponding diagonal.
\par
There are many test functions you could use.
You could use all ones.
You could use random numbers.
You could use a pile of random old data,
though I'm not sure what you would use for old models.
Take the output.
Take its absolute value.
Maybe smooth it.
Take the square root since the half you put in
$\bold F$ appears a second time in $\bold F\T$.
\par
I know one more trick.
In seismology many operators appear as integrals.
One of many such operators is called ``Kirchhoff migration''.
Because these operators and their adjoints contain integrations they boost low frequencies.
We can attenuate them back to their original size
by having $\bold M$ or $\bold D$ apply $\sqrt{-i\omega}$
(known in the time domain as the ``Hankel tail'').
%\par
%Sometimes Jacobians or determinents
%give guidance to finding the inverse and thereby deducing the diagonal matrices.
%That's a nice start, but practicalities often arise
%(such as data truncation, depth variable velocity)
%that keep the guessing game alive.
%
\par
What, may we ask is the interpretation of the $(\bold u, \bold v)$ variables?
They feel like ``energy conservation'' variables, though it makes no sense
to say the physical energy in $\bold m$ or $\bold d$ should be conserved
in the way of Parseval's theorem of Fourier transforms.
I imagined the $(\bold u,\bold v)$ variables might be especially suitable for display
(like preconditioned variables) but now I am less certain.

%\end{notforlecture}

\section{THE WORLD OF CONJUGATE GRADIENTS}

Nonlinearity arises in two ways:
First, modeled data might be a nonlinear function of the model parameters.
Second, observed data could contain imperfections that force us to use
\bx{nonlinear methods} of statistical estimation.

\subsection{Physical nonlinearity}
Methods of physics may
relate modeled data $\bold d_{\rm theor}$ to model parameters $\bold m$,
with a nonlinear relation,
say $\bold d_{\rm theor} =\bold f(\bold m)$.
The power-series approach then leads to
representing modeled data as:
\begin{equation}
\bold d_{\rm theor} \eq
  \bold f(\bold m_0 + \Delta \bold m)
  \quad\approx\quad
  \bold f\bold (\bold m_0) + \bold F\Delta \bold m
\end{equation}
where $\bold F$ is the matrix of partial derivatives
of data values by model parameters,
say $\partial d_i/\partial m_j$,
evaluated at $\bold m_0$.
The modeled data  $\bold d_{\rm theor}$ minus
the observed data $\bold d_{\rm obs}$ is the residual we minimize.
\begin{eqnarray}
\bold 0 \quad\approx\quad
 \bold d_{\rm theor} - \bold d_{\rm obs}
 &=& \bold F\bold \Delta\bold  m +[\bold f(\bold m_0) - \bold d_{\rm obs}] \\
\bold r_{\rm new}
 &=& \bold F\bold \Delta\bold  m + \bold r_{\rm old}
\label{eqn:resupdatenl}
\end{eqnarray}
It is worth noticing that the residual updating
(\ref{eqn:resupdatenl})
in a nonlinear application is the same
as that in a linear application (\ref{eqn:resupdatelin}).
If you make a large step $\Delta \bold m$, however,
the new residual
is different from that expected by
(\ref{eqn:resupdatenl}).
Thus, 
you should always re-evaluate the residual vector at the new location,
and if you are reasonably cautious,
you should be sure the residual norm has actually decreased
before you accept a large step.

\par
The pathway of inversion with physical nonlinearity
is well developed in the academic literature,
and Bill \bx{Symes} at Rice University has a particularly active group.

\par
There are occasions to change the weighting function during model fitting.
Then, 
one simply restarts the calculation from the current model.
In the code, 
you would flag a restart with the expression \texttt{forget=true}.

\subsection{Coding nonlinear fitting problems}
An adaptation of a linear method gives us a nonlinear solver
by simply recomputing the gradient at each iteration.
Omitting the weighting function (for simplicity) the \bx{template} is:
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> {\rm iterate \{ }                                                    \\
\>      \> $\bold r \padarrow \bold f( \bold m) - \bold d$       \\
\>      \> Define $\bold F=\partial \bold d/\partial \bold m$.       \\
\>      \>  $\Delta\bold m  \padarrow \bold F\T\         \bold r$ \\
\>      \>  $\Delta\bold r\ \padarrow \bold F \ \Delta \bold m$  \\
\>      \>  $(\bold m,\bold r) \padarrow {\rm step}
             (\bold m,\bold r, \Delta\bold m,\Delta\bold r )$ \\
\>      \> \}
\end{tabbing}

\par
A formal theory for the optimization exists,
but we are not using it here.
The assumption we make is that the step size is small,
so that familiar line-search and plane-search approximations
can succeed in reducing the residual.
Unfortunately,
this assumption is not reliable.
What we should do is test that the residual really does decrease,
and if it does not, 
we should revert
to smaller step size.
Perhaps, we should test an incremental variation on the status quo:
where inside \texttt{solver}, %\vpageref{lst:tinysolver},
we check to see if the residual
diminished in the {\it previous} step; and if it did not,
restart the iteration (choose the {\it current} step to be steepest descent instead of CD).
\par
Experience shows that nonlinear applications have many pitfalls.
Start with a linear problem,
add a minor physical improvement or abnormal noise,
and the problem becomes nonlinear and probably has another solution
far from anything reasonable.
When solving such a nonlinear problem,
we cannot arbitrarily begin from zero, as we do with linear problems.
We must choose a reasonable starting guess.
Chapter \ref{paper:iin} on the topic of regularization
offers an additional way to reduce the dangers of nonlinearity.

\subsection{Inverse of a $2\times 2$ matrix}

\begin{eqnarray}
\bold A ^{-1} \quad\quad\quad\quad \bold A \quad\quad &=& \ \quad\bold I
\\
\label{eqn:twobytwoinv}
\frac{1}{ad-bc}
\left[
\begin{array}{rr}
d & -b  \\
-c & a
\end{array}
\right]
\quad
\left[
\begin{array}{rr}
a & b  \\
c & d
\end{array}
\right]
&=&
\left[
\begin{array}{rr}
1 & 0  \\
0 & 1
\end{array}
\right]
\end{eqnarray}




\subsection{Standard methods}
The conjugate-direction method is really a family of methods.
Mathematically, where there are $n$ unknowns, these algorithms all
converge to the answer in $n$ (or fewer) steps.  The various methods
differ in numerical accuracy, treatment of underdetermined systems,
accuracy in treating ill-conditioned systems, space requirements, and
numbers of dot products.  Technically, the method of CD used in the
\texttt{cgstep} module 
%\vpageref{lst:cgstep} 
is not the
conjugate-gradient method itself, but is equivalent to it.  This
method is more properly called the \bx{conjugate-direction method}
with a memory of one step.  I chose this method for its clarity and
flexibility.  If you would like a free introduction and summary of
conjugate-gradient methods, I particularly recommend {\it An
  Introduction to Conjugate Gradient Method Without Agonizing Pain }
by Jonathon Shewchuk, which you can \htmladdnormallinkfoot{download}{%
http://www.cs.cmu.edu/afs/cs/project/quake/public/papers/painless-conjugate-gradient.ps%
}.

\par
I suggest you skip over the remainder of this section and return
after you have seen many examples and have developed some expertise,
and have some technical problems.
\par
The \bx{conjugate-gradient method} was introduced
by \bx{Hestenes} and \bx{Stiefel} in 1952.
To read the standard literature and relate it to this book,
you should first realize that when I write fitting goals like
\begin{eqnarray}
 0  &\approx&  \bold W( \bold F \bold m - \bold d ) \\
 0  &\approx&  \bold A \bold m,
\end{eqnarray}
they are equivalent to minimizing the quadratic form:
\begin{equation}
\bold  m:  \quad\quad
\min_{\bold m}  Q(\bold m) \eq
( \bold m\T\,\bold F\T - \bold d\T)\bold W\T\,\bold W
( \bold F \bold m  - \bold d)
\ +\ \bold m\T\,\bold A\T\,\bold A\bold m
\label{eqn:geoinvtheory}
\end{equation}
The optimization theory (OT) literature starts from a minimization of
\begin{equation}
 \bold x:  \quad\quad
 \min_{\bold x} Q(\bold x) \eq \bold x\T\,\bold H \bold x - \bold b\T\,\bold x
\label{eqn:optimtheory}
\end{equation}
To relate equation (\ref{eqn:geoinvtheory}) to (\ref{eqn:optimtheory})
we expand the parentheses in (\ref{eqn:geoinvtheory}) 
and abandon the constant term $\bold d\T\,\bold d$.
Then gather the quadratic term in $\bold m$ and the linear term in $\bold m$.
There are two terms linear in $\bold m$
that are transposes of each other.
They are scalars so they are equal.
Thus, to invoke ``standard methods,'' you take
your problem-formulation operators $\bold F$, $\bold W$, $\bold A$
and create two subroutines that apply:
\begin{eqnarray}
 \bold H   &=&  \bold F\T\,\bold W\T\,\bold W\bold F + \bold A\T\,\bold A  \\
 \bold b\T  &=&  2(\bold F\T\,\bold W\T\,\bold W\bold d)\T
\end{eqnarray}
The operators $\bold H$ and $\bold b\T$ operate on model space.
Standard procedures do not require their adjoints
because $\bold H$ is its own adjoint and $\bold b\T$
reduces model space to a scalar.
You can see that computing $\bold H$ and $\bold b\T$ requires
one temporary space the size of data space
(whereas \texttt{cgstep} requires two).
\par
When people have trouble with conjugate gradients or conjugate
directions, I always refer them to the \bx{Paige and Saunders
  algorithm} {\tt LSQR}.  Methods that form $\bold H$ explicitly or
implicitly (including both the standard literature and the book3
method) square the condition number, that is, they are twice as
susceptible to rounding error as is {\tt LSQR}. The Paige and
Saunders method is reviewed by Nolet in a geophysical context. 
%I include module \texttt{lsqr}
%\vpageref{/prog:lsqr}
%without explaining
%why it works. The interface is similar to \texttt{solver}
%\vpageref{/prog:smallsolver}.
%Note that the residual vector does not appear
%explicitly in the program and that we cannot start from a nonzero
%initial model.  \moddex{lsqr}{LSQR solver}

\begin{comment}
\subsection{Understanding CG magic and advanced methods}
This section includes Sergey Fomel's explanation on the ``magic''
convergence properties of the conjugate-direction methods. It also
presents a classic version of conjugate gradients, which can be found
in numerous books on least-square optimization.

The key idea for constructing an optimal iteration is to update the
solution at each step in the direction, composed by a linear
combination of the current direction and all previous solution steps.
To see why this is a helpful idea, let us consider first the method of
random directions. Substituting expression (\ref{eqn:alfa}) into
formula (\ref{eqn:mindot}), we see that the residual power
decreases at each step by
\begin{equation}
  \label{eqn:resdecr}
  \bold r \cdot \bold r -
  \bold r_{\rm new} \cdot \bold r_{\rm new} \eq
  \frac{(\bold r \cdot \Delta \bold r )^2}
  {( \Delta \bold r \cdot \Delta \bold r )}\;.
\end{equation}
To achieve a better convergence, we need to maximize the right hand
side of (\ref{eqn:resdecr}). Let us define a new solution step $\bold
s_{\rm new}$ as a combination of the current direction $\Delta \bold
x$ and the previous step $\bold s$, as follows:
\begin{equation}
  \label{eqn:snew}
  \bold s_{\rm new} \eq \Delta \bold x + \beta \bold s\;.
\end{equation}
The solution update is then defined as
\begin{equation}
\bold x_{\rm new} \eq \bold x+\alpha \bold s_{\rm new}\;.
\label{eqn:newx}
\end{equation}
The formula for $\alpha$ (\ref{eqn:alfa}) still holds, because we have
preserved in (\ref{eqn:newx}) the form of equation (\ref{eqn:oldx})
and just replaced $\Delta \bold x$ with $\bold s_{\rm new}$. In fact,
formula (\ref{eqn:alfa}) can be simplified a little bit. From
(\ref{eqn:newresperp}), we know that $\bold r_{\rm new}$ is orthogonal
to $\Delta \bold r = \bold F \bold s_{\rm new}$. Likewise, $\bold r$
should be orthogonal to $\bold F \bold s$ (recall that $\bold r$ was
$\bold r_{\rm new}$ and $\bold s$ was $\bold s_{\rm new}$ at the
previous iteration). We can conclude that
\begin{equation}
  \label{eqn:rdr}
  (\bold r \cdot \Delta \bold r ) \eq 
  (\bold r \cdot \bold F \bold s_{\rm new}) \eq
  (\bold r \cdot \bold F \Delta \bold x) + 
  \beta (\bold r \cdot \bold F \bold s) \eq
  (\bold r \cdot \bold F \Delta \bold x)\;.
\end{equation}
Comparing (\ref{eqn:rdr}) with (\ref{eqn:resdecr}), we can see that
adding a portion of the previous step to the current direction does
not change the value of the numerator in expression
(\ref{eqn:resdecr}). However, the value of the denominator can be
changed. Minimizing the denominator maximizes the residual increase at
each step and leads to a faster convergence. This is the denominator
minimization that constrains the value of the adjustable coefficient
$\beta$ in (\ref{eqn:snew}).
\par
The procedure for finding $\beta$ is completely analogous to the
derivation of formula (\ref{eqn:alfa}). We start with expanding the
dot product $(\Delta \bold r \cdot \Delta \bold r)$:
\begin{equation}
  \label{eqn:dotrexp}
  (\bold F \bold s_{\rm new} \cdot \bold F \bold s_{\rm new}) \eq
  \bold F \Delta \bold x \cdot \bold F \Delta \bold x +
  2 \beta (\bold F \Delta \bold x \cdot \bold F \bold s) +
  \beta^2\,\bold F \bold s \cdot \bold F \bold s\;.
\end{equation}
Differentiating with respect to $\beta$ and setting the derivative to
zero,
we find that
\begin{equation}
  \label{eqn:beta0}
  0 \eq 2 (\bold F \Delta \bold x + \beta \bold F \bold s) 
  \cdot \bold F \bold s\;.
\end{equation}
Equation (\ref{eqn:beta0}) states that the \emph{conjugate direction}
$\bold F \bold s_{\rm new}$ is orthogonal (perpendicular) to the
previous conjugate direction $\bold F \bold s$. It also defines the
value of $\beta$ as
\begin{equation}
  \label{eqn:beta}
  \beta \eq - \frac{ (\bold F \Delta \bold x \cdot \bold F \bold s )}
  {(\bold F \bold s \cdot \bold F \bold s )}\;.
\end{equation}
\par
Can we do even better? The positive quantity that we minimized in
(\ref{eqn:dotrexp}) decreased by
\begin{equation}
  \label{eqn:sdecr}
  \bold F \Delta \bold x \cdot \bold F \Delta \bold x -
  \bold F \bold s_{\rm new} \cdot \bold F \bold s_{\rm new} \eq
  \frac{ (\bold F \Delta \bold x \cdot \bold F \bold s )^2}
  {(\bold F \bold s \cdot \bold F \bold s )}
\end{equation}
Can we decrease it further by adding another previous step? In
general, the answer is positive, and it defines the method of
conjugate directions. I will state this result without a formal proof
(which uses the method of mathematical induction). 
\begin{itemize}
\item If the new step is
composed of the current direction and a combination of all the
previous steps:
\begin{equation}
  \label{eqn:sn}
  \bold s_n \eq \Delta \bold x_n + \sum_{i < n} \beta_i \bold s_i\;, 
\end{equation}
then the optimal convergence is achieved when
\begin{equation}
  \label{eqn:betai}
  \beta_i \eq - \frac{ (\bold F \Delta \bold x_n \cdot \bold F \bold s_i )}
  {(\bold F \bold s_i \cdot \bold F \bold s_i )}\;.
\end{equation}
\item The new conjugate direction is orthogonal to the previous ones:
  \begin{equation}
    \label{eqn:cdortho}
    (\bold F \bold s_n \cdot \bold F \bold s_i) \eq 0 
    \quad \mbox{for all} \quad i < n
  \end{equation}
\end{itemize}
\par
To see why this is an optimally convergent method, it is sufficient to
notice that vectors $\bold F \bold s_i$ form an orthogonal basis in
the data space. The vector from the current residual to the smallest
residual also belongs to that space. If the data size is $n$, then $n$
basis components (at most) are required to represent this vector, hence
no more then $n$ conjugate-direction steps are required to find the
solution.
\par
The computation template for the method of conjugate directions is
\label{'cdtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> {\rm iterate \{ }                                                    \\
\>      \>  $\Delta \bold x   \padarrow {\rm random\ numbers}$          \\
\>      \>  $\bold s   \padarrow \Delta \bold x + 
\sum_{i < n} \beta_i \bold s_i \quad \mbox{\rm where} \quad 
\beta_i = - \frac{(\bold F \Delta \bold x \cdot \bold F \bold s_i )}
  {(\bold F \bold s_i \cdot \bold F \bold s_i )}$                       \\
\>      \>  $\Delta \bold r\  \padarrow \bold F \bold  s$               \\
\>      \> $\alpha \padarrow
                -(       \bold r \cdot \Delta\bold r )/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\bold s$              \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}
\par
Let us derive this, as well as some useful related formulas.
\par
According to formula (\ref{eqn:newresperp}), the new residual $\bold
r_{\rm new}$ is orthogonal to the conjugate direction $\Delta \bold r
= \bold F \bold s_{\rm new}$. According to the orthogonality condition
(\ref{eqn:cdortho}), it is also orthogonal to all the previous
conjugate directions. Defining $\Delta \bold x$ equal to the gradient
$\bold F\T \bold r$ and applying the definition of the adjoint
operator, it is convenient to rewrite the orthogonality condition in
the form
  \begin{equation}
    \label{eqn:rnortho}
    0 \eq (\bold r_n \cdot \bold F \bold s_i) \eq 
    (\bold F\T \bold r_n \cdot \bold s_i) \eq
    (\Delta \bold x_{n+1} \cdot \bold s_i) 
    \quad \mbox{for all} \quad i \leq n
  \end{equation}
  According to formula (\ref{eqn:sn}), each solution step $\bold s_i$
  is just a linear combination of the gradient $\Delta \bold x_i$ and
  the previous solution steps. We deduce from formula
  (\ref{eqn:rnortho}) that
  \begin{equation}
    \label{eqn:cgortho}
    0 \eq (\Delta \bold x_n \cdot \bold s_i) \eq 
    (\Delta \bold x_n \cdot \Delta \bold x_i)
    \quad \mbox{for all} \quad i < n
  \end{equation}
  In other words, in the method of conjugate gradients, the current
  gradient direction is always orthogonal to all the previous
  directions. The iteration process constructs not only an orthogonal
  basis in the data space but also an orthogonal basis in the model
  space, composed of the gradient directions.
  
  Now let us take a closer look at formula (\ref{eqn:betai}). Note
  that $\bold F \bold s_i$ is simply related to the residual step at
  $i$-th iteration: 
  \begin{equation}
  \label{eqn:simple}
\bold F \bold s_i = \frac{\bold r_i - \bold
    r_{i-1}}{\alpha_i}\;.
  \end{equation}
  Substituting relationship (\ref{eqn:simple}) into formula
  (\ref{eqn:betai}) and applying again the definition of the adjoint
  operator, we obtain
\begin{equation}
  \label{eqn:betan}  
   \beta_i = 
  - \frac{ \bold F \Delta \bold x_n \cdot (\bold r_i - \bold r_{i-1})}
  {\alpha_i (\bold F \bold s_i \cdot \bold F \bold s_i )} =
  - \frac{\Delta \bold x_n \cdot \bold F\T (\bold r_i - \bold r_{i-1})}
  {\alpha_i (\bold F \bold s_i \cdot \bold F \bold s_i )} =
  - \frac{ \Delta \bold x_n \cdot (\Delta \bold x_{i+1} - \Delta \bold x_i)}
  {\alpha_i (\bold F \bold s_i \cdot \bold F \bold s_i )} 
\end{equation}
Since the gradients $\Delta \bold x_i$ are orthogonal to each other,
the dot product in the numerator is equal to zero unless $i = n-1$. It
means that only the immediately preceding step $\bold s_{n-1}$
contributes to the definition of the new solution direction $\bold
s_n$ in (\ref{eqn:sn}). This is precisely the property of the
conjugate gradient method we wanted to prove.
\par
To simplify formula (\ref{eqn:betan}), rewrite formula (\ref{eqn:alfa}) as
\begin{equation}
  \label{eqn:cgalfa}
  \alpha_i \eq - \frac 
  { (\bold r_{i-1} \cdot \bold F \Delta \bold x_i )}
  {( \bold F \bold s_i \cdot \bold F \bold s_i ) } \eq - \frac
  { (\bold F\T \bold r_{i-1} \cdot \Delta \bold x_i )}
  {( \bold F \bold s_i \cdot \bold F \bold s_i ) } \eq - \frac
  { (\Delta \bold x_i \cdot \Delta \bold x_i )}
  {( \bold F \bold s_i \cdot \bold F \bold s_i ) }
\end{equation}
Substituting (\ref{eqn:cgalfa}) into (\ref{eqn:betan}), we obtain 
\begin{equation}
  \label{eqn:cgbeta}  
   \beta = 
  - \frac{( \Delta \bold x_n \cdot \Delta \bold x_n)}
  {\alpha_{n-1} (\bold F \bold s_{n-1} \cdot \bold F \bold s_{n-1} )} =
  \frac{(\Delta \bold x_n \cdot \Delta \bold x_n)}
  {(\Delta \bold x_{n-1} \cdot \Delta \bold x_{n-1})}\;.
\end{equation}
\par
The computation template for the method of conjugate gradients is then
\label{'cgtemplate'}
\begin{tabbing}
mmmmmm \= mmmmmm \= mmmmm \kill
\> $\bold r \padarrow \bold F \bold x - \bold d$ \\
\> $\beta \padarrow 0$ \\
\> {\rm iterate \{ }                                               \\
\>      \>  $\Delta \bold x   \padarrow \bold F\T \bold r$          \\
\>      \>  {\rm if not the first iteration} 
$\beta \padarrow \frac{ (\Delta \bold x \cdot \Delta \bold x )}
                            { \gamma}$                              \\
\>      \>  $\gamma \padarrow (\Delta \bold x \cdot \Delta \bold x )$ \\
\>      \>  $\bold s   \padarrow \Delta \bold x + \beta \bold s$      \\
\>      \>  $\Delta \bold r  \padarrow \bold F \bold  s$               \\
\>      \> $\alpha \padarrow
                - \gamma/
                 (\Delta \bold r \cdot \Delta\bold r )
                $
                \\
\>      \> $\bold x   \padarrow \bold x   + \alpha\bold s$              \\
\>      \> $\bold r\  \padarrow \bold r\  + \alpha\Delta \bold r$       \\
\>      \> \}                                                   
\end{tabbing}
\end{comment}

%Module \texttt{conjgrad} \vpageref{lst:conjgrad} provides an
%implementation of this method. The interface is exactly similar to
%that of \texttt{cgstep} \vpageref{lst:cgstep}, therefore you can
%use \texttt{conjgrad} as an argument to \texttt{solver}
%\vpageref{lst:smallsolver}. 
%
%When the orthogonality of the gradients, (implied by the classical
%conjugate-gradient method) is not numerically assured, the
%\texttt{conjgrad} algorithm may loose its convergence properties. This
%problem does not exist in the algebraic derivations, but appears in
%practice because of numerical errors. A proper remedy is to
%orthogonalize each new gradient against previous ones. Naturally, this
%increases the cost and memory requirements of the method.
%
%\moddex{conjgrad}{one step of CG}

%\section{REFERENCES}

%\reference{Gill, P.E., Murray, W., and Wright, M.H., 1981,
%        Practical optimization:  Academic Press.
%        }
%\reference{Hestenes, M.R., and Stiefel, E., 1952,
%        Methods of
 %       conjugate gradients for solving linear systems:
 %       J. Res. Natl. Bur. Stand., {\bf 49}, 409-436.
 %       }
%\reference{Luenberger, D.G., 1973,
%        Introduction to linear and nonlinear programming:
%        Addison-Wesley.
%        }
%\reference{Nolet, G., 1985,
%        Solving or resolving inadequate and noisy
%        tomographic systems:
%        J. Comp. Phys., {\bf 61}, 463-482.
%        }
%\reference{Paige, C.C., and Saunders, M.A., 1982a,
 %       LSQR: an algorithm for sparse linear equations
  %      and sparse least squares:
 %       Assn. Comp. Mach. Trans. Mathematical Software,
 %       {\bf 8,} 43-71.
 %       }
%\reference{Paige, C.C., and Saunders, M.A., 1982b,
%        Algorithm 583, LSQR:
%        sparse linear equations and least squares problems:
%        Assn. Comp. Mach. Trans. Mathematical Software,
%        {\bf 8,}  195-209.
%        }
%\reference{Strang, G., 1986,
%        Introduction to applied mathematics:
%        Wellesley-Cambridge Press.
%        }

%\end{notforlecture}

\begin{exer}
\item It is possible to reject two dips with the operator:
\begin{equation}
(\partial_x + p_1 \partial_t)(\partial_x + p_2 \partial_t)
\end{equation}
This is equivalent to:
\begin{equation}
\left(
\frac{\partial^2}{\partial x^2} + a \frac{\partial^2}{\partial x \partial t} + b \frac{\partial^2}{\partial t^2} 
\right)
u(t,x) \eq v(t,x) \quad \approx \quad 0
\end{equation}
where $u$ is the input signal, and $v$ is the output signal.  Show how to solve for $a$ and $b$ by minimizing the energy in $v$.

\item Given $a$ and $b$ from the previous exercise, what are $p_1$ and $p_2$?

\item
Reduce $\bold d =\bold F\bold m$ to the special case of one data point and two model points like this:
\begin{equation}
d \quad =\quad
\left[
\begin{array}{cc}
2 & 1
\end{array}
\right]
\left[
\begin{array}{c}
m_1
\\
m_2
\end{array}
\right]
\end{equation}
What is the null space?

\item In 1695,
150 years before Lord Kelvin's absolute temperature scale,
120 years before Sadi Carnot's PhD.~thesis,
40 years before Anders Celsius,
and 20 years before Gabriel Fahrenheit,
the French physicist Guillaume Amontons,
deaf  since birth,
took a mercury manometer (pressure gauge) and sealed it inside a glass pipe (a constant volume of air).
He heated it to the boiling point of water at 100$^\circ$C.
As he lowered the temperature to freezing at 0$^\circ$C,
he observed the pressure dropped by {25\%} .
He could not drop the temperature any further, but he supposed that if he could drop it further by a factor of three,
the pressure would drop to zero (the lowest possible pressure), and the temperature would have been the lowest possible temperature.
Had he lived after Anders Celsius, he might have calculated this temperature to be $-300^\circ$C (Celsius).
Absolute  zero is now known to be $-273^\circ$C.
\par
It is your job to be Amontons' lab assistant.
You make your {\it i}-th measurement of temperature $T_i$ with Issac
Newton's thermometer; 
and you measure pressure $P_i$ and volume $V_i$ in the metric system.
Amontons needs you to fit his data with the regression $0 \approx \alpha (T_i - T_0 ) - P_i V_i$ and calculate the temperature shift $T_0$ that Newton should have made when he defined his temperature scale.
Do not solve this problem!
Instead,
cast it in the form of equation (\ref{eqn:carnot}),
identifying the data $d$ and the two column vectors $f_1$ and $f_2$ that are the fitting functions.
Relate the model parameters $x_1$ and $x_2$ to the physical parameters $\alpha$ and $T_0$ .
Suppose you make ALL your measurements at room temperature,
can you find $T_0$ ?
Why or why not? 

\item
One way to remove a mean value $m$ from signal $s(t)= \bold s$
is with the fitting goal $\bold 0 \approx \bold s - m$.
What operator matrix is involved?

\item
What linear operator subroutine from Chapter \ref{paper:ajt}
can be used for finding the mean?

\item
How many CD iterations should be required to get the exact mean value?

\item
Write a mathematical expression for finding the mean by the CG method.

\end{exer}


\clearpage

