\lefthead{Fomel \& Claerbout} \righthead{Galilee} 
\footer{SEP--84}

\title{Searching the Sea of Galilee \\ 
\normalsize \bf{The splendors and miseries of
iteratively reweighted least squares}}
%\keywords{interpolation, linear, modeling, graphics, noise, least squares }

\email{sergey@sep.stanford.edu, jon@sep.stanford.edu} 
\author{Sergey Fomel and Jon F. Claerbout}
\maketitle

%\vspace{1cm}

\begin{quote} As the former time made light the land of Zebulun and
the land of 
Naphtali, so the latter hath honoured the way of the sea, beyond
the Jordan, 
Galilee of the nations. {\em Isaiah 9:1} 
\end{quote} 

\begin{abstract}
We applied the inverse linear interpolation method to pro\-cess a 
bot\-tom sounding survey data set from the Sea of Galilee in Israel.
Non-Gaussian behavior of the noise led us to employ a version of the
iteratively reweighted least squares (IRLS) technique. The IRLS enhancement
of the method was able to remove the image artifacts caused by the
noise at the cost of a loss in the image resolution. Untested
alternatives leave room for further research.
\end{abstract}

\section{Introduction}
%%%%%%%%%%%%%%%%%%%

\plot{galgol}{width=5.in}{Map of the Sea of Galilee
vicinity, grabbed from a state
gopher site in Israel.}

The Sea of Galilee, also known in Israel as Lake Kinneret (Figure
\ref{fig:galgol}), is a unique natural 
object because of both its ancient history and its geography. It
is a rare 
case of a freshwater lake below sea level. From a geological point
of view, the location of the lake is connected to a great rift crossing east
Africa. Our project addressed processing of the bottom sounding survey data
collected by Zvi Ben Abraham at Tel-Aviv University \shortcite{zvi}.
\inputdir{gal}
\par
The original data include over 100,000 triples $(x_i,y_i,z_i)$ . The range of
$x$'s is about 12 km, and the range of $y$'s is about 20 km. 
The depth coordinates
range from 211 m  to 257 m below sea level. 
Consecutive measurements are
close to each other and cover the area of the lake with a fairly even grid.
However, a number of empty bins remains even after sparse binning (Figure
\ref{fig:galsea}). Using primitive preprocessing editing, we got rid of several
measurements containing zeroes or evident mispositioning errors.

\plot{galsea}{width=5.0in,height=3.in}{Sea of Galilee data after
simple binning. On the left is a sparse grid (the size of a bin is
about 100 by 100 m); on the right, a denser grid (50 by 50 m). The
bordered square was chosen for the first set of experiments.}
\par
The major goal of this project was to interpolate the data to a regular grid.
Properly interpolated data can enable searching for the 
local features of interest,
including geological structures as well as sunken ships and other exotic
archeological objects. Our attention was focused on 
developing a data interpolation
technique that might be useful in a wide variety of geophysical applications.
\par
The general method we chose for the problem is linear inverse interpolation
with a known filter, as described by \shortcite{Claerbout.tdf.82}.
This method did 
produce the desired result, but a non-Gaussian noise distribution caused
serious problems in its implementation. To cope with these problems,
a version of the iteratively reweighted least squares (IRLS) technique
\cite{SEG.1988.S7.1,Darche.sep.61.281}
was applied.

\section{METHOD: INVERSE LINEAR INTERPOLATION WITH KNOWN FILTER}

The theory of inverse linear interpolation with a known filter is
given in {\em Applications of Three-Dimensional Filtering}
\cite{Claerbout.tdf.82}, section 2.6, and can be easily extended to
the 2-D case. The algorithm is based on the following procedure. To invert 
the data vector ${\bf d}$ given on an irregular grid for a regularly sampled
model ${\bf m}$, 
we run the conjugate-gradient solver on the system of equations
%\samepage{
\begin{equation} {\bf d \approx Lm\;, \label{eqn:linear} }\end{equation} 
\begin{equation} {\bf 0 \approx \epsilon \, Am\;. \label{eqn:regression}}
\end{equation}
%}
Equation (\ref{eqn:linear}) formulates the basic assumption of the method,
stating that the data 
is related to 
the model with a linear interpolation operator $\mathbf{L}$. The next equation
(\ref{eqn:regression}) is required to
constrain an underdetermined part of the inverse problem. Minimizing the
output power 
of the model 
filtered by some roughening filter $\mathbf{A}$ is a way to smooth the
model components that are not determined by equation (\ref{eqn:linear}). 
Laplacian filter of 
the form
\begin{equation} 
{\bf A}=
\begin{array}{|c|c|c|}
\hline
. & 1 & . \\
\hline
1 & -4 & 1 \\
\hline
. & 1 & . \\
\hline
\end{array}
\label{eqn:laplacian}
\end{equation}
is a conventional choice for smoothing in two dimensions.

\section{FIRST RESULTS}

To test the inverse linear interpolation method, we cut a 4 by 4 km patch
from the initial data plane and posed the problem of interpolating the
data to a 
regular mesh. Figure \ref{fig:galpat} shows the patch
after sparse, medium, and dense simple binning.
\plot{galpat}{width=6.0in,height=2.in}{Simple binning of a patch.
The left plot is the result of binning on a sparse grid (the size of a
bin is 160 by 160 m); the right, on a 
denser grid ($40 \times 40$ m). The middle plot represents the medium
case (80 by 80 m). Black spots are empty bins.}
%\pagebreak
\par
The result of the inverse linear interpolation on the dense grid after
200 iterations is shown in 
Figure \ref{fig:gallap}. For a better display, we convolve the interpolated
data with a 
set of first-order derivatives, taken in different directions, such as:
%$\pmatrix{1&.\cr -1&.\cr}$, $\pmatrix{1&.\cr .&-1\cr}$, 
%$\pmatrix{1&-1\cr.&.\cr}$, $\pmatrix{.&-1\cr 1&.\cr}$
\begin{center}
$\begin{array}{cccc}
\begin{array}{|c|c|}
\hline
1 & . \\
\hline
-1 & . \\
\hline
\end{array} &
\begin{array}{|c|c|}
\hline
1 & . \\
\hline
. & -1 \\
\hline
\end{array} &
\begin{array}{|c|c|}
\hline
1 & -1 \\
\hline
. & . \\
\hline
\end{array} &
\begin{array}{|c|c|}
\hline
. & -1 \\
\hline
1 & . \\
\hline
\end{array}
\end{array}
$  .
\end{center}
\plot{gallap}{width=6.0in,height=2.in}{Interpolation on the dense
grid. The left plot is the initial result of the LS interpolation. The middle
plot is the same result
illuminated by a directional 
first-order derivative filter. The right plot is the illuminated result of the
simple binning on a sparse grid.}
\par
The result looks encouraging, since the interpolation produced a
more detailed 
image than simple binning. However, this image is not satisfactory.
Studying the sharp dipping discontinuity in the upper-right quarter of
the patch 
revealed that it was produced not by a real geological structure but by an
inconsistent track present in the data. The results of applying the
interpolation 
technique to the whole lake bottom (the left side of Figure
\ref{fig:galsen}) show more 
examples of that 
kind of noise and demonstrate that the problem of data inconsistency among
different tracks is common for most parts of the lake. 
\par
Such an inconsistency is
easily explained by taking into account the fact that the measurements
were made on 
different days with different weather and human conditions. To allow for these
circumstances, we had to reformulate the initial least squares optimization
approach applied to inversion.
%\pagebreak
\plot{galsen}{width=5.0in,height=3.in}{Sea of Galilee
data after LS 
inversion interpolation. The left plot is the result of interpolation
with a Laplacian
filter; the right, of interpolation with a gradient filter.}

\section{ENHANCEMENTS OF THE METHOD: IRLS}

A minor improvement in the method was the change from the Laplacian filter
(\ref{eqn:laplacian}) in equation (\ref{eqn:regression}) to the gradient filter consisting
of the two
orthogonal first-order derivatives
\begin{equation} 
{\bf A_1}=
\begin{array}{|c|c|}
\hline
1 & -1 \\
\hline
\end{array}
,\,\, {\bf A_2}=
\begin{array}{|c|}
\hline
1 \\
\hline
-1 \\
\hline
\end{array}
\,\,.
\label{eqn:gradient} 
\end{equation}
The reason for this change is evident in Figure \ref{fig:galsen}. The
gradient filter is more appropriate for the
sharp edges (the first-derivative jump) of the lake bottom, while the Laplacian
filter tends to smooth them out. This fact is related to the close
correspondence between gradient filter smoothing and local linear
interpolation on one side, and Laplacian smoothing and cubic (spline)
interpolation on the other. 
\par
The major enhancement is the introduction of a weighting operator
${\bf W}$ into 
equation 
(\ref{eqn:linear}). Thus system (\ref{eqn:linear}) - (\ref{eqn:regression}) takes the
form
%\samepage{
\begin{equation} {\bf 0 \approx W (d - Lm)}\;, \label{eqn:irls}
\end{equation}
\begin{equation} 
{\bf 0 \approx \epsilon \, A_1 m}\;, \label{eqn:gradient1} \end{equation}
\begin{equation} 
{\bf 0 \approx \epsilon \, A_2 m}\;. \label{eqn:gradient2} \end{equation}
%}
Though we didn't apply any formal straightforward theory to choose the
weighting operator $W$, our heuristic choice followed two formal principles:
\begin{enumerate} 
\item Bad (inconsistent) data points are indicated by large
values of the residual $r = d - Lm$ left after conventional least squares
inversion. 
\item Abnormally large residuals attract most of the conjugate
gradient solver's efforts, directing it the wrong way. The residual should be
whitened to distribute the solver's attention equally among all the
data points and emphasize the role of the ``consistent majority.'' 
\end{enumerate}
\par
Figure \ref{fig:galdat} demonstrates changes in the residual space caused by
the weighting. 
Each plot contains one long vector cut into several traces for better
viewing. 
The top left plot is the original data
from the patch used in the first experiments. The top right plot is
the residual after 200 
conventional conjugate gradient iterations. Note two major features of the
residual distribution: statically shifted tracks 
(the zero-frequency
component) and local bursts of energy (mostly in the upper part of the
plot). To take away the zero-frequency component, we convolved the
residual with 
the first-derivative operator. The result of differentiation appears on the
bottom left plot. 
Finally, to take into account both large spikes created by differentiation and
local bursts of energy, we constructed the following weighting function:
\begin{equation} w_i = w(r_i) = {{2\,\bar{r}} \over {\mid r_i \mid +
\bar{r_i}}} \,. 
\label{eqn:weight} \end{equation} 
Here $\bar{r}$ stands for the median of the absolute
residual values from the whole data set, and $\bar{r_i}$ is the
median in a 
small window around a current point $r_i$. The denominator of the weighting
function is designed to reduce the value of the residual if it belongs
either to 
a spike (large $\mid r_i \mid$) or to a local burst of energy (large
$\bar{r_i}$). The numerator was chosen for scaling purposes. The
bottom right plot in Figure
\ref{fig:galdat} justifies our choice of the weighting function, displaying the
residual derivative after weighting. The signal distribution resembles white
noise in accordance with the second formal principle.

\activeplot{galdat}{width=6.0in,height=6.0in}{figdir}{View from the data space
(explanation in the text).}
\par
Thus the weighting operator ${\bf W}$ in equation (\ref{eqn:irls}) includes two
components: the first-derivative filter ${\bf D}$ taken in the data space
along the recording tracks and the weighting function $w$. Since $w$
depends upon the residual, the inversion problem becomes nonlinear,
and the algorithm approaches the IRLS class \cite{SEG.1988.S7.1}. 
\par
The result of the IRLS inversion is shown in Figure
\ref{fig:galrep}. While the noisy
portions of the model disappeared as we had
expected, a large-scale structure (valley) crossing the lake from north to
south remained on the image.
Comparison of Figures
\ref{fig:galavl} and \ref{fig:galavn} demonstrates that the price for that
improvement is a certain loss in the
image resolution. This type of loss occurs because we rely on the
model smoothing to remove the noise influence. The role of the smoothing
increases when we apply IRLS in the manner of piece-wise
linearization. The first step of the piece-wise linear algorithm is
the conventional least squares minimization. The next step consists of
reweighted least squares iterations made in several cycles with reweighting
applied only at the beginning of each cycle. Thus the problem is linearized
within a cycle, and the conjugate gradient solver opens each cycle with the
steepest descent iteration. 
What happens when the nonlinear reweighting is applied on each
iteration is shown in Figure \ref{fig:galres}. Round
``villages'' on the bottom of the lake were identified as the
consequence of
occasional erroneous data points. 

\activeplot{galrep}{width=5.0in,height=8.in}{figdir}{Result of the
IRLS inverse interpolation (piece-wise linear).}

\activeplot{galres}{width=5.0in,height=8.in}{figdir}{Result of the
IRLS inverse interpolation (nonlinear).}

%\pagebreak
\section{DISCUSSION}
 
The method of inverse linear interpolation with IRLS enhancement was able to
produce a fairly clear image of the Sea of Galilee bottom, showing large-scale,
presumably geological structures. The lessons learned from the
Sea of Galilee project include the following:
\begin{itemize} 
\item If a model (interpolation result) contains sharp edges, it can
be preferable to use a gradient filter instead of a Laplacian one for
smoothing purposes. 
\item The iteratively
reweighted least-squares approach is capable of removing non-Gaussian noise
influence on the result of inversion. 
\item The conjugate-gradient solver can behave differently in nonlinear and piece-wise linear implementations of nonlinear problems.
\end{itemize} 
\par
The main cause of the regular noise in the Galilee data set is
the inconsistency among repeated measurements. An
alternative approach could be applied to the problem
of the data track inconsistencies. A well-known
example of such an approach is the seismic mis-tie resolution
technique \cite{GEO.56.11.18251830}. Combining a technique of this
kind with the
inverse  linear interpolation is a possible direction for 
future research.  
\par
Another interesting
untested opportunity is preconditioning. As Bill Harlan has pointed out, 
the two equations in the system (\ref{eqn:linear}) - (\ref{eqn:regression})
contradict each other,  
since the first one aims to build details in the model, while
the second tries to smooth them out. Applying the model
preconditioning method could change the contradictory 
nature of the algorithm and speed up its convergence. 
The inversion problem takes the form
\begin{equation} {\bf 0 \approx W (d - LBx)} \label{eqn:precon}
\end{equation}
\begin{equation} {\bf 0 \approx \epsilon \, x} \,\,\,,\label{eqn:nograd} 
\end{equation}
where ${\bf m=Bx}$, and ${\bf B}$ is a smoothing operator (an approximate
inverse of the  roughening operator ${\bf A}$ in (\ref{eqn:regression})). 
Possible advantages of preconditioning 
deserve special investigation.

\activeplot{galavl}{width=5in,height=3.5in}{figdir}{Result of the
LS inverse interpolation plotted in AVS.}
\activeplot{galavn}{width=5in,height=3.5in}{figdir}{Result of the
IRLS inverse interpolation plotted in AVS.}

\bibliographystyle{seg}
\bibliography{SEG,paper}









