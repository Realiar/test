%\documentstyle[seg,manuscript,algorithm,algorithmic]{revtex} % This line gives a manuscript style
%\documentstyle[algorithm,algorithmic]{revtex} % This line gives a manuscript style
%
%\begin{document}
\published{Geophysics, 71, no. 4, R59-R67, (2006)}
\title{Conjugate guided gradient (CGG) method for robust inversion and its application to velocity-stack inversion }

\author{Jun Ji}

\address{Department of Information System Engineering, \\ Hansung University, Samsun-dong 2-ga, Sungbuk-ku, Seoul, Korea}

%\date{\today}
%
\maketitle
%{CGG method for robust inversion}
\begin{abstract}
This paper proposes a modified conjugate gradient (CG) method,
called the conjugate guided gradient (CGG) method, 
that is a robust iterative inversion method producing a parsimonious model estimation.
The CG method for solving least-squares (LS) (i.e. $\ell^2$-norm minimization)
problems is modified to solve for different norms
or different minimization criteria by guiding the gradient vector appropriately during iteration steps.
The guiding is achieved by iteratively reweighting either the residual vector 
or the gradient vector during iteration steps like IRLS (Iteratively Reweighted Least Squares) method does.
The robustness is achieved by weighting the residual vector
and the parsimonious model estimation is obtained by weighting the gradient vector.
Unlike the IRLS method, however, the CGG method doesn't change 
the corresponding forward operator of the problem 
and is implemented in a linear inversion template.
Therefore the CGG method requires less computation than the IRLS method does.
Since the solution in the CGG method is found in a least-squares sense 
along the gradient direction guided by the weights, 
the solution found by the CGG method can be interpreted 
as the LS solution located in the guided gradient direction.
Guiding the gradient gives us more flexibility in choice of
weighting parameters than the one of the IRLS method.
I applied the CGG method to velocity-stack inversion,
and the results show that the CGG method gives 
a far more robust and parsimonious model estimation
than the standard $\ell^2$-norm solution, 
with the results comparable to the $\ell^1$-norm
IRLS solution.
\end{abstract}

\section{INTRODUCTION}

The inverse problem has received considerable attention 
in various geophysical applications. 
One of the most popular inverse solutions is the least squares (LS) solution.
The LS solution is a member of a family of generalized 
$\ell^p$-norm solutions that are deduced from a maximum-likelihood formulation.
This formulation allows the design of various statistical inversion solutions.  
Among the various $\ell^p$-norm solutions, 
the $\ell^1$-norm solution is more robust than the $\ell^2$-norm solution, 
because it is less sensitive to spiky, high-amplitude noise 
\cite{GEO38-05-08260844,GEO44-01-00390052,GIT00-00-00250050,SEG-1988-S7.1}.
In order to take advantages of both $\ell^2$ and $\ell^1$ norm solutions, 
hybrid $\ell^1/\ell^2$ - norm solutions are also 
tried \cite{Huber:73,GEO62-04-11831195,GEO68-04-13101319}.
However, the implementation of the algorithm to find
$\ell^1$-norm solutions is not a trivial task, which uses linear programming techniques 
\cite{GEO44-01-00390052} and needs a large quantity of computer memory.
Iterative inversion algorithm called Iteratively Reweighted Least Squares (IRLS) method
\cite{GEO51-02-03570368,GIT00-00-00250050,SEG-1988-S7.1,GEO62-04-11831195} 
is a good choice for solving
$\ell^p$-norm minimization problems for $1\le p \le2$.  
The IRLS approach which was originally developed for nonlinear inversion
can be adapted to solve linear inverse problems in $\ell^p$-norm sense
by modifying the iterative inversion method such as conjugate gradient (CG) method
~\cite{Darche.sep.61.281,Nichols.sep.82.1,Claerbout.iee.www}.

The $\ell^p$-norm minimizing IRLS inversion can be used to any inversion problem 
whose required properties are the robustness to spiky noise and the parsimony of the model,
and the velocity-stack inversion is one of them.
The velocity-stack inversion is useful not only for velocity analysis 
but also for various data processing applications.
The applications of the velocity-stack inversion include
non-hyperbolic noise removal in CMP gathers ~\cite{Nichols.sep.82.1,GEO68-04-13101319}, 
multiple-removal 
~\cite{GEO50-12-27272741,Hampson-22-044055,GEO57-03-03860395,SEG-1995-1464,SEG-1995-1460,TLE18-01-00660073,SEG-2000-19531956}
and missing offset reconstruction ~\cite{Ji.sep.82.195,GEO60-04-11691177},
and so on.
In these applications, the velocity-stack panels obtained 
by inversion are usually required
to be as spiky and sparse as possible.
Then the hyperbolic events represented by the isolated peaks 
in the velocity-stack panel
are more easily distinguished from the rest of the noise.

This paper introduces a modification of the conventional CG method for solving LS problem 
so as to be robust and produce a parsimonious model estimation.
The modified CG method is called Conjugate Guided Gradient (CGG) method.
The modification of the CG method is performed by guiding the gradient vector during the iteration steps.
Guiding the gradient vector is achieved by iteratively reweighting 
either the residual vector or the gradient vector during iteration steps 
like IRLS (Iteratively Reweighted Least Squares) method does.
The weighting of the residual vector makes the CGG method robust
and the weighting of the gradient vector makes the CGG method produce a parsimonious model estimation.
In the first section, I review
the conventional CG method for solving LS problems and
show how the IRLS approach differs from the standard LS approach.
Next, I explain the CGG method and contrast it with both LS and IRLS methods.
Finally the proposed CGG method is tested on velocity-stack inversions
with both synthetic and real data and the results of the CGG method are compared
 with conventional LS and $\ell^1$-norm IRLS results.

\section{CG method for LS Inversion}
Most inversion problems start by formulating the forward problem,
which describes the forward operator $\mathbf L$ 
that transforms the model vector $\mathbf m$ into the data vector $\mathbf d$

\begin{equation}
\mathbf d = \mathbf L \mathbf m .
\end{equation}
In general, the measured data $\mathbf d$ may be inexact, and 
the forward operator $\mathbf L$ may be ill-conditioned. 
In that case, instead of solving the above equation directly, 
different approaches are used to find an optimum solution $\mathbf m$ 
for given data $\mathbf d$.
The most popular method is finding a solution that minimizes the misfit between
the data $\mathbf d$ and the modeled data $\mathbf L\mathbf m$.
The misfit is often referred as residual vector $\mathbf r$ and is described as follows:

\begin{equation}
\mathbf r =  \mathbf L\mathbf m - \mathbf d .
\end{equation}
In least-squares inversion, the solution $\mathbf m$ is the one that minimizes 
the squares of the residual vector as follows:

\begin{equation}
 min_m ({\mathbf r^T r}) = min_m {(\mathbf L\mathbf m - \mathbf d)^T(\mathbf L\mathbf m - \mathbf d)} .
\end{equation}

Most iterative solvers for the LS problem search the minimum solution
on a line or a plane in the solution space.  In the CG algorithm, not
a line, but rather a plane is searched.  A plane is made from an
arbitrary linear combination of two vectors.  One vector is chosen to
be the gradient vector.  The other vector is chosen to be the previous
descent step vector.  Following \cite{ClaerPVI:92}, a
conjugate-gradient algorithm for the LS solution can be summarized as
shown in Algorithm~\ref{alg1}.

\begin{algorithm}
\caption{ CG method for LS solution}
\label{alg1}
\begin{algorithmic}
\STATE $ {\mathbf r} \Leftarrow \mathbf L \mathbf m - \mathbf d $
\WHILE { \rm condition }
\STATE $ \mathbf \Delta \mathbf m \Leftarrow \mathbf L^T \mathbf r $
\STATE $ \mathbf \Delta \mathbf r \Leftarrow \mathbf L \mathbf \Delta \mathbf m $
\STATE $ (\mathbf m, \mathbf r) \Leftarrow \rm cgstep(\mathbf m, \mathbf r, \mathbf \Delta \mathbf m, \mathbf \Delta \mathbf r)  $
\ENDWHILE
\end{algorithmic}
\end{algorithm}

In Algorithm~\ref{alg1}, the $\rm condition$ represents a convergence check
such as the tolerance of residual vector $\mathbf r$, 
a maximum number of iteration, and so on.
The subroutine {\mbox cgstep()} updates model $\mathbf m$ and residual $\mathbf r$
using the previous iteration descent vector in the conjugate space 
$\Delta \mathbf s = L (\mathbf m_i - \mathbf m_{i-1})$, where $i$ is the iteration step,
and the conjugate gradient vector $\Delta \mathbf r$.
The update step size is determined by minimizing 
the quadrature function composed from 
$\Delta \mathbf r$ (the conjugate gradient) 
and $\Delta \mathbf s$ (the previous iteration descent vector in the conjugate space)
as follows \cite{ClaerPVI:92}:
$$Q(\alpha,\beta) = (\mathbf r-\alpha \Delta \mathbf r -\beta \Delta \mathbf s)^T
                    (\mathbf r-\alpha \Delta \mathbf r -\beta \Delta \mathbf s) .$$ 
Notice that
the gradient vector ($\Delta \mathbf m$) in the CG method for LS solution 
is the gradient of the squared residual 
and is determined by taking the derivative of the squared residual
(i.e. the $\ell^2$-norm of the residual, $\mathbf r^T \mathbf r$)
with respect to the model $\mathbf m^T$:

\begin{equation}
\Delta \mathbf m = 
{\partial \over \partial \mathbf m^T }
(\mathbf L\mathbf m -\mathbf d)^T(\mathbf L\mathbf m -\mathbf d)
= \mathbf L^T\mathbf r  .
\end{equation}

\subsection{CG method for Iteratively Reweighted Least Squares (IRLS)}

Instead of the $\ell^2$-norm solution obtained by the conventional LS method, 
$\ell^p$-norm minimization solutions, with $1\le p \le2$, are often tried. 
Iterative inversion algorithms called IRLS 
(Iteratively Reweighted Least Squares) 
algorithms have been developed to solve these problems, which lie
between the least-absolute-values problem and the classical least-squares problem.
The main advantage of IRLS is that it provides an easy way to compute 
the approximate $\ell^p$-norm solution.
Among the various $\ell^p$-norm solutions, 
$\ell^1$-norm solutions are known to be more robust than $\ell^2$-norm solutions, 
being less sensitive to spiky, high-amplitude noise 
\cite{GEO38-05-08260844,GEO44-01-00390052,GIT00-00-00250050,SEG-1988-S7.1}.

The problem solved by IRLS is a minimization of the weighted residual/model 
in the least-squares sense. The residual to be minimized in the weighted problem 
is described as \begin{equation}
\mathbf r = \mathbf W_r (\mathbf L \mathbf W_m \mathbf m - \mathbf d)
\end{equation}
where $\mathbf W_r$ and $\mathbf W_m$ are the weights for residual and model, respectively.
These residual and model weights are for enhancing our preference regarding
the residual and model. 
They can be applied separately or together according to a given inversion goal. 
In this section, for simplicity, the explanation will be limited 
to the case of applying both weights together,
but the examples given in a later section will show all the cases 
including the residual and the model weights separately for comparison.
Those weights can be any matrices, but diagonal matrices are often used for them 
and this paper will assume all weights are diagonal matrices.
Then the gradient for the weighted least-squares becomes 
\begin{equation}
\mathbf W_m^T \mathbf L^T \mathbf W_r^T \mathbf r = 
{\partial \over \partial \mathbf m^T} 
(\mathbf L \mathbf W_m \mathbf m -\mathbf d)^T\mathbf W_r^T\mathbf W_r(\mathbf L \mathbf W_m\mathbf m -\mathbf d) .
\end{equation}
A particular choice for the residual weight $\mathbf W_r$ is the one 
that results in minimizing the $\ell^p$-norm of the residual. 
Choosing the $i^{th}$ diagonal element of $\mathbf W_r$
to be a function of the $i^{th}$ component of the residual vector
as follows:
\begin{equation}
diag ({\mathbf W_r})_i = |r_i|^{(p-2)/2},
\label{eqn:rw}
\end{equation}
the $\ell^2$-norm of the weighted residual is then
\begin{equation}
\|\mathbf W_r r\|_2^2
= \mathbf r^T \mathbf W_r^T \mathbf W_r \mathbf r 
= \mathbf r^T \mathbf W_r^2 \mathbf r = \|\mathbf r\|_p^p  .
\end{equation}
Therefore, the minimization of the $\ell^2$-norm of the weighted residual 
with an weight as shown Equation~(\ref{eqn:rw})
can be thought as a minimization of the $\ell^p$-norm of the residual.
This method is valid for $\ell^p$-norms where $1 \le p \le 2$. 
When the $\ell^1$-norm is desired, the weighting is as follows:
$$ diag({\mathbf W_r})_i = |r_i|^{-1/2}.$$
This weight will reduce the contribution of large residuals
and improve the fit to the data that is already well-estimated.
Thus, the $\ell^1$-norm-based minimization is robust, 
less sensitive to noise bursts in the data. 
In practice the weighting operator is modified slightly to avoid dividing by zero.
For this purpose, a damping parameter $\epsilon$ is chosen and the weighting operator is modified to be:
\[
diag({\mathbf W_r})_i =
\left\{
\begin{array}{cc}
|r_i|^{-1/2}, & |r_i| > \epsilon \\
\epsilon, & |r_i| \le \epsilon  \\
\end{array}
\right.
\]
The choice of this parameter is related to the distribution of the residual values.
Some authors choose it as a relatively small value like $\epsilon = \max|\mathbf d|/100$
and others choose it as a value that corresponds to a small percentile of data as 2 percentile 
(which is the value with 98\% of the values above and 2\% below).
In this paper, I used the percentile approach to decide the parameter $\epsilon$ because 
it can reflect the distribution of the residual values in it 
and shows more stable behavior in the experiments performed in this paper.

The use of the model weight $\mathbf W_m$ is to enhance our preference regarding the model, 
for example the parsimony or the smoothness of the solution.
The introduction of the model weight corresponds to applying precondition
and solving the problem
$$ \mathbf L \mathbf W_m \hat {\mathbf m} = d$$
followed by
$$ \mathbf m = \mathbf W_m \mathbf{ \hat{m}}.$$
The iterative solution of this system minimizes the energy of the new model parameter $\mathbf{\hat m}$
$$\| \hat{ \mathbf m} \|_2^2 = \mathbf{\hat{m}}^T \mathbf{\hat{m}} = \mathbf m^T \mathbf W_m^{-T}\mathbf W_m^{-1}\mathbf m.$$
In the same vein as the residual weight, the model weight $\mathbf W_m$ can be chosen as
\begin{equation}
diag ({\mathbf W_m})_i = |m_i|^{(2-p)/2}.
\label{eqn:mw}
\end{equation}
Then the weighted model energy that is minimized is now
$$ \mathbf m^T \mathbf W_m^{-2} \mathbf m = \|\mathbf m\|_p^p, $$
which is the $\ell^p$-norm of the model.
When the minimum $\ell^1$-norm model is desired, the weighting is as follows:
$$ diag ({\mathbf W_m})_i = |m_i|^{1/2}.$$ 
The IRLS method can be easily incorporated in CG algorithms by including
the weights $\mathbf W_r$ and $\mathbf W_m$ 
such that the operator $\mathbf L$ has a postmultiplier $\mathbf W_r$ 
and a premultiplier $\mathbf W_m$ 
and the adjoint operator $\mathbf L^T$ has a premultiplier $\mathbf W_m^T$ and
postmultiplier $\mathbf W_r^T$ \cite{Claerbout.iee.www}.
However, the introduction of weights that change during iterations
leads us to implement a nonlinear CG method with two nested loops.
The outer loop is for the iteration of changing weights
and the inner loop is for the iteration of the LS solution for a given weighted operator.
Even though we do not know the real residual/model vector 
at the beginning of the iteration, 
we can approximate the real residual/model with a residual/model 
of the previous iteration step, and it will converge to 
a residual/model that is very close to the real residual/model as the iteration step continues. 
This method can be summarized as Algorithm~\ref{alg2}, where $f(\mathbf r)$ and $f(\mathbf m)$ represent
functions of residual and model described in Equation~(\ref{eqn:rw}) and Equation~(\ref{eqn:mw}), respectively.

\begin{algorithm}
\caption{ CG method for IRLS solution}
\label{alg2}
\begin{algorithmic}
\STATE $ \mathbf r \Leftarrow \mathbf L \mathbf m - \mathbf d $
\WHILE {condition}
\STATE $ {diag } (\mathbf W_r )\Leftarrow \rm f (\mathbf r) $
\STATE $ {diag } (\mathbf W_m )\Leftarrow \rm f (\mathbf m) $
\STATE $ \mathbf r \Leftarrow \mathbf W_r ( \mathbf L \mathbf W_m \mathbf m - \mathbf d ) $
\WHILE {condition}
\STATE $ \mathbf \Delta \mathbf m \Leftarrow \mathbf W_m^T \mathbf L^T \mathbf W_r^T \mathbf r $
\STATE $ \mathbf \Delta \mathbf r \Leftarrow \mathbf W_r \mathbf L \mathbf W_m \mathbf \Delta \mathbf m $
\STATE $ (\mathbf m, \mathbf r) \Leftarrow \rm cgstep(\mathbf m, \mathbf r, \mathbf \Delta \mathbf m, \mathbf \Delta \mathbf r)  $
\ENDWHILE
\STATE $ \mathbf m \Leftarrow \mathbf W_m \mathbf m $
\ENDWHILE
\end{algorithmic}
\end{algorithm}

For efficiency, Algorithm~\ref{alg2} is often implemented not to 
wait until the convergence of the inner loop
and instead implemented to finish the inner loop
after a certain number of iterations 
and recomputes the weights and the corresponding residual
~\cite{Darche.sep.61.281,Nichols.sep.82.1}. 
In order to take advantage of plane search in CG,
however, the number of the iterations of the inner loop 
should be more than or equal to two.
The experiments performed for the examples of this paper have shown 
almost no differences between the results of different iteration steps
of the inner loop. In this paper, therefore, the IRLS algorithm is implemented
to finish the inner loop after two iterations.

\section{Conjugate-Guided-Gradient (CGG) method}
From the algorithmic viewpoint of the CG method,
the IRLS algorithm can be considered as an LS method,
but with its operator, $\mathbf L$, modified by the weights, $\mathbf W_r$ and $\mathbf W_m$.
The only change in the problems to solve that distinguishes the IRLS algorithm 
from the LS one is the substitution of 
$\mathbf W_r \mathbf L\mathbf W_m$ and $\mathbf W_m^T\mathbf L^T \mathbf W_r^T$ 
for $\mathbf L$ and $\mathbf L^T$, respectively.
Since the weights $\mathbf W_r$ and $\mathbf W_m$ are functions of the residual and the model, respectively,
and the residual $\mathbf r$ and the model $\mathbf m$ are changing during the iteration, 
the problem that IRLS method solves is a nonlinear problem. 
Therefore, the IRLS method obtains the $\ell^p$-norm solution at the cost of nonlinear implementation.
I propose another algorithm that obtains $\ell^p$-norm solution
without breaking the linear inversion template.
Instead of modifying the operator which results in nonlinear inversion,
we can choose a way to guide the search 
to find the minimum $\mathbf \ell^2$-norm solution in a specific model subspace
so as to obtain a solution that meets a user's specific criteria.
The specific model subspace could be 
guided by a specific $\mathbf \ell^p$-norm's gradient 
or constrained by an {\it a priori} model.
Such guiding of the model vector can be realized by
weighting the residual vector or gradient vector in the CG algorithm.
Since the weights are basically changing the direction of the gradient vector in the CG algorithm, 
this proposed algorithm is named as Conjugate Guided Gradient (CGG) method.


\subsection{CGG with residual weight guide}
Suppose we apply the same residual weight $\mathbf W_r$ as the one we used in the IRLS method,
to the residual when we compute the gradient 
$\mathbf \Delta \mathbf m$
but do not apply the weight when we compute the conjugate gradient $\mathbf \Delta \mathbf r$.
This means that we do not change the operator from $\mathbf L$ to $\mathbf W_r \mathbf L$,
and the weight affects only the gradient direction.
This corresponds to guiding the gradient direction with a weighted residual,
and the resultant gradient will be the same gradient as 
we used for the $\ell^p$-norm residual solution in the IRLS method. 
Unlike the IRLS method, however, 
we don't need to recompute the residual when the weight has changed
since we did not change the operator while the iteration goes
and the problem is the same problem as before we change the weight 
(i.e. we are solving a linear problem).
This algorithm can be implemented as shown in Algorithm~\ref{alg3}.

\begin{algorithm}
\caption{ CGG method with residual weight guide}
\label{alg3}
\begin{algorithmic}
\STATE $ \mathbf r \Leftarrow \mathbf L \mathbf m - \mathbf d $
\WHILE {condition}
\STATE $ {\rm diag } (\mathbf W_r )\Leftarrow \rm f (\mathbf r) $
\STATE $ \mathbf \Delta \mathbf m \Leftarrow \mathbf L^T \mathbf W_r^T \mathbf r $
\STATE $ \mathbf \Delta \mathbf r \Leftarrow \mathbf L \mathbf \Delta \mathbf m $
\STATE $ (\mathbf m, \mathbf r) \Leftarrow \rm cgstep(\mathbf m, \mathbf r, \mathbf \Delta \mathbf m, \mathbf \Delta \mathbf r)  $
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Notice that Algorithm~\ref{alg3} is different from the original
CG method (Algorithm~\ref{alg1}) only at the step of gradient $\mathbf \Delta \mathbf m$
computation;
the modification of the gradient is performed by changing the residual
before the gradient is computed from it.
By choosing the weight as a function of the residual 
of the previous iteration step,
as we did in the IRLS method, we can guide the gradient vector
to the gradient vector of the $\ell^p$-norm.
Thus the result obtained by weighting the residual in the CGG method
could be interpreted as a localized LS solution in the subspace
composed by the $\ell^p$-norm gradient vectors, 
not in the whole solution space.
The minimum $\ell^2$-norm location is unlikely to be located
along the gradient direction of the different $\ell^p$-norm 
which is guided by the applied weight. 
Therefore, it is more likely that the solution will be close to 
the minimum $\ell^p$-norm location which is guided by the applied weight.

\subsection{CGG with model weight guide}
Another way to modify the gradient direction
is to modify the gradient vector after the gradient is computed
from a given residual.
Since the gradient vector is in the model space, 
any modification of the gradient vector imposes
some constraint in the model space.
If we know some characteristics of the solution 
which can be expressed in terms of weighting in the solution space, 
we can use the weight to redirect the gradient vector by applying the weight to it.
Again, by keeping the forward operator unchanged, 
we don't need to recompute the residual when the weight has changed.
This algorithm can be implemented as shown in Algorithm~\ref{alg4}.

\begin{algorithm}
\caption{ CGG method with model weight guide}
\label{alg4}
\begin{algorithmic}
\STATE $ \mathbf r \Leftarrow \mathbf L \mathbf m - \mathbf d $
\WHILE {condition}
\STATE $ {\rm diag } (\mathbf W_m )\Leftarrow \rm f (\mathbf m) $
\STATE $ \mathbf \Delta \mathbf m \Leftarrow \mathbf W_m^T \mathbf L^T \mathbf r $
\STATE $ \mathbf \Delta \mathbf r \Leftarrow \mathbf L \mathbf \Delta \mathbf m $
\STATE $ (\mathbf m, \mathbf r) \Leftarrow \rm cgstep(\mathbf m, \mathbf r, \mathbf \Delta \mathbf m, \mathbf \Delta \mathbf r)  $
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Even though the model weighting has  
different meaning from from residual weighting in the inversion result, 
the analyses are similar in both cases.
As we redefined the contribution of each residual element
by weighting it with the absolute value of itself to some power,
we can do the same thing with each model element in the solution,
\begin{equation}
\label{eqn:modelweight}
diag({\mathbf W_m})_i = |m_i|^{p} ,
\end{equation}
where $p$ is a real number that depends on the problem we wish to solve.
If the operator used in the inversion is close to unitary,
the solution obtained after the first iteration already 
closely approximates the real solution.
Therefore, weighting the gradient 
with some power of the absolute value of the previous
iteration means that we down-weight the importance of small model values 
and improve the fit to the data by emphasizing model components 
that already have large values.


\subsection{CGG with residual and model weights guide}
In the previous two subsections, we examined the meaning of weighting
the residual vector and the gradient vector, respectively.
Since applying the weighting in both residual space
and model space is nothing but changing the direction of the descent 
for the solution search, 
the weighting is not limited either to residual or to model space. 
We can weight both the residual and the gradient as shown in Algorithm~\ref{alg5}.

\begin{algorithm}
\caption{CGG method with residual and model weights guide}
\label{alg5}
\begin{algorithmic}
\STATE $ \mathbf r \Leftarrow \mathbf L \mathbf m - \mathbf d $
\WHILE {condition}
\STATE $ {\rm diag } (\mathbf W_r )\Leftarrow \rm f (\mathbf r) $
\STATE $ {\rm diag } (\mathbf W_m )\Leftarrow \rm f (\mathbf m) $
\STATE $ \mathbf \Delta \mathbf m \Leftarrow \mathbf W_m^T \mathbf L^T \mathbf W_r^T \mathbf r $
\STATE $ \mathbf \Delta \mathbf r \Leftarrow \mathbf L \mathbf \Delta \mathbf m $
\STATE $ (\mathbf m, \mathbf r) \Leftarrow \rm cgstep(\mathbf m, \mathbf r, \mathbf \Delta \mathbf m, \mathbf \Delta \mathbf r)  $
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Again, Algorithm~\ref{alg5} is different from the conventional CG method (Algorithm~\ref{alg1})
only in the step of gradient computation.
Whether we modify the gradient in the residual sense 
or in the model sense, 
it changes only the gradient direction 
(i.e. the direction in which the solution is sought)
and the solution is found in the least-squares sense in that direction.
Therefore, the problem solved by the CGG method is a linear problem and 
the CGG algorithm always converges to a solution, which is different
from the LS solution that is located along the original gradient direction.
Notice that the CGG algorithm (Algorithm~\ref{alg5}) is simpler 
than the IRLS algorithm (Algorithm~\ref{alg2}),
but the CGG method gives a similar solution as the one of the IRLS method,
which are demonstrated with examples shown in the following section.

\section{Application of the CGG method in Velocity-stack inversion}
\inputdir{syn}

The CGG method described in the previous section can be used to any inversion problem 
whose required properties are the robustness to spiky noise and the parsimony of the model.
In this section, the CGG method is tested on a velocity-stack inversion
which is useful not only for velocity analysis 
but also for various data processing applications.
The conventional velocity-stack
is performed by summing or estimating semblance ~\cite{GEO34-06-08590881}
along the various hyperbolas in a CMP gather, 
resulting in a velocity-stack panel.
Ideally a hyperbola in a CMP gather should be mapped 
onto a point in a velocity-stack panel. 
Summation along a hyperbola, or hyperbolic Radon transform (HRT), 
does not give such resolution.
To obtain a velocity-stack panel with better resolution,
\cite{GEO50-12-27272741} and \cite{Hampson-22-044055}
formulated it as an inverse problem in which the velocity
domain is the unknown space.  
If we find an operator $\mathbf H$ that transforms 
a point in a model space (velocity-stack panel) $\mathbf m$ 
into a hyperbola in data space (CMP gather) $\mathbf d$, 
\begin{equation}
\mathbf d = \mathbf H \mathbf m ,
\end{equation}
and also find its adjoint operator $\mathbf H^T$,
we can pose the velocity-stack problem as an inverse problem.
The adjoint operator $\mathbf H^T$ corresponds to the velocity stacking operator 
for a given range of velocities(or slownesses), which generates a velocity-stack panel 
and can be described as
\begin{equation}
{\mathbf m}(s,\tau) = \sum_{h=h_{min}}^{h_{max}} {\mathbf d}(h,t=\sqrt{\tau^2+h^2 s^2})
\label{eqn:adjoint}
\end{equation}
where ${\mathbf d(h,t)}$ denotes the common-midpoint (CMP) gather and 
${\mathbf m}(s,\tau)$ denotes velocity-stack panel. 
The slowness-time pair $(s,\tau)$ are the coordinate axes of the velocity-stack 
and the offset-time pair $(h,t)$ are the coordinate axes of the CMP gather.
A straightforward definition for the forward operator $\mathbf H$ is the adjoint 
of the operator $\mathbf H^T$ defined by Equation~(\ref{eqn:adjoint}). 
Through the suitable definition of the inner product, 
$\mathbf H$ turns out to be simply the process of reverse NMO and stacking ~\cite[]{GEO50-12-27272741}:
\begin{equation}
{\mathbf d}(h,t) = \sum_{s=s_{min}}^{s_{max}} {\mathbf m}(s,\tau = \sqrt{t^2-h^2 s^2}).
\end{equation}
Inverse theory helps us to find an optimal velocity-stack panel which synthesizes
a given CMP gather via the operator $\mathbf H$.
The usual process is to implement the inverse as the minimization 
of a least-squares problem and calculate the solution by solving
the normal equation:
\begin{equation}
\mathbf H^T \mathbf H \mathbf m = \mathbf H^T \mathbf d.
\end{equation}
Since the number of equations and unknowns may be large, 
an iterative least-squares solver such as CG is 
usually preferred to solving the normal equation directly.

The least-squares solution has some attributes that may be undesirable. 
If the model space is overdetermined and has busty noise in data,
the least-squares solutions usually will be spread over all the possible solutions. 
Other methods may be more useful 
if we desire a parsimonious representation of the solution. 
To obtain a more robust solution,
\cite{Nichols.sep.82.1} and \cite{GEO68-01-03860399} 
used the IRLS method for $\ell^1$-norm minimization,
and \cite{GEO68-04-13101319} used 
a quasi-Newton method called limited-memory BFGS 
~\cite{Broyden-16-670,Fletcher-13-03170322,Goldfarb-24-023026,Shanno-24-06470657,Nocedal-95-03390353} 
for Huber-norm minimization.
Another possibility is the CGG method proposed in the preceding section.
In the next subsections the results of the CGG method 
for the velocity-stack inversion are 
compared with the results of conventional 
LS method and $\ell^1$-norm IRLS method.


\subsection{Examples on synthetic data}
\inputdir{syn}
To examine the performance of the proposed CGG method,
a synthetic CMP data set with various types of noise is used.
Figure~\ref{fig:syn-data} shows the synthetic data
with three types of noise 
--- Gaussian noise in the background, busty spike noises, 
and a trace with only Gaussian noises.
Figure~\ref{fig:syn-data} (right) is the same data as Figure~\ref{fig:syn-data} (left), 
but displayed in wiggle format to clearly show the busty spike noises
that were not discernible because of the clipping in the raster format display.
The relative amplitudes of three noise types are compared to the maximum amplitude of the hyperbolic data:
ten times for the busty spikes, two times for the noisy trace, 
and 0.2 times for the Gaussian noise, respectively.
%%
\plot{syn-data}{width=5.2in,height=3.5in}
{Synthetic data with various types of noise in raster format (left) and in wiggle format (right).}

\plot{syn-L2}{height=3.8in,width=5.0in}
{The remodeled synthetic data (a) from the velocity-stack (b) obtained by LS inversion 
using CG method for the noisy synthetic data (Figure~\ref{fig:syn-data}).}

Figure~\ref{fig:syn-L2} shows 
the inversion result (Figure ~\ref{fig:syn-L2}b) obtained using the conventional 
CG algorithm for LS solution and remodeled data (Figure ~\ref{fig:syn-L2}a) from it. 
In the CG method, the iteration was performed 30 times 
and the same number of iterations was also used for all the examples presented in this paper
(including the number of iterations in the inner loop in the case of IRLS CG method).
From Figure~\ref{fig:syn-L2} we can clearly see the limit of $\ell^2$-norm minimization.
In the remodeled data, the noise with Gaussian statistics was removed quite well,
but some spurious events were generated around the busty noise spikes and noisy trace.
The inversion result obtained as a velocity-stack panel 
also shows many noisy values that correspond to
the noise part of the data which was not removed completely.

Figures~\ref{fig:syn-IRLS}d through ~\ref{fig:syn-IRLS}f show the inversion results 
obtained using the IRLS algorithm with $\ell^1$-norm residual weight only, 
$\ell^1$-norm model weight only, and $\ell^1$-norm residual and model weights together, respectively.  
Figures~\ref{fig:syn-IRLS}a through ~\ref{fig:syn-IRLS}c show the remodeled data 
from the corresponding inversion results.
From the results of $\ell^1$-norm residual weight (Figures~\ref{fig:syn-IRLS}a and ~\ref{fig:syn-IRLS}d),
we can see the robustness of $\ell^1$-norm residual minimization for the busty noise
and the successful removal of the Gaussian noise, too.
From the results of $\ell^1$-norm model weight (Figures~\ref{fig:syn-IRLS}b and ~\ref{fig:syn-IRLS}e),
we can see the improvement in the parsimony of the model compared 
to the result of LS inversion (Figure~\ref{fig:syn-L2}b).
The $\ell^1$-norm model weight also seems to have some effect to reduce low amplitude noise quite well
but the result shows some limit in reducing high amplitudes noises by making some spurious event around
the busty spike noises (Figure~\ref{fig:syn-IRLS}b).
From the results with $\ell^1$-norm residual and model weights together 
(Figures~\ref{fig:syn-IRLS}c and ~\ref{fig:syn-IRLS}f),
we can see that the IRLS method can achieve both goals, the robustness to the busty noises
and the parsimony of the model representation, quite well.

Figures~\ref{fig:syn-CGG}d through ~\ref{fig:syn-CGG}f show the inversion results 
obtained using the CGG algorithm with the residual weight only,
the model weight only, and the residual and the model weights together, respectively.  
Figures~\ref{fig:syn-CGG}a through ~\ref{fig:syn-CGG}c show the remodeled data 
from the corresponding inversion results.
From the results of the residual weight (Figures~\ref{fig:syn-CGG}a and ~\ref{fig:syn-CGG}d),
we can also see the robustness of the residual weight for the busty spike noises,
and also the success in the removal of the Gaussian noise.
Here the residual weight used was the same residual weight as the one used
in the $\ell^1$-norm residual minimizing IRLS method.
Thus we can say that guiding the gradient using the $\ell^1$-norm-like residual weight 
in the CGG method seems to behave as the $\ell^1$-norm residual minimizing IRLS method does. 
From the results of model weight (Figures~\ref{fig:syn-CGG}b and ~\ref{fig:syn-CGG}e),
we can also see the improvement in the parsimony of the model estimation
compared to the result of LS inversion (Figure~\ref{fig:syn-L2}b)
and the similar behavior in reducing noises as the $\ell^1$-norm model minimizing IRLS method does.
For the model weight, I used $diag({\mathbf W_m})_i = |m_i|^{1.5}$, where the exponent 1.5 was decided empirically.
If we want the model weight as the one used in the $\ell^1$-norm model weight in the IRLS method,
the model weight $diag({\mathbf W_m})_i$ would be $|m_i|^{0.5}$, but the result of it
was not as successful as the IRLS method.
So the appropriate value for the exponent was decided to be $1.5$ 
after experiments with several exponent values from 0.5 to 3.  
From the results of the residual and model weights together 
(Figures~\ref{fig:syn-CGG}c and ~\ref{fig:syn-CGG}f),
we can see that the CGG method also achieves both goals, the robustness to the busty noises
and the parsimony of the model representation, quite well,
and the results of the CGG method are comparable to the results of the IRLS method
(Figures~\ref{fig:syn-IRLS}c and ~\ref{fig:syn-IRLS}f).

Figure~\ref{fig:syn-diff} shows the differences of the results of the IRLS method 
and of the CGG method from the original synthetic data, respectively.
We can see that both differences contain nothing but the noise part of data. 
This demonstrates that both the IRLS method and the CGG methods 
are very successful in removing various types of noises.
Therefore, we can say that the CGG inversion method 
can be used to achieve the same goal to make an inversion robust and
to produce parsimonious model estimation as the IRLS method does.
In addition, the CGG method requires less computation than the IRLS method
by solving a linear inversion problem (which requires one iteration loop)
instead of solving a nonlinear inversion problem (which requires two nested iteration loops).

\plot{syn-IRLS}{height=8.2in,width=6.2in}
{The remodeled data and the velocity-stack inversion results obtained by IRLS method 
with three different norm criteria : (a) and (d) are for the $\ell^1$-norm residual weight only,
(b) and (e) are for the $\ell^1$-norm model weight only,
and (c) and (f) are for the $\ell^1$-norm residual/model weights together.}

\plot{syn-CGG}{height=8.2in,width=6.2in}
{The remodeled data and the velocity-stack inversion results obtained by CGG method 
with three different guiding weights : (a) and (d) are for the residual weight only, 
(b) and (e) are for the model weight only,
and (c) and (f) are for the residual/model weights together.}

\plot{syn-diff}{height=3.8in,width=5.0in}
{ (a) The difference of the remodeled data obtained by IRLS method from the original synthetic data: 
the original noisy synthetic data (Figure~\protect\ref{fig:syn-data}) was subtracted 
 from the remodeled data with IRLS method (Figure~\protect\ref{fig:syn-IRLS}c),
(b) The difference of the remodeled data obtained by CGG method from the original synthetic data: 
the original noisy synthetic data (Figure~\protect\ref{fig:syn-data}) was subtracted 
 from the remodeled data with CGG method (Figure~\protect\ref{fig:syn-CGG}c).
}

\subsection{Examples on real data}
\inputdir{real}

I tested the proposed CGG method on a real data set
that contains various types of noise.  
The data set is a shot gather from a land survey. 
However the trajectories of the events in the data set
look "hyperbolic" enough to be tested with a hyperbolic inversion.

Figure~\ref{fig:wz08-L2} shows the real data set used for testing
and the results, the velocity-stack (Figure~\ref{fig:wz08-L2}c) and 
the remodeled data (Figure~\ref{fig:wz08-L2}b) from it, when we used the conventional LS inversion. 
We can see that the real data (Figure~\ref{fig:wz08-L2}a) originally contains 
various types of noise such as the strong ground roll, 
the amplitude anomalies early at near-offset and 
late at 0.8km offset, 
and the time shifts around offsets 1.6 km and 2.0 km.
The conventional LS inversion generally does a good job in removing most dominant noises
except some whose characteristics are somehow busty (i.e. the amplitude anomalies early at near-offset and 
late at 0.8km offset and the time shift around offsets 1.6 km and 2.0 km).
The resultant velocity-stack panel (Figure~\ref{fig:wz08-L2}c) was filled with various noises that
requires some more processing if we want to perform any velocity-stack oriented processing 
such as velocity picking, multiple removal, and so on.

Figure~\ref{fig:wz08-IRLS} shows the remodeled data
from the inversion results (Figure~\ref{fig:wz08-IRLS-vel}) obtained using the IRLS method 
with three different weighting combinations ($\ell^1$-norm residual only, $\ell^1$-norm model only,
and $\ell^1$-norm residual and model together).
We can see that all three remodeled results show quite successful removal of most noises similarly.
The main difference among the three inversion results is the degree of parsimony 
of the corresponding velocity-stacks as shown in Figure~\ref{fig:wz08-IRLS-vel}.
Even though the approach of the $\ell^1$-norm residual weight can reduce many noisy signals in 
the velocity-stack, the result of the $\ell^1$-norm model weight shows better parsimony of the velocity-stack.

Figure~\ref{fig:wz08-CGG} shows the remodeled data
from the inversion results (Figure~\ref{fig:wz08-CGG-vel})
obtained using the CGG algorithm with three different guiding types
(the residual weight only, the model weight only,
and the residual and the model weights together).
All three remodeled data (Figures~\ref{fig:wz08-CGG}a through ~\ref{fig:wz08-CGG}c)
show quite similar quality as the ones obtained with IRLS method (Figure~\ref{fig:wz08-IRLS}).
The differences in the parsimony of the velocity-stacks 
among the different guiding types are also clearly shown 
in the Figure~\ref{fig:wz08-CGG-vel} and they are similar to the results of IRLS method.
In the case of the guiding with the residual weight (Figure~\ref{fig:wz08-CGG}a)
I used the weight, $diag({\mathbf W_r})_i = |r_i|^{-0.75}$ to achieve 
the similar quality in the noise removal as the one of the $\ell^1$-norm residual minimizing IRLS method.
The exponent value $-0.75$ is also decided empirically after experiments with various exponents values.

\plot{wz08-L2}{height=7.0in,width=6.0in}
{The real data set (a), the remodeled data from the inversion result (b), and the LS inversion result (c).}

\plot{wz08-IRLS}{height=7.0in,width=6.0in}
{The remodeled data from the velocity-stack inversion results (Figure~\protect\ref{fig:wz08-IRLS-vel}) 
of the real data (Figure~\protect\ref{fig:wz08-L2}a) using the IRLS method with
different norm criteria: $\ell^1$-norm residual only (a), $\ell^1$-norm model only (b),
and $\ell^1$-norm residual/model together (c).}

\plot{wz08-IRLS-vel}{height=7.0in,width=6.0in}
{The velocity-stack inversion results of the real data 
(Figure~\protect\ref{fig:wz08-L2}a) using the IRLS method with
different norm criteria: $\ell^1$-norm residual only (a), $\ell^1$-norm model only (b),
and $\ell^1$-norm residual/model together (c).}

\plot{wz08-CGG}{height=7.0in,width=6.0in}
{The remodeled data from the velocity-stack inversion results (Figure~\protect\ref{fig:wz08-CGG-vel}) 
of the real data (Figure~\protect\ref{fig:wz08-L2}a) using the CGG method with
different guiding weights: the residual weight only (a), the model weight only (b),
and the residual/model weights together (c).}

\plot{wz08-CGG-vel}{height=7.0in,width=6.0in}
{The velocity-stack inversion results of the real data (Figure~\protect\ref{fig:wz08-L2}a) 
using the CGG method with
different guiding weights: the residual weight only (a), the model weight only (b),
and the residual/model weights together (c).}

In the real data example above,
the values of exponent of the weight functions in the CGG method
were decided empirically and were sometimes different from the exponents of the weights 
used in the $\ell^1$-norm IRLS method.
In the IRLS approach, the meaning of the exponent for the weight functions
can be explained either with the $\ell^p$-norm sense or
with the relative weighting for each values of the residual/model.
Even though the two explanations are closely related, 
the meaning of the exponent for the weight function in the CGG method
can be explained better with the latter since it only
changes the gradient vector and doesn't minimize $\ell^p$-norm.
So if we increase the value of exponent of the model weight function, 
the relatively high amplitude model values get more emphasis 
in fitting the model to the data.
Likewise, if we decrease the value of exponent of the residual weight function
which is a negative value, the relatively high amplitude residual values get less emphasis
in fitting the model to the data.
The optimum value of exponent or relative emphasis in the model and the residual
depends on the distribution of the values of the model/residual
and could be found empirically as performed in this paper.
The experiments performed in the paper demonstrate
that the value of exponent of weight function used for $\ell^1$-norm residual/model in the IRLS approach 
are good choices for a start, but could be increased or decreased appropriately for each case
if any further improvement is required.

\section{CONCLUSIONS}
The proposed CGG (Conjugate Guided Gradient) inversion method 
is a modified CG (Conjugate Gradient) inversion method,
which guides the gradient vector during the iteration
and allows the user to impose various constraints for residual, model, or both of them.
The guiding is implemented by weighting the residual vector 
and the gradient vector, either separately or together.
Weighting the residual vector with the residual itself 
corresponds to guiding the solution search toward the $\ell^p$-norm minimization;
weighting the gradient vector with the model itself corresponds to 
guiding the solution search toward {\it a priori} information imposed.
Testing the CGG algorithm for the velocity-stack inversion
of synthetic and real data demonstrates 
that the guiding with residual weighting gives
a robust model estimation comparable to the IRLS method
and the guiding with model weighting produces a parsimonious velocity spectrum
also comparable to the IRLS method.
So we can say that the CGG method can be use to achieve the same goals as the IRLS method does,
but with less computation by solving the linear problem instead of solving nonlinear problem
and more flexibility in choice of weighting parameters.
Therefore, the CGG method seems to be a good alternative to the
IRLS method for robust and parsimonious model estimation inversion of seismic data.

\section{ACKNOWLEDGEMENT}
This research was financially supported by Hansung University in the year of 2006.
I thank two anonymous reviewers and the associate editor for their helpful and constructive comments.

\bibliographystyle{seg}
\bibliography{paper,SEP2,SEG}

%\end{document}
