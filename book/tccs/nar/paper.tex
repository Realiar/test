\published{Geophysics, 78, no. 6, O69-O76, (2013)}

\title{Seismic data decomposition into spectral components using regularized nonstationary autoregression}

\author{Sergey Fomel}

\address{
Bureau of Economic Geology, \\
John A. and Katherine G. Jackson School of Geosciences \\
The University of Texas at Austin \\
University Station, Box X \\
Austin, TX 78713-8924 \\
sergey.fomel@beg.utexas.edu}

\footer{TCCS-6}
\lefthead{Fomel}
\righthead{Regularized nonstationary autoregression}

\maketitle

\ms{GEO-2013}

\begin{abstract}
  Seismic data can be decomposed into nonstationary spectral
  components with smoothly variable frequencies and smoothly variable
  amplitudes. To estimate local frequencies, I use a nonstationary
  version of Prony's spectral analysis method defined with the help of
  regularized nonstationary autoregression (RNAR). To estimate local
  amplitudes of different components, I fit their sum to the data
  using regularized nonstationary regression (RNR). Shaping
  regularization ensures stability of the estimation process and
  provides controls on smoothness of the estimated
  parameters. Potential applications of the proposed technique
  include noise attenuation, seismic data compression, and seismic
  data regularization.
\end{abstract}

\section{Introduction}

Decomposing data into components has an immediate application in
noise-attenuation problems in cases where signal and noise correspond
to different components. The classic Fourier transform, Radon
transform \cite[]{gardner}, wavelet transform \cite[]{mallat},
curvelet frame \cite[]{herrmann}, and seislet transform and
frame \cite[]{seislet} are some examples of possible decompositions
applicable to seismic data. A fundamental characteristic of seismic
data is non-stationarity. In 1D (time dimension), seismic data are
nonstationary because of wave-attenuation effects. In 2D and 3D
(time and space dimensions), non-stationarity is manifested by
variable slopes of seismic events.  The nonstationary character of
seismic data can be captured by EMD (empirical mode decomposition)
proposed by \cite{emd}. EMD has found a number of important
applications in seismic data
analysis \cite[]{SEG-1999-19491952,battista,bekara,han}. However, it
remains ``empirical'' because its properties are not fully
understood. \cite{daubechies} recently proposed an EMD-like
decomposition using the continuous wavelet transform and
\emph{synchrosqueezing} \cite[]{squeeze}. Synchro\-squ\-e\-ezing
improves the analysis but remains an indirect method when it comes to
extracting spectral attributes \cite[]{henry}.

In this paper, I develop an efficient decomposition algorithm, which
explicitly fits seismic data to a sum of oscillatory signals with
smoothly varying frequencies and smoothly varying amplitudes. Such a
decomposition is close in properties to the one generated by EMD but
with explicit controls on the frequencies and amplitudes of different
components and on their smoothness. Recently, \cite{hou,hou2}
developed an explicit data-adaptive decomposition based on
matching-pursuit sparse optimization, an accurate but computationally
expensive method. To implement a faster approach, I adopt regularized
nonstationary regression, or RNR \cite[]{lpf}, a general method for
fitting data to a set of basis functions with nonstationary
coefficients. RNR was previously applied to time-frequency
decomposition over a set of regularly sampled
frequencies \cite[]{liu2012}. When the input signal is fitted to
shifted versions of itself, RNR turns into regularized nonstationary
autoregression, or RNAR, and is related to adaptive
prediction-error filtering. RNAR was previously applied to data
regularization \cite[]{yang} and noise removal
\cite[]{guochang,guochang2}. In this paper, I use it for spectral analysis and
estimating different frequencies present in the data using a
nonstationary extension of Prony's method of autoregressive spectral
analysis \cite[]{marple,MSA00-00-05300530}. After the frequencies have
been identified, I use RNR to determine local, smoothly-varying
amplitudes of different components.

The paper opens with a brief review of RNR and RNAR and explains an
extension of Prony's method to the nonstationary case. Next, I use
simple synthetic and field-data examples to illustrate performance of
the proposed technique.

\section{Regularized nonstationary regression}

Regularized nonstationary regression \cite[]{lpf} is based on the
following simple model. Let $d(\mathbf{x})$ represent the data as a
function of data coordinates $\mathbf{x}$, and $b_n(\mathbf{x})$,
$n=1,2,\ldots,N$, represent a collection of basis functions. The goal
of \emph{stationary} regression is to estimate coefficients $a_n$,
$n=1,2,\ldots,N$ such that the prediction error
\begin{equation}
  \label{eq:pred}
  e(\mathbf{x}) = d(\mathbf{x}) - \sum_{n=1}^{N} a_n\,b_n(\mathbf{x})
\end{equation}
is minimized in the least-squares sense. In the case of regularized
\emph{nonstationary} regression (RNR), the coefficients become
variable, 
\begin{equation}
  \label{eq:predhat}
  \hat{e}(\mathbf{x}) = d(\mathbf{x}) - \sum_{n=1}^{N} \hat{a}_n(\mathbf{x})\,b_n(\mathbf{x})\;.
\end{equation}
The problem in this case is underdetermined but can be constrained by regularization \cite[]{engl}. I use
shaping
regularization \cite[]{shape} to implement an explicit control on the resolution and variability of regression coefficients.
Shaping regularization applied to RNR
amounts to linear inversion,
\begin{equation}
  \label{eq:rinv}
  \mathbf{a} = \mathbf{M}^{-1}\,\mathbf{c}\;,
\end{equation}
where $\mathbf{a}$ is a vector composed of $\hat{a}_n(\mathbf{x})$,
the elements of vector $\mathbf{c}$ are
\begin{equation}
  \label{eq:cj}
c_i(\mathbf{x}) = \mathbf{S}\left[b_i^{*}(\mathbf{x})\,d(\mathbf{x})\right]\;,
\end{equation}
the elements of matrix $\mathbf{M}$ are
\begin{equation}
  \label{eq:raij}
  M_{ij}(\mathbf{x}) = \lambda^2\,\delta_{ij} + 
  \mathbf{S}\left[b_i^{*}(\mathbf{x})\,b_j(\mathbf{x}) - 
    \lambda^2\,\delta_{ij}\right]\;,
\end{equation}
$\lambda$ is a scaling coefficient, and $\mathbf{S}$ represents a
shaping (typically smoothing) operator. When inversion in
equation~\ref{eq:rinv} is implemented by an iterative method, such
as conjugate gradients, strong smoothing makes
$\mathbf{M}$ close to identity and easier (taking less iterations) to
invert, whereas weaker smoothing slows down the inversion but allows for
more details in the solution. This intuitively logical behavior
distinguishes shaping regularization from alternative methods \cite[]{lpf}.

Regularized nonstationary autoregression (RNAR) corresponds to the
case of basis functions being causal translations of the input data
itself. In 1D, with $\mathbf{x}=t$, this condition implies $b_n(t) =
d(t-n\,\Delta t)$.

\section{Autoregressive spectral analysis}

Prony's method of data analysis was developed originally for
representing a noiseless signal as a sum of exponential components
\cite[]{prony}. It was extended later to noisy signals, complex
exponentials, and spectral analysis
\cite[]{pisarenko,marple,MSA00-00-05300530,beylkin}. The basic idea follows
from the fundamental property of exponential functions:
$e^{\alpha\,(t+\Delta t)} = e^{\alpha\,t}\,e^{\alpha\,\Delta t}$.  In
signal-processing terms, it implies that a time sequence
$d(t)=A\,e^{\alpha\,t}$ (with real or complex $\alpha$) is predictable
by a two-point prediction-error filter $\left(1,-e^{\alpha\,\Delta
t}\right)$, or, in the Z-transform notation,
\begin{equation}
F_0(Z) = 1-Z/Z_0\;,
\label{eq:pef}
\end{equation}
where $Z_0 = e^{-\alpha\,\Delta t}$. If the signal is composed of multiple exponentials,
\begin{equation}
  d(t) \approx \sum\limits_{n=1}^{N} A_n\,e^{\alpha_n\,t}\;,
\label{eq:exp2}
\end{equation}
they can be predicted simultaneously by using a convolution of several
two-point prediction-error filters:
\begin{eqnarray}
\nonumber
  F(Z) & = & (1-Z/Z_1)\, (1-Z/Z_2)\,\cdots\, (1-Z/Z_N) \\ 
  & = & 1 + a_1\,Z + a_2\,Z^2 + \cdots + a_n\,Z^N\;,
\label{eq:pef2}
\end{eqnarray}
where $Z_n = e^{-\alpha_n\,\Delta t}$.
This observation suggests the following three-step algorithm:
\begin{enumerate}
\item Estimate a prediction-error filter from the data by determining filter coefficients $a_1, a_2, \ldots, a_N$ from the least-squares minimization of
\begin{equation}
\label{eq:res}
e(t) =  d(t) - \sum_{n=1}^{N} a_n\,d(t-n\,\Delta t)\;.
\end{equation}
\item Writing the filter as a $Z$ polynomial (equation~\ref{eq:pef2}), find its complex roots $Z_1, Z_2, \ldots, Z_N$. The exponential factors $\alpha_1, \alpha_2, \ldots, \alpha_N$ are determined then as 
\begin{equation}
\label{eq:alpha}
\alpha_n = -\ln(Z_n)/\Delta t\;.
\end{equation}
\item Estimate amplitudes $A_1,A_2, \ldots, A_N$ of different components in equation~\ref{eq:exp2} by linear least-squares fitting.
\end{enumerate}

Prony's method can be applied in sliding windows, which was a
technique developed by Russian
geophysicists \cite[]{us,mitrofanov2011} for identifying low-frequency
seismic anomalies \cite[]{SEG-1998-1157}. I propose to extend it to 
smoothly nonstationary analysis by applying the following modifications:
\begin{enumerate}
\item Using RNAR, the filter coefficients $a_n$ become smoothly-varying functions of time $\hat{a}_n(t)$, which allows the filter to adapt to nonstationary changes in the input data. 
\item At each instance of time, roots of the corresponding $Z$ polynomial also become functions of time $\hat{Z}_n(t)$. I apply a robust, eigenvalue-based algorithm for root finding \cite[]{roots}.
The instantaneous frequency of different components $f_n(t)$ is
determined directly from the phase of different roots:
\begin{equation}
\label{eq:group}
f_n(t) = -Re\left[\arg\left(\frac{\hat{Z}_n(t)}{2\pi\,\Delta t}\right)\right]\;.
\end{equation}
\item Finally, using RNR, I estimate smoothly-varying amplitudes of different components $\hat{A}_n(t)$.
\end{enumerate}
The nonstationary decomposition model for a complex signal $d(t)$ is thus
\begin{equation}
  d(t) \approx \sum\limits_{n=1}^{N} d_n(t)\;,\quad\mbox{where}\quad d_n(t) = \hat{A}_n(t)\,e^{i\,\phi_n(t)}
\label{eq:prony}
\end{equation}
and the local phase $\phi_n(t)$ corresponds to time integration of the
instantaneous frequency determined in Step 2:
\begin{equation}
  \phi_n(t) = 2\pi\,\int\limits_{0}^{t} f_n(\tau) d\tau\;.
\label{eq:freq}
\end{equation}
For ease of analysis, real signals can be transformed to the
complex domain by using analytical traces \cite[]{GEO44-06-10411063}.

\subsection{Benchmark tests}

\inputdir{chirp}

\multiplot{3}{sig,tf,group}{width=0.8\columnwidth}{(a) Test signal from \cite{guochang2011} composed of two
  variable-frequency components. (b) Time-frequency decomposition. (c)
Instantaneous frequencies estimated by RNAR.}
\multiplot{2}{sign1,sign2}{width=0.8\columnwidth}{Decomposition of the signal from
  Figure~\ref{fig:sig} into two spectral components using RNR.}

Figure~\ref{fig:sig,tf,group}a shows a benchmark signal from \cite{guochang2011},
which consists of two nonstationary components with smoothly varying
(par\-a\-bolic) frequencies. The corresponding time-freq\-u\-ency analysis
over a range of regularly sampled frequencies is shown in
Figure~\ref{fig:sig,tf,group}b. Two instantaneous frequencies were extracted at
Step 2 of the algorithm from a time-varying, three-point prediction-error
filter (Figure~\ref{fig:sig,tf,group}c). They correspond
precisely to the two components present in the synthetic
signal. Finally, Step 3 separates these components
(Figure~\ref{fig:sign1,sign2}.)

\inputdir{mirko}        

\multiplot{3}{msig,mtf,mgroup}{width=0.8\columnwidth}{(a) Test signal from \cite{henry} composed of several
  variable-frequency components. (b) Time-frequency decomposition. (c)
Instantaneous frequencies estimated by RNAR.}
\plot{narall}{width=0.8\columnwidth}{Decomposition of the signal from
  Figure~\ref{fig:msig} into spectral components using RNR. The components are sorted in the order of decreasing frequency.}
\plot{emd}{width=0.8\columnwidth}{Decomposition of the signal from
  Figure~\ref{fig:msig} into intrinsic mode functions using
  EMD. Compare with Figure~\ref{fig:narall}.}

The next benchmark example is taken from
\cite{henry}. Figure~\ref{fig:msig} shows the input signal, which is
composed of several components with variable frequencies. The
components can be identified in the time-frequency decomposition
analysis (Figure~\ref{fig:mtf}) generated with the method of
\cite{guochang2011}. More directly, they are extracted using RNAR (the
method of this paper), with the output shown in
Figure~\ref{fig:mgroup}. Fitting a sum of different components to the
data by RNR, we can estimate their respective amplitudes. The output
is shown in Figure~\ref{fig:narall}. For comparison, I show the output
of EMD (empirical mode decomposition) in Figure~\ref{fig:emd}. For
robustness, I used EEMD (ensemble EMD) suggested by
\cite{eemd}. Although EEMD succeeds in separating the signal into
components with variable frequencies, the individual components are
not as meaningful as those identified by RNAR.

\inputdir{hou}
\multiplot{3}{hsig,htf,hgroup}{width=0.8\columnwidth}{(a) Test signal from \cite{hou2} composed of several
  variable-frequency and variable-amplitude components. (b) Time-frequency decomposition. (c)
Instantaneous frequencies estimated by RNAR.}
\plot{sigs}{width=0.75\columnwidth}{Signal components making the signal in
  Figure~\ref{fig:hsig}.}
\plot{nar}{width=0.75\columnwidth}{Decomposition of the signal from
  Figure~\ref{fig:hsig} into spectral components using RNR.}
\plot{hfit}{width=0.75\columnwidth}{Fitting the signal from
  Figure~\ref{fig:hsig}  with the sum of three components shown in Figure~\ref{fig:nar}.}

The third benchmark test is taken from \cite{hou2}. The signal in this
case (Figure~\ref{fig:hsig}) consists of three components with
variable frequencies and amplitudes (Figure~\ref{fig:sigs}). RNAR
correctly identifies the three components (Figure~\ref{fig:hgroup}) that
are also visible on the time-frequency plot
(Figure~\ref{fig:htf}). Next, RNR extracts the three components
(Figure~\ref{fig:nar}) by fitting their sum to the data
(Figure~\ref{fig:hfit}) and adjusting the amplitudes. The sparse
inversion algorithm proposed by \cite{hou2} achieves a more accurate
result in this case but at a much higher computational cost.

\subsection{Discussion}

The cost of the proposed decomposition is
$O\left(N\,N_t\,N_{iter}\right)$, where $N$ is the number of
components, $N_t$ is the number of time samples, and $N_{iter}$ is the
number of conjugate-gradient iterations for
shaping regularization (typically between 10 and 100). This is
significantly faster than the $O\left(N_t^2\,N_{iter}\right)$ cost of
time-frequency decomposition for a regularly sampled range of
frequencies.

Although the examples of this paper use only 1D analysis, the
proposed technique is also directly applicable to analyzing variable
slopes of 2D and 3D seismic events, where the analysis applies to
different frequency slices in the $f$-$x$ domain
\cite[]{SEG-1984-S10.1,TLE18-01-00550058,SEG-2000-19691972,guochang,guochang2}.

\section{Examples}

To illustrate performance of the proposed approach in field-data
applications, I first use a simple 1D example: a single seismic
trace from a marine survey. Figure~\ref{fig:cerr} shows the input
trace and the output of RNAR, with a five-point adaptive
prediction-error filter. The four variable instantaneous frequencies
extracted from the roots of the filter are shown in
Figure~\ref{fig:tgroup}. They correspond to four different spectral
components extracted from the data in Step 3 (Figure~\ref{fig:csign}.)
Surprisingly, only four components with smoothly varying frequencies
and amplitudes are sufficient to describe a
significant portion of the signal, including the
effect of attenuating frequencies at later times
(Figure~\ref{fig:cfit}.)

\inputdir{trace}

\plot{cerr}{width=0.75\columnwidth}{Seismic trace and residual
  after adaptive prediction-error filtering with RNAR.}

\plot{tgroup}{width=0.75\columnwidth}{Instantaneous frequencies of four components extracted
  from seismic trace in Figure~\ref{fig:cerr} using RNAR.}

\plot{csign}{width=0.75\columnwidth}{Four nonstationary spectral components
  corresponding to frequencies in Figure~\ref{fig:tgroup}.}

\plot{cfit}{width=0.75\columnwidth}{Fitting input seismic trace with sum of
  four spectral components shown in Figure~\ref{fig:csign}.}

The second example is a 2D section from a land seismic survey
(Figure~\ref{fig:vdata}a), analyzed previously by \cite{shape}
and \cite{liu2012}. I choose a three-point prediction-error filter to
highlight the two most significant data components. The fitting
error is shown in Figure~\ref{fig:vdif} and contains mostly random
noise.  The two estimated spectral components are shown in
Figure~\ref{fig:vsign}, with the corresponding instantaneous frequencies
$f_n)t)$ shown in Figure~\ref{fig:vgroup}. The corresponding
amplitudes $|\hat{A}_n(t)|$ are shown in
Figure~\ref{fig:vcwht}. Comparing frequency and amplitude attributes
from different components, a low-frequency anomaly (a zone of
attenuated high frequencies) in the top-left part of the section
becomes apparent. This anomaly might indicate presence of
gas \cite[]{TLE22-02-01200127}.

\inputdir{vecta}

\plot{vdata}{width=0.7\columnwidth}{(a) 2D seismic data section. (b) Result of fitting data with two components shown in Figure~\ref{fig:vsign}.}

\plot{vdif}{width=0.7\columnwidth}{Residual error after 
fitting seismic data from Figure~\ref{fig:vdata} with two components shown in Figure~\ref{fig:vsign}.}

\plot{vsign}{width=0.7\columnwidth}{Two nonstationary spectral components: high-frequency (Component 1) and
  low-frequency (Component 2) estimated from the data shown in
  Figure~\ref{fig:vdata}a.}

\plot{vgroup}{width=0.7\columnwidth}{Instantaneous frequencies of
  high-frequency and low-frequency components from 
  decomposition shown in Figure~\ref{fig:vsign}.}

\plot{vcwht}{width=0.7\columnwidth}{Amplitudes of
  high-frequency and low-frequency components from 
  decomposition shown in Figure~\ref{fig:vsign}.
 The apparent attenuation of high frequencies in the top left
  part of the section may indicate presence of gas.}

\section{Conclusions}

I have presented a constructive approach to decomposing seismic data
into spectral components with smoothly variable frequencies and
smoothly variable amplitudes. The output of the proposed algorithm is
close to that of empirical model decomposition (EMD) and related
techniques, such as the synchrosqueezing transform (SST), but with a
more explicit control on parameters and more direct access to
instantaneous-frequency and amplitude attributes. The main tool for
the task is regularized nonstationary regression (RNR), which is
applied twice: first to estimate local frequencies by autoregression
(RNAR) and then to estimate local amplitudes. Although all examples
shown in this paper use only 1D analysis, the proposed technique is
also applicable to analyzing 2D or 3D variable-slope seismic events in
the $f$-$x$ domain. Potential applications may include noise
attenuation, data compression, and data regularization.

\section{Acknowledgments}

I would like to thank Sergey Gritsenko for teaching me Prony's method
more than 20 years ago. I am also grateful to Yihua Cai, Yangkang
Chen, Henry Herrera, Guochang Liu, Yang Liu, Karl Schleicher, Mirko
van der Baan, and Lexing Ying for inspiring discussions.

%\onecolumn

\bibliographystyle{seg}
\bibliography{SEG,nar}
