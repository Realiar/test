%\documentclass[paper]{geophysics}
%\documentclass[manuscript,revised]{geophysics}

%\begin{document}
\published{Geophysical Prospecting, 58, 415-427, (2010)}
\title{Nonlinear structure-enhancing filtering using plane-wave prediction}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\ms{GP-2009-0736-Final}

\address{
\footnotemark[1] College of Geo-Exploration Science and Technology,\\
Jilin University \\
No.6 Xi minzhu street, \\
Changchun, China, 130026 \\
\footnotemark[2] Bureau of Economic Geology,\\
John A. and Katherine G. Jackson School of Geosciences \\
The University of Texas at Austin \\
University Station, Box X \\
Austin, TX, USA, 78713-8924 \\
\footnotemark[3] State Key Laboratory of Petroleum Resource and Prospecting,\\
China University of Petroleum-Beijing \\
18 Fuxue road, Changping, \\
Beijing, China, 102249 }

\author{Yang Liu\footnotemark[1]\footnotemark[2], Sergey Fomel\footnotemark[2], 
        Guochang Liu\footnotemark[3]}

\footer{GP-2009-0736}
\lefthead{Liu etc.}
\righthead{Structurally nonlinear filtering}
\maketitle

\begin{abstract}

Attenuation of random noise and enhancement of structural continuity
can significantly improve the quality of seismic interpretation. We
present a new technique, which aims at reducing random noise while
protecting structural information. The technique is based on combining
structure prediction with either similarity-mean filtering or
lower-upper-middle (LUM) filtering. We use structure prediction to
form a structural prediction of seismic traces from neighboring
traces. We apply a nonlinear similarity-mean filter or an LUM filter
to select best samples from different predictions. In comparison with
other common filters, such as mean or median, the additional
parameters of the nonlinear filters allow us to better control the
balance between eliminating random noise and protecting structural
information. Numerical tests using synthetic and field data show the
effectiveness of the proposed structure-enhancing filters.

\end{abstract}

\section{Introduction}

Extracting structural information is the most important goal of
seismic interpretation. A number of approaches
have been proposed to preserve and enhance structural
information. \cite{Hoeber06} applied nonlinear filters, such as
median, trimmed mean, and adaptive Gaussian, over planar surfaces
parallel to the structural dip. \cite{Fehmers03} and \cite{Hale09}
applied structure-oriented filtering based on anisotropic
diffusion. \cite{Fomel06b} suggested the method of plane-wave
construction. \cite{AlBinHassan06} developed 3-D edge-preserving
smoothing (EPS) operators to reduce noise without blurring sharp
discontinuities. \cite{Traonmilin08} used a mix of
finite-impulse-response (FIR) and infinite-impulse-response (IIR)
filters, followed by \emph{f}-\emph{x} filtering, to perform
structure-preserving seismic processing. \cite{Whitcombe08} introduced
a frequency-dependent, structurally conformable filter. Random noise
attenuation and structure protection are always ambivalent problems in
seismic data processing. The main challenge in designing effective
structure-enhancing filters is controlling the balance between noise
attenuation and signal preservation.

Mean filter (stacking) plays an important role in improving
signal-to-noise ratio in seismic data processing
\cite[]{Yilmaz01}. Conventional stacking is a simple and effective
method of denoising, but it is optimal only when noise has a Gaussian
distribution \cite[]{Mayne62}. Therefore, alternative stacking methods
were proposed. \cite{Robinson70} proposed a
signal-to-noise-ratio-based weighted stack to further minimize
noise. \cite{Tyapkin05} developed optimum-weighted stacking. To
eliminate artifacts in angle-domain common-image gathers,
\cite{Tang07} presented a selective-stacking approach, which applied
local smoothing of envelope function to achieve the weighting
function. \cite{Rashed08} proposed ``smart stacking,'' which is based
on optimizing seismic amplitudes of the stacked signal by excluding
harmful samples from the stack and applying more weight to the center
of the samples population. \cite{Liu09b} introduced time-dependent
smooth weights to design a similarity-mean stack, which is a
nonstationary, nonlinear operator.

Lower-upper-middle (LUM) filters \cite[]{Hardie93} are a class of
rank-order-based filters that generalize the concept of the median
filter. They combine the characteristics of two subclasses, LUM
smoothers and LUM sharpeners, and find a variety of signal and
image-processing applications. A special LUM smoother, probably still
the most widely used, is a simple median of the points within a
window. The median filter is a well-known method that can effectively
suppress spike-like noise in nonstationary signal processing. Median
filters find important applications in seismic data
analysis. \cite{Mi00} incorporated noise reduction
using median filter into standard Kirchhoff time
migration. \cite{Zhang03} used a hyperbolic median filter to suppress
multiples. \cite{Liu06,Liu09a} implemented random-noise attenuation
using 2-D multistage median filter and nonstationary time-varying
median filter. However, in many cases the median filter introduces too
much smoothing. LUM sharpeners have excellent detail preserving
characteristics because of their ability to pass small details with
minimal distortion. For a given window size and shape, the parameters
of LUM filter can be adjusted independently to yield a wide range of
characteristics.

In this paper, we combine structure prediction \cite[]{Fomel08a} with
either nonlinear similarity-mean filtering or lower-upper-middle (LUM)
filtering to define a structure-enhancing filter. Structure prediction
generates structure-conforming predictions of seismic events from
neighboring traces, whereas a similarity-mean filtering or an LUM
filtering reduces several predictions to optimal output values. We
test the proposed method on synthetic and field data examples to
demonstrate its ability to enhance structural details of seismic
images.

\section{Theory}

\subsection{Structure prediction}

The method of plane-wave destruction \cite[]{Claerbout92} uses a local
plane-wave model for characterizing the structure of seismic data. It
finds numerous applications in seismic imaging and data processing
\cite[]{Fomel02}. Letting a seismic section, $\mathbf{s}$, be a
collection of traces, $\mathbf{s} = \left[\mathbf{s}_1 \;
\mathbf{s}_2 \; \ldots \; \mathbf{s}_N\right]^T$, the plane-wave destruction 
operation can be defined in a linear operator notation as
\begin{equation}
  \label{eq:pwd}
  \mathbf{d} = \mathbf{D(\sigma)\,s}\;,
\end{equation}

where $\mathbf{d}$ is the destruction residual and $\mathbf{D}$ is the
nonstationary plane-wave destruction operator defined as follows
\begin{equation}
  \label{eq:d}
  \left[\begin{array}{c}
     \mathbf{d}_1 \\
     \mathbf{d}_2 \\
     \mathbf{d}_3 \\
        \cdots \\
     \mathbf{d}_N  \\
 \end{array}\right] =
  \left[\begin{array}{ccccc}
      \mathbf{I} & 0 & 0 & \cdots & 0 \\
      - \mathbf{P}_{1,2}(\sigma_1) & \mathbf{I} & 0 & \cdots & 0 \\
      0 & - \mathbf{P}_{2,3}(\sigma_2) & \mathbf{I} & \cdots & 0 \\
      \cdots & \cdots & \cdots & \cdots & \cdots \\
      0 & 0 & \cdots & - \mathbf{P}_{N-1,N}(\sigma_{N-1}) & \mathbf{I} \\
    \end{array}\right]
  \left[\begin{array}{c}
     \mathbf{s}_1 \\
     \mathbf{s}_2 \\
     \mathbf{s}_3 \\
        \cdots \\
     \mathbf{s}_N  \\
 \end{array}\right]\;,
\end{equation}
where $\mathbf{I}$ stands for the identity operator, $\sigma_i$ is
local dip pattern, and $\mathbf{P}_{i,j}(\sigma_i)$ is an operator for
prediction of trace $j$ from trace $i$ according to the dip pattern
$\sigma_i$. A trace is predicted by shifting it according to the local
seismic event slopes. Minimizing the destruction residual $\mathbf{d}$
provides a method of estimating the local slopes $\sigma$
\cite[]{Fomel02}.

The least-squares minimization of $\mathbf{d}$ is achieved by using 
iterative conjugate-gradient (CG) method and smooth
regularization. Local dip at a fault position cannot be accurately
estimated, its value will depend on the initial slope estimate and
regularization.

Prediction of a trace from a distant neighbor can be
accomplished by simple recursion, i.e., predicting trace $k$ from
trace $1$ is simply
\begin{equation}
  \label{eq:pred}
  \mathbf{P}_{1,k} = \mathbf{P}_{k-1,k}\,
  \cdots\,\mathbf{P}_{2,3}\,\mathbf{P}_{1,2}\;.
\end{equation}

\cite{Fomel08a} applied plane-wave prediction to predictive painting 
of seismic images. In this paper, we use a similar construction to
recursively predict a trace from its neighbors.

An example is shown in Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}a.
The input data is borrowed from \cite{Claerbout08a}: a synthetic
seismic image containing dipping beds, an uncomformity, and a
fault. Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}b shows the same
image with Gaussian noise
added. Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}c shows local
slopes measured from the noisy image by plane-wave destruction. The
estimated slope field correctly depicts the constant slope in the top
part of the image and the sinusoidal variation of slopes in the
bottom. In the next step, we predict every trace from its neighbor
traces according to the local slope, as described by
\cite{Fomel08a}. We chose a total of 14 prediction steps (7 from the
left and 7 from the right), which, with the addition of the original
section, generated a data volume
(Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}d). The prediction
axis corresponds to index $k$ in equation~\ref{eq:pred}. The volume
is flat along the prediction direction, which confirms the ability of
plane-wave destruction to follow the local
structure. However, it still contains some discontinuous information
because of the faults. In the next step, we apply nonlinear
structure-enhancing filtering to process the data along the prediction
direction.

\inputdir{spray}
  \multiplot{4}{sigmoid1,gnoise1,ndip1,cube}{width=0.47\textwidth}{Noise-free
               synthetic image (a), noisy image (b), local slopes
               estimated from
               Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}b (c), and
               predictive data volume, where every trace is
               supplemented with predictions from
               its neighbors (d).}

  \subsection{Choices of nonlinear filters}

We have tried two different choices to select best samples from
prediction data volume: Gaussian similarity-mean filter and
lower-upper-middle (LUM) filter.

    \subsubsection{Gaussian similarity-mean filter}

The similarity-mean filter is a nonlinear filter that uses local
correlation coefficients as desired weight coefficients
\cite[]{Liu09b}. It is described in Appendix A. We chose 
the shaping operator with smoothing radius of $10$ samples in time to
calculate local similarity coefficients between the predicted data at
each prediction step and original data (reference
data). Figure~\ref{fig:weight01,weightcube1,pwdatacube,wdatacube}a
displays similarity-weight coefficients from local correlation. The
elements with the shortest prediction distance get largest weights
because they provide the most accurate prediction and therefore are
most similar to the original image. We used zero-value boundary
conditions for the prediction, so the predicted amplitudes from the
left most and the right most sides are zero. This results in the
similarity coefficients on the corners of the weight cube to be
zero. In the weighted mean filter, large weight coefficients get
selected when the similarity is strong between processed data and
reference data. We introduce additionally Gaussian weights to localize
the smoothing characteristics of the filter
\begin{equation}
  \label{eq:ismf}
    w_i = e^{-h_i^2/{h_r^2}}\;,
\end{equation}
where $h_i$ is the distance to trace $i$ and $h_r$ is the reference
parameter that controls the shape of the weight function. This
construction is analogous to bilateral or non-local filtering
\cite[]{Tomasi98,Gilboa08}. Figure~\ref{fig:weight01,weightcube1,pwdatacube,wdatacube}b
shows the product of Gaussian weights and similarity weights. The
prediction data volumes only with similarity weights applied and with
Gaussian similarity weights applied are shown in
Figure~\ref{fig:weight01,weightcube1,pwdatacube,wdatacube}c and
\ref{fig:weight01,weightcube1,pwdatacube,wdatacube}d respectively. 
After applying Gaussian and similarity weights, we stack the data in
Figure~\ref{fig:weight01,weightcube1,pwdatacube,wdatacube}d along the
prediction direction. The result is shown in
Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}b.

      \multiplot{4}{weight01,weightcube1,pwdatacube,wdatacube}{width=0.47\textwidth}{Similarity
                    weights (a), product of Gaussian
                    weights and similarity weights (b), the data
                    only with similarity weights applied (c), and the
                    data with Gaussian similarity weights applied
                    (d).}

For comparison, we used the standard mean filter to process the
prediction data volume (Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}d)
along the prediction direction. The result is shown in
Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}a and corresponds, in
this case, to simple box smoothing along the local image
structure. The standard mean filter simply stacks all information
along the prediction direction. It enhances structural
continuity but smears information across the fault.

For further discussion, we show the difference between the noisy image
(Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}b) and
structure-enhancing results with the standard mean filter and the
Gaussian similarity-mean filter
(Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}a and
\ref{fig:mean1,gsimilarstack1,mf,median1}b). We kept the same scale of
magnitude and plotting clips as that of the input image. From
Figure~\ref{fig:mdif1,gdif1,mdif,ldif1}a and
\ref{fig:mdif1,gdif1,mdif,ldif1}b, the coherent events are
well protected by the two methods because structure prediction can
exactly predict coherent information. However fault information is
destroyed by the mean filter
(Figure~\ref{fig:mdif1,gdif1,mdif,ldif1}a), while the similarity-mean
filter provides a result where fault information is protected well,
whereas random noise is attenuated
(Figure~\ref{fig:mdif1,gdif1,mdif,ldif1}b).

   \multiplot{4}{mean1,gsimilarstack1,mf,median1}{width=0.47\textwidth}{Structure-enhancing
                 results using different methods. Standard mean
                 filtering (a), similarity-mean filtering (b),
                 standard median filtering with filter-window length
                 $15$ (c), and lower-upper-middle (LUM)
                 filtering with parameters $k=l=7$ (d).}

   \subsubsection{Lower-upper-middle filter}

The lower-upper-middle (LUM) filter is a nonlinear
filter that is simple to define and yet effective for noise
attenuation in non-stationary signal processing \cite[]{Hardie93}.  It
has two parameters, one for smoothing and the other for sharpening. A
general class of LUM filters includes LUM smoothers and LUM sharpeners
as special cases (Appendix B).

By manipulating the parameters $k$ (for smoothing) and $l$ (for
sharpening), the lower-upper-middle (LUM) filter takes
on a variety of characteristics. We found that $k=l=(N-1)/2$ works
well for our synthetic and field data examples. In the synthetic
example, special smoothing and sharpening parameters, $k=l=7$, were
chosen to balance the ability between noise attenuation and fault
protection. After applying the LUM filter on the synthetic noisy
image, we obtain the image shown in
Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}d. Comparing with
Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}b, the LUM filter
displays the similar-quality result as Gaussian similarity-mean
filter. However, the LUM filter is somewhat easier to control than the
similarity-mean filter.

The standard median filter with filter-window length $15$ is compared
to the lower-upper-middle (LUM) filter. After applying
the median filter on the prediction direction of
Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}d, the result is shown in
Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}c. When comparing with
the mean filter (Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}a),
the median filter has a better fault-protection ability but weaker
noise-attenuation result. However, it still makes edges of some faults
ambiguous. The LUM filter uses smoothing and sharpening parameters to
limit the smoothing characteristics of the standard median
filter. Therefore, it strikes a reasonable balance between structure
enhancement and fault
protection. Figure~\ref{fig:mdif1,gdif1,mdif,ldif1}c and
\ref{fig:mdif1,gdif1,mdif,ldif1}d show the difference between the
noisy image (Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}b) and
structure-enhancing results with the standard median filter
(Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}c) and the LUM filter
(Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}d). While coherent
events can be preserved by either of the two filters, the median
filter has a better result of fault protection than the mean filter
(Figure~\ref{fig:mdif1,gdif1,mdif,ldif1}c) and the LUM filter can
further reduce the fault damage of the standard median filter
(Figure~\ref{fig:mdif1,gdif1,mdif,ldif1}d).

   \multiplot{4}{mdif1,gdif1,mdif,ldif1}{width=0.47\textwidth}{Difference
                 between Figure~\ref{fig:sigmoid1,gnoise1,ndip1,cube}b
                 and structure-enhancing results
                 (Figure~\ref{fig:mean1,gsimilarstack1,mf,median1}). Standard
                 mean filtering (a), similarity-mean filtering (b),
                 standard median filtering (c), and
                 lower-upper-middle (LUM) filtering
                 (d).}

The key steps of our method are illustrated schematically in Figure~\ref{fig:strat}.
\inputdir{XFig}
   \plot{strat}{width=0.9\textwidth}{Schematic illustration of the
         proposed workflow.}

 \section{Field Data Examples}

For the first field-data test, we use a time-migrated seismic 
image from a historic Gulf of Mexico dataset
\cite[]{Claerbout08a}. The input is shown in 
Figure~\ref{fig:bei,rdip}a. After estimating the field of local slopes
(Figure~\ref{fig:bei,rdip}b), we apply structure prediction to
calculate the structural data volume
(Figure~\ref{fig:rcube,rweight}a), which effectively flattens the
data. As in the synthetic example, the length of the prediction axis
is 15. Next, we apply two different filtering approaches. The first
choice is the similarity-mean filter. We display the product of
Gaussian weights and similarity weights in
Figure~\ref{fig:rcube,rweight}b. After applying the similarity-mean
filter, we can get a structure-enhanced image
(Figure~\ref{fig:rgsimilarstack,rmedian}a): the fault structures are
protected well while noise is removed. Our second choice is an
lower-upper-middle (LUM) filter with parameters $k=l=7$. The enhanced
result is shown in Figure~\ref{fig:rgsimilarstack,rmedian}b. Similarly
to the similarity-mean filter, the LUM filter highlights locally
continuous reflectors while preserving the geometry of
faults. Figure~\ref{fig:rgdif,rldif} shows the difference sections, in
which the structure-enhancing results using two filtering methods
(Figure~\ref{fig:rgsimilarstack,rmedian}) are subtracted from the
original data (Figure~\ref{fig:bei,rdip}a). After processing, the
coherent events are preserved by both methods. The LUM filter also
removes some short events with crossing dips.

\inputdir{bei}
  \multiplot{2}{bei,rdip}{width=0.75\textwidth}{2-D field data (a) and
                local slopes (b).}

  \multiplot{2}{rcube,rweight}{width=0.75\textwidth}{Predictive data
                volume (a) and product of Gaussian weights and
                similarity weights (b).}

 \multiplot{2}{rgsimilarstack,rmedian}{width=0.75\textwidth}{Structure-enhancing
                results for image from Figure~\ref{fig:bei,rdip}a
                using similarity-mean filtering (a) and
                lower-upper-middle (LUM) filtering
                (b).}
              
  \multiplot{2}{rgdif,rldif}{width=0.75\textwidth}{Difference between
                Figure~\ref{fig:bei,rdip}a and structure-enhancing
                results
                (Figure~\ref{fig:rgsimilarstack,rmedian}). Similarity-mean
                filtering (a) and lower-upper-middle (LUM) filtering (b).}

An extension of the method to 3-D is straightforward and follows
the 3-D predictive painting construction of \cite{Fomel08a}. We use a
3-D field data volume from the Gulf of Mexico to further test our
methods (Figure~\ref{fig:sgr-ori}). The dataset includes faults having
an upside-down cone shape and some amount of random noise. Although
variations in amplitude along reflectors can be stratigraphically
significant, our goal is to enhance structurally continuous events.
We chose a total of 24 prediction traces (2 prediction-step distances
around the reference trace), which, with the addition of the original
trace, generated a 4-D data volume. The corresponding inline and
crossline dips were measured automatically from the image using
plane-wave destruction (Figure~\ref{fig:sgr-dip1} and
\ref{fig:sgr-dip2}). Dip sections display different tendencies in
inline and crossline directions. Figure~\ref{fig:sgr-gsstack} and
\ref{fig:sgr-lum} show results of nonlinear
structure-enhancing filtering. When processing along the prediction
direction, both of our nonlinear filters work well
for noise reduction and fault
protection. Figure~\ref{fig:sgr-gdif} and \ref{fig:sgr-ldif} show
the difference sections between the original data
(Figure~\ref{fig:sgr-ori}) and the structure-enhancing results using
two filtering methods (Figure~\ref{fig:sgr-gsstack} and
\ref{fig:sgr-lum}). Similar to the results in the synthetic and 2-D
field examples, both methods can protect useful information well.

  \inputdir{hongliu}
 
    \plot{sgr-ori}{width=0.75\textwidth}{3-D image from Gulf of
                   Mexico.}

    \plot{sgr-dip1}{width=0.75\textwidth}{Local inline dip for the 3-D
                   image from Figure~\ref{fig:sgr-ori}.}

    \plot{sgr-dip2}{width=0.75\textwidth}{Local crossline dip for the
                    3-D image from Figure~\ref{fig:sgr-ori}.}

    \plot{sgr-gsstack}{width=0.75\textwidth}{Structure-enhancing
                       result using similarity-mean filtering. Compare
                       with Figure~\ref{fig:sgr-ori}.}

    \plot{sgr-lum}{width=0.75\textwidth}{Structure-enhancing result
                  using lower-upper-middle (LUM)
                  filtering. Compare with Figure~\ref{fig:sgr-ori}.}

    \plot{sgr-gdif}{width=0.75\textwidth}{Difference between
                   Figure~\ref{fig:sgr-ori} and
                   Figure~\ref{fig:sgr-gsstack}.}

    \plot{sgr-ldif}{width=0.75\textwidth}{Difference between
                   Figure~\ref{fig:sgr-ori} and
                   Figure~\ref{fig:sgr-lum}.}

 \section{Conclusion}

We have introduced a new technique that combines structure prediction
and signal-enhancing filters: similarity-mean filter and
lower-upper-middle (LUM) filter, for attenuating random noise while
enhancing structural information. The technique is generic. One can
replace the proposed filters with other prediction and
signal-enhancing filter methods. Experiments with synthetic and field
data show that nonlinear structure-enhancing filters can both
eliminate random noise and preserve fault information in seismic
images. Our method makes an implicit assumption that data is made of
long, coherent events and random noise. It may need a further
improvement to deal with areas where either crossing dips or no
coherent events are present.

\section{Acknowledgments}

We thank Hongliu Zeng for useful discussions. We thank assistant
editor Tijmen Jan Moser, associate editor Peter Hanssen, and two
anonymous reviewers for their constructive comments and
suggestions. We thank Texaco for providing the 3-D Gulf of Mexico
dataset, and BGP Americas for partial financial support of this
work. Publication is authorized by the Director, Bureau of Economic
Geology, The University of Texas at Austin.

\appendix
\section{Appendix A: Similarity-mean filter}

\cite{Fomel07a} defined local similarity as follows. The global 
correlation coefficient between two different signals $a(t)$ and
$b(t)$ is the functional
\begin{equation}
  \label{eq:sweight1}
     \gamma = \frac {\langle a(t),b(t)\rangle}{\sqrt{\langle a(t),a(t)\rangle \langle b(t),b(t)\rangle}}\;,
\end{equation} 
where $\langle x(t),y(t)\rangle$ denotes the dot product between two signals
\begin{equation}
  \label{eq:sweight2}
     \langle x(t),y(t)\rangle = \int x(t)y(t)dt\;.
\end{equation} 

In a linear algebra notation, the squared correlation coefficient $\gamma$
from equation~\ref{eq:sweight1} can be represented as a product of two
least-squares inverses
\begin{equation}
  \label{eq:sweight3}
     \gamma^2 = \gamma_1 \gamma_2\;,
\end{equation} 
\begin{equation}
  \label{eq:sweight4}
     \gamma_1 = (\mathbf{a}^T \mathbf{a})^{-1}(\mathbf{a}^T \mathbf{b})\;,
\end{equation} 
\begin{equation}
  \label{eq:sweight5}
     \gamma_2 = (\mathbf{b}^T \mathbf{b})^{-1}(\mathbf{b}^T \mathbf{a})\;,
\end{equation} 
where $\mathbf{a}$ is a vector notation for $a(t)$, $\mathbf{b}$ is a
vector notation for $b(t)$, and $\mathbf{x}^T \mathbf{y}$ denotes the
dot product operation defined in equation~\ref{eq:sweight2}. Let
$\mathbf{A}$ be a diagonal operator composed of the elements of
$\mathbf{a}$ and $\mathbf{B}$ be a diagonal operator composed of the
elements of $\mathbf{b}$. Localizing equations~\ref{eq:sweight4}
and~\ref{eq:sweight5} amounts to adding regularization to
inversion. Scalars $\gamma_1$ and $\gamma_2$ turn into vectors
$\mathbf{c}_1$ and $\mathbf{c}_2$ defined, using shaping
regularization \cite[]{Fomel07b}
\begin{equation}
  \label{eq:sweight6}
     \mathbf{c}_1 = [\lambda^2 \mathbf{I} + \mathbf{S}(\mathbf{A}^T \mathbf{A} - \lambda^2 \mathbf{I})]^{-1}\mathbf{S}\mathbf{A}^T\mathbf{b}\;,
\end{equation} 
\begin{equation}
  \label{eq:sweight7}
     \mathbf{c}_2 = [\lambda^2 \mathbf{I} + \mathbf{S}(\mathbf{B}^T \mathbf{B} - \lambda^2 \mathbf{I})]^{-1}\mathbf{S}\mathbf{B}^T\mathbf{a}\;,
\end{equation} 
where $\lambda$ scaling controls the relative scaling of operators
$\mathbf{A}$ and $\mathbf{B}$. Finally, the componentwise product of
vectors $\mathbf{c}_1$ and $\mathbf{c}_2$ defines the local similarity
measure.

For using time-dependent smooth weights in the stacking process, the
local similarity amplitude can be chosen as a weight for stacking
seismic data. We thus stack only those parts of the predicted data whose
similarity to the reference one is comparatively large \cite[]{Liu09b}.

\appendix
\section{Appendex B: Lower-upper-middle filter}

In this appendix, we review lower-upper-middle (LUM)
filters introduced by \cite{Hardie93}. Consider a window function
containing a set of $N$ samples centered about the sample $x^{\star}$. We
assume $N$ to be odd. This set of observations will be denoted by
$\{x_1,x_2,\cdots,x_N\}$. The rank-ordered set can be written as
\begin{equation}
   x_{(1)} \le x_{(2)} \le \cdots \le x_{(N)}\,.
  \label{eq:lum1}
\end{equation}

The estimate of the center sample will be denoted $y^{\star}$.

\begin{description} 
      \item[Lower-upper-middle smoother] \ \\
Lower-upper-middle (LUM) smoother is equivalent to 
center-weighted medians \cite[]{Justusson81}. The output of the LUM
smoother with parameter $k$ is given by
\begin{equation}
    y^{\star} = med \{x_{(k)},x^{\star},x_{(N-k+1)}\}\,,
  \label{eq:lumsm} 
\end{equation}
where $1 \le k \le (N+1)/2$.

Thus, the output of the lower-upper-middle (LUM)
smoother is $x_{(k)}$ if $x^{\star} < x_{(k)}$. If $x^{\star} > x_{(N-k+1)}$, then
the output of the LUM smoother is $x_{(N-k+1)}$. Otherwise the output
of the LUM smoother is simply $x^{\star}$.

     \item[Lower-upper-middle sharpener] \ \\
We can define a value centered between the lower- and upper-order
statistics, $x_{(l)}$ and $x_{(N-l+1)}$. This midpoint or average,
denoted $t_l$, is given by
\begin{equation}
   t_l = (x_{(l)}+x_{(N-l+1)})/2\,.
  \label{eq:lumsh1}
\end{equation}

Then, the output of the lower-upper-middle (LUM)
sharpener with parameter $l$ is given by 

\begin{equation}
   y^{\star} = \left\{ \begin{array}{ll} x_{(l)}, &
 \textrm{if $x_{(l)}< x^{\star} \le t_l$}\\ x_{(N-l+1)}, & \textrm{if
 $t_{l}< x^{\star} < x_{(N-l+1)}$}\\ x^{\star} & \textrm{otherwise}
\end{array} \right..
 \label{eq:lumsh2} 
\end{equation}

Thus, if $x_{(l)}<x^{\star}<x_{(N-l+1)}$, then $x^{\star}$ is shifted outward to
$x_{(l)}$ or $x_{(N-l+1)}$ according to which is closest to
$x^{\star}$. Otherwise the sample $x^{\star}$ is unmodified. By changing the
parameter $l$, various levels of sharpening can be achieved.  In the
case where $l=(N+1)/2$, no sharpening occurs and the
lower-upper-middle (LUM) sharpener is simply an
identity filter. In the case where $l=1$, a maximum amount of
sharpening is achieved since $x^{\star}$ is being shifted to one of the
extreme-order statistics $x_{(1)}$ or $x_{(N)}$.

     \item[Lower-upper-middle filter] \ \\
To obtain an enhancing filter that is robust and can reject outliers,
the philosophies of the lower-upper-middle (LUM)
smoother and lower-upper-middle (LUM) sharpener must be
combined. This leads us to the general
lower-upper-middle (LUM) filter. A direct definition is
as follows: 
\begin{equation} 
  y^{\star} = \left\{
\begin{array}{ll} x_{(k)}, & \textrm{if $ x^{\star} < x_{(k)}$}\\ x_{(l)}, &
\textrm{if $ x_{(l)} < x^{\star} < t_l$}\\ x_{(N-l+1)}, & \textrm{if $ t_l <
x^{\star} < x_{(N-l+1)}$}\\ x_{(N-k+1)}, & \textrm{if $ x_{(N-l+1)} <
x^{\star}$}\\ x^{\star}, & \textrm{otherwise}
\end{array} \right..
  \label{eq:lum5} 
\end{equation}
where $t_l$ is the midpoint between $x_{(l)}$ and $x_{(N-l+1)}$
defined in equation
\ref{eq:lumsh1}, and  $1 \le k \le l \le (N+1)/2$.
\end{description}

\bibliographystyle{seg}
\bibliography{SEG,paper}

%\end{document}

