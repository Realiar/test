\published{Geophysics, 72 , no. 2, R29-R36, (2007)}
\title{Shaping regularization in geophysical estimation problems}

%\ms{GEO-2006-0102.R1}

\lefthead{Fomel}
\righthead{Shaping regularization}

\author{Sergey Fomel}

\address{Bureau of Economic Geology, \\
John A. and Katherine G. Jackson School of Geosciences \\
The University of Texas at Austin \\
University Station, Box X \\
Austin, TX 78713-8972}

\maketitle

\begin{abstract}
  Regularization is a required component of geophysical estimation
  problems that operate with insufficient data. The goal of
  regularization is to impose additional constraints on the estimated
  model. I introduce shaping regularization, a general method for
  imposing constraints by explicit mapping of the estimated model to
  the space of admissible models. Shaping regularization is integrated
  in a conjugate-gradient algorithm for iterative least-squares
  estimation. It provides the advantage of a better control on the
  estimated model in comparison with traditional regularization
  methods and, in some cases, leads to a faster iterative
  convergence. Simple data interpolation and seismic velocity
  estimation examples illustrate the concept\footnote{Presented at the SEG Annual Meeting in 2005}.
\end{abstract}

\section{Introduction}
A great number of geophysical estimation problems are mathematically
ill-posed because they operate with insufficient data
\cite[]{jackson}. Regularization is a technique for making the
estimation problems well-posed by adding indirect constraints on the
estimated model \cite[]{engl,zhdanov}. Developed originally by
\cite{tikhonov} and others, the method of regularization has become an
indispensable part of the inverse problem theory and has found many
applications in geophysical problems: traveltime tomography
\cite[]{SEG-1999-12951298,clapp}, migration velocity analysis
\cite[]{SEG-1998-1218,SEG-2003-06660669}, high-resolution Radon
transform \cite[]{GEO68-01-03860399}, spectral decomposition
\cite[]{port}, etc.

While the main goal of inversion is to fit the observed data,
Tikhonov's regularization adds another goal of fitting the estimated
model to a priorly assumed behavior. The contradiction between the two
goals often leads to a slow convergence of iterative estimation
algorithms \cite[]{harlan}. The speed can be improved considerably by an
appropriate model reparameterization or preconditioning
%\cite[]{GEO59-05-08180829,
\cite[]{GEO68-02-05770588}. However, the difficult situation
of trying to satisfy two contradictory goals simultaneously 
leads sometimes to an undesirable behavior of the solution at the early
iterations of an iterative optimization scheme.

In this paper, I introduce \emph{shaping regularization}, a new
general method of imposing regularization constraints. A shaping
operator provides an explicit mapping of the model to the space of
acceptable models. The operator is embedded in an iterative
optimization scheme (the conjugate-gradient algorithm) and allows for
a better control on the estimation result. Shaping into the space of
smooth functions can be accomplished with efficient lowpass
filtering. Depending on the desirable result, it is also possible to
shape the model into a piecewise-smooth function, a function following
geological structure, or a function representable in a predefined
basis.
%Some general rules for designing and combining
%shaping operators can be developed following an analogy with triangle
%smoothing filters in signal processing. 
I illustrate the shaping concept with simple numerical experiments
of data interpolation and seismic velocity estimation.

\section{Review of Tikhonov's regularization}
If the data are represented by vector $\mathbf{d}$, model parameters by
vector $\mathbf{m}$, and their functional relationship is defined by the
forward modeling operator $\mathbf{L}$, the least-squares optimization
approach amounts to minimizing the least-squares norm of the residual
difference $\mathbf{L\,m - d}$. In Tikhonov's regularization approach, one
additionally attempts to minimize the norm of $\mathbf{D\,m}$, where $\mathbf{D}$
is the \emph{regularization} operator. Thus, we are looking for the model
$\mathbf{m}$ that minimizes the least-squares norm of the compound vector
$\left[\begin{array}{cc} \mathbf{L m - d} & \epsilon \mathbf{D m}
  \end{array}\right]^T$,
where $\epsilon$ is a scalar scaling parameter. The formal solution has the
well-known form 
\begin{equation}
  \widehat{\mathbf{m}} = 
  \left(\mathbf{L}^T\,\mathbf{L} +
    \epsilon^2\,\mathbf{D}^T\,\mathbf{D}\right)^{-1}\,\mathbf{L}^T\,\mathbf{d}\;,
  \label{eqn:minv1}  
\end{equation}
where $\widehat{\mathbf{m}}$ denotes the least-squares estimate of $\mathbf{m}$,
and $\mathbf{L}^T$ denotes the adjoint operator.  One can carry out the
optimization iteratively with the help of the conjugate-gradient method
\cite[]{hestenes} or its analogs. Iterative methods have computational
advantages in large-scale problems when forward and adjoint operators are
represented by sparse matrices and can be computed efficiently
\cite[]{saad,vorst}.

In an alternative approach, one obtains the regularized estimate by
minimizing the least-squares norm of the compound vector
$\left[\begin{array}{cc} \mathbf{p} & \mathbf{r} \end{array}\right]^T$
under the constraint
\begin{equation}
  \epsilon \mathbf{r = d - L m = d - L P p}\;.
\label{eqn:r}
\end{equation}
Here $\mathbf{P}$ is the \emph{model reparameterization} operator that
translates vector $\mathbf{p}$ into the model vector $\mathbf{m}$,
$\mathbf{r}$ is the scaled residual vector, and $\epsilon$ has the
same meaning as before. The formal solution of the preconditioned
problem is given by
\begin{equation}
  \widehat{\mathbf{m}} = 
  \mathbf{P}\,\widehat{\mathbf{p}} =
  \mathbf{P}\,\mathbf{P}^T\,\mathbf{L}^T\,\left(
    \mathbf{L}\,\mathbf{P}\,\mathbf{P}^T\,\mathbf{L}^T +
    \epsilon^2\,\mathbf{I}\right)^{-1}\, \mathbf{d}\;,
  \label{eqn:dinv1}  
\end{equation}
where $\mathbf{I}$ is the identity operator in the data space.
Estimate~\ref{eqn:dinv1} is mathematically equivalent to
estimate~\ref{eqn:minv1} if 
$\mathbf{D}^T\,\mathbf{D}$ is invertible
and
\begin{equation}
  \label{eq:equiv}
 \left(\mathbf{D}^T\,\mathbf{D}\right)^{-1} = 
 \mathbf{P}\,\mathbf{P}^T  = \mathbf{C}\;.
\end{equation}
Statistical theory of least-squares estimation connects $\mathbf{C}$
with the model covariance operator \cite[]{tarantola}. In a more
general case of reparameterization, the size of $\mathbf{p}$ may be
different from the size of $\mathbf{m}$, and $\mathbf{C}$ may not have
the full rank. In iterative methods, the preconditioned formulation
often leads to faster convergence. \cite{GEO68-02-05770588} suggest
constructing preconditioning operators in multi-dimensional problems
by recursive helical filtering.

\inputdir{smoo}

\section{Smoothing by regularization}
Let us consider an application of Tikhonov's regularization to one of the simplest
possible estimation problems: smoothing. The task of smoothing is to find a
model $\mathbf{m}$ that fits the observed data $\mathbf{d}$ but is in a
certain sense smoother. In this case, the forward operator $\mathbf{L}$ is
simply the identity operator, and the formal solutions~\ref{eqn:minv1} and~\ref{eqn:dinv1} take the form
\begin{equation}
  \widehat{\mathbf{m}} = 
  \left(\mathbf{I} +
    \epsilon^2\,\mathbf{D}^T\,\mathbf{D}\right)^{-1}\,\mathbf{d} =
  \mathbf{C}\,\left(\mathbf{C} +
    \epsilon^2\,\mathbf{I}\right)^{-1}\,\mathbf{d}\;.
  \label{eqn:msmoo}  
\end{equation}
Smoothness is controlled by the choice of the regularization
operator~$\mathbf{D}$ and the scaling parameter~$\epsilon$.

Figure~\ref{fig:exp} shows the impulse response of the regularized
smoothing operator in the 1-D case when $\mathbf{D}$ is % chosen to be
the first difference operator. The impulse response has exponentially
decaying tails. Repeated application of smoothing in this case is
equivalent to applying an implicit Euler finite-difference scheme to
the solution of the diffusion equation
\begin{equation}
  {\frac{\partial \mathbf{m}}{\partial t}} = 
  -\mathbf{D}^T\,\mathbf{D}\,\mathbf{m}
  \label{eq:diff}
\end{equation}
The impulse response converges to a Gaussian bell-shape curve in the physical
domain, while its spectrum converges to a Gaussian in the frequency domain.

\plot{exp}{width=\columnwidth}{Left: impulse response of regularized
  smoothing. Repeated smoothing converges to a Gaussian bell shape. Right:
  frequency spectrum of regularized smoothing. The spectrum also converges to
  a Gaussian.}

As far as the smoothing problem is concerned, there are better ways to
smooth signals than applying
equation~\ref{eqn:msmoo}. One example is triangle
smoothing \cite[]{Claerbout.blackwell.92}. To define triangle
smoothing of one-dimensional signals, start with box smoothing, which,
in the $Z$-transform notation, is a convolution with the filter
\begin{equation}
  \label{eq:zbox}
  B_k(Z) = \frac{1}{k}\,\left(1 + Z + Z^2 + \cdots +  Z^k\right) = 
  \frac{1}{k}\,\frac{1-Z^{k+1}}{1-Z}\;,
\end{equation}
where $k$ is the filter length. Form a triangle smoother by
correlation of two boxes
\begin{equation}
  \label{eq:ztri}
  T_k(Z) = B_k(1/Z)\,B_k(Z) 
\end{equation}
Triangle smoothing is more
efficient than regularized smoothing, because it requires twice less
floating point multiplications. It also provides smoother results
while having a compactly supported impulse response
(Figure~\ref{fig:tri}). Repeated application of triangle smoothing
also makes the impulse response converge to a Gaussian shape but at a
significantly faster rate.
%\cite{Claerbout.blackwell.92} writes:
%\begin{quote}
%  I habitually smoothed with damped exponentials, but I switched to triangles
%  after I encountered several examples where the exponential tails decreased
%  too slowly.
%\end{quote}
One can also implement smoothing by Gaussian filtering in the frequency domain
or by applying other types of bandpass filters. 

\plot{tri}{width=\columnwidth}{Left: impulse response of triangle
  smoothing. Repeated smoothing converges to a Gaussian bell shape. Right:
  frequency spectrum of triangle smoothing. Convergence to
  a Gaussian is faster than in the case of regularized smoothing. Compare to
  Figure \ref{fig:exp}.}

%\multiplot{2}{exp2,tri2}{width=0.45\columnwidth,height=0.25\columnwidth}{Impulse
%responses of smoothing operators, a: regularized smoothing. b:
%triangle smoothing. Repeated smoothing converges to a Gaussian bell
%shape faster in the case of triangle smoothing.}

%Anisotropic diffusion
%\cite[]{weickert} generates special smoothing for piece-wise-smooth functions
%and for functions having elongated features.

\section{Shaping regularization in theory}
The idea of shaping regularization starts with recognizing smoothing
as a fundamental operation. In a more general sense, smoothing implies
mapping of the input model to the space of admissible functions. I
call the mapping operator \emph{shaping}. Shaping operators do not
necessarily smooth the input but they translate it into an acceptable
model.

%How does the shaping concept connect with the regularization theory? 
Taking equation~\ref{eqn:msmoo} and using it as the
definition of the regularization operator $\mathbf{D}$, we can write
\begin{equation}
  \label{eq:r2s}
  \mathbf{S} = \left(\mathbf{I} +
    \epsilon^2\,\mathbf{D}^T\,\mathbf{D}\right)^{-1}  
\end{equation}
or
\begin{equation}
  \label{eq:r2s2}
  \epsilon^2\,\mathbf{D}^T\,\mathbf{D} = \mathbf{S}^{-1} - \mathbf{I}\;.
\end{equation}
Substituting equation~\ref{eq:r2s2}
into~\ref{eqn:minv1} yields a formal solution of the
estimation problem regularized by shaping:
\begin{equation}
  \widehat{\mathbf{m}} = 
  \left(\mathbf{L}^T\,\mathbf{L} + \mathbf{S}^{-1} -
    \mathbf{I}\right)^{-1}\,\mathbf{L}^T\,\mathbf{d}
  = \left[\mathbf{I} + 
    \mathbf{S}\,\left(\mathbf{L}^T\,\mathbf{L} - \mathbf{I}\right)\right]^{-1}\,
  \mathbf{S}\,\mathbf{L}^T\,\mathbf{d}\;.
  \label{eqn:shape}  
\end{equation}

%\begin{comment}
The meaning of equation~\ref{eqn:shape} is easy to
interpret in some special cases:
\begin{itemize}
\item If $\mathbf{S = I}$ (no shaping applied), we obtain the solution of
  unregularized problem.
\item If $\mathbf{L}^T\,\mathbf{L} = \mathbf{I}$ ($\mathbf{L}$ is a unitary operator),
  the solution is simply $\mathbf{S}\,\mathbf{L}^T\,\mathbf{d}$ and does not require
  any inversion.
\item If $\mathbf{S} = \lambda\,\mathbf{I}$ (shaping by scaling), the
  solution approaches $\lambda\,\mathbf{L}^T\,\mathbf{d}$ as $\lambda$ goes to
  zero.
\end{itemize}
%\end{comment}

The operator $\mathbf{L}$ may have physical units that require
scaling. Introducing scaling of $\mathbf{L}$ by $1/\lambda$ in
equation~\ref{eqn:shape}, we can rewrite it as
\begin{equation}
  \widehat{\mathbf{m}} = 
 \left[\lambda^2\,\mathbf{I} + 
    \mathbf{S}\,\left(\mathbf{L}^T\,\mathbf{L} - \lambda^2\,\mathbf{I}\right)
  \right]^{-1}\,
  \mathbf{S}\,\mathbf{L}^T\,\mathbf{d}\;.
  \label{eqn:shape2}  
\end{equation}
The $\lambda$ scaling in equation~\ref{eqn:shape2}
controls the relative scaling of the forward operator $\mathbf{L}$ but
not the shape of the estimated model, which is controlled by the
shaping operator $\mathbf{S}$.

Iterative inversion with the conjugate-gradient algorithm requires
symmetric positive definite operators \cite[]{hestenes}. The inverse
operator in equation~\ref{eqn:shape2} can be symmetrized when the
shaping operator is symmetric and representable in the form $\mathbf S
= \mathbf{H H}^T$ with a square and invertible $\mathbf{H}$. The
symmetric form of equation~\ref{eqn:shape2} is
\begin{equation}
  \widehat{\mathbf{m}} = 
  \mathbf{H}\,\left[\lambda^2\,\mathbf{I} + 
    \mathbf{H}^T\,\left(\mathbf{L}^T\,\mathbf{L} - 
      \lambda^2\,\mathbf{I}\right)\,
    \mathbf{H}\right]^{-1}\,
  \mathbf{H}^T\,\mathbf{L}^T\,\,\mathbf{d}\;.
  \label{eqn:sym}  
\end{equation}
%where a scaling factor~$\lambda$ was added to account for possible
%physical units in the $\mathbf{L}$ operator.  
When the inverted matrix
is positive definite, equation~\ref{eqn:sym} is suitable for an
iterative inversion with the conjugate-gradient algorithm. Appendix A
contains a complete algorithm description.

%\begin{comment}
\section{From triangle smoothing to triangle shaping}
The idea of triangle smoothing can be generalized to produce different shaping
operators for different applications. Let us assume that the estimated model
is organized in a sequence of records, as follows: $\mathbf{m} = \left[\begin{array}{cccc}
    \mathbf{m}_1 &
    \mathbf{m}_2 &
    \vdots &
    \mathbf{m}_n
  \end{array}\right]^T$.
Depending on the application, the records can be samples, traces, shot
profiles, etc. Let us further assume that, for each pair of neighboring
records, we can design a prediction operator $\mathbf{Z}_{k \rightarrow k+1}$,
which predicts record $k+1$ from record $k$. A global prediction operator is
then
\begin{equation}
  \label{eq:z}
  \mathbf{Z} = \left[\begin{array}{cccccc}
      0 & 0 & 0 & \cdots & 0 & 0 \\
      \mathbf{Z}_{1 \rightarrow 2} & 0 & 0 & \cdots & 0 & 0 \\
      0 & \mathbf{Z}_{2 \rightarrow 3} & 0 & \cdots & 0 & 0 \\
      0 & 0 & \mathbf{Z}_{3 \rightarrow 4} & \cdots & 0 & 0 \\
      \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\
      0 & 0 & 0 & \cdots & \mathbf{Z}_{n-1 \rightarrow n} & 0 \\
    \end{array}\right]\;.
\end{equation}
The operator $\mathbf{Z}$ effectively shifts each record to the next one. When
local prediction is done with identity operators, this operation is completely
analogous to the $Z$ operator used in the theory of digital signal processing. 
The $\mathbf{Z}$ operator can be squared, as follows:
\begin{equation}
  \label{eq:z2}
  \mathbf{Z}^2 = \left[\begin{array}{cccccc}
      0 & 0 & \cdots & 0 & 0 & 0 \\
      0 & 0 & \cdots & 0 & 0 & 0 \\
      \mathbf{Z}_{2 \rightarrow 3}\, \mathbf{Z}_{1 \rightarrow 2} & 0 & 
      \cdots & 0 & 0 & 0 \\
      0 & \mathbf{Z}_{3 \rightarrow 4}\, \mathbf{Z}_{2 \rightarrow 3} & 
      \cdots & 0 & 0 & 0 \\
      \cdots & \cdots & \cdots & \cdots & \cdots& \cdots \\
      0 & 0 & \cdots & 
      \mathbf{Z}_{n-1 \rightarrow n}\,\mathbf{Z}_{n-2 \rightarrow n-1} & 
      0 & 0 \\
    \end{array}\right]\;.
\end{equation}
In a shorter notation, we can denote prediction of record $j$ from record $i$
by $\mathbf{Z}_{i \rightarrow j}$ and write
\begin{equation}
  \label{eq:z21}
  \mathbf{Z}^2 = \left[\begin{array}{cccccc}
      0 & 0 & \cdots & 0 & 0 & 0 \\
      0 & 0 & \cdots & 0 & 0 & 0 \\
      \mathbf{Z}_{1 \rightarrow 3} & 0 & 
      \cdots & 0 & 0 & 0 \\
      0 & \mathbf{Z}_{2 \rightarrow 4} & 
      \cdots & 0 & 0 & 0 \\
      \cdots & \cdots & \cdots & \cdots & \cdots& \cdots \\
      0 & 0 & \cdots & \mathbf{Z}_{n-2 \rightarrow n} & 0 & 0 \\
    \end{array}\right]\;.
\end{equation}
Subsequently, the prediction operator $\mathbf{Z}$ can be taken to higher
powers. This leads immediately to an idea on how to generalize box smoothing:
predict each record from the record immediately preceding it, the record two
steps away, etc. and average all those predictions and the actual records. In
mathematical notation, a box shaper of length $k$ is then simply
\begin{equation}
  \label{eq:bk}
  \mathbf{B}_k = \frac{1}{k}\,\left(\mathbf{I} + \mathbf{Z} + 
    \mathbf{Z}^2 + \cdots +  \mathbf{Z}^k\right)\;,
\end{equation}
which is completely analogous to equation~\ref{eq:zbox}.
Implementing equation~\ref{eq:bk} directly requires many
computational operations. Noting that
\begin{equation}
  \label{eq:rec}
  \left(\mathbf{I} - \mathbf{Z}\right)\,\mathbf{B}_k =
    \frac{1}{k}\,\left(\mathbf{I} - \mathbf{Z}^{k+1}\right)\;,
\end{equation}
we can rewrite equation~\ref{eq:bk} in the compact form
\begin{equation}
  \label{eq:bcomp}
  \mathbf{B}_k = 
  \frac{1}{k}\,\left(\mathbf{I} - \mathbf{Z}\right)^{-1}\,
    \left(\mathbf{I} - \mathbf{Z}^{k+1}\right)\;,
\end{equation}
which can be implemented economically using recursive inversion of the lower
triangular operator $\mathbf{I} - \mathbf{Z}$.  Finally, combining two
generalized box smoothers creates a symmetric generalized triangle shaper
\begin{equation}
  \label{eq:tk}
  \mathbf{T}_k = \mathbf{B}_k^T\,\mathbf{B}_k\;,
\end{equation}
which is analogous to equation~\ref{eq:ztri}. A triangle shaper uses
local predictions from both the left and the right neighbors of a
record and averages them using triangle weights.

\inputdir{lomo}

\plot{tris}{width=\columnwidth}{Shaping by smoothing along local dip
  directions according to operator~$\mathbf{T}_k$ from
  equation~\ref{eq:tk}. a: an example image, b: local dip estimation,
  c: smoothing random numbers along local dips, d: impulse responses
  of oriented smoothing for nine different locations in the image
  space.}

Figure~\ref{fig:tris} illustrates generalized triangle shaping by
constructing a non-stationary smoothing operator that follows local
structural dips. Figure~\ref{fig:tris}a shows a synthetic image from
\cite{bei}. Figure~\ref{fig:tris}b is a local dip estimate obtained
with plane-wave destruction
\cite[]{GEO67-06-19461960}. Figure~\ref{fig:tris}c is the result of
applying triangle smoothing oriented along local dip directions to a
field of random numbers. Oriented smoothing generates a pattern
reflecting the structural composition of the original image. This
construction resembles the method of
\cite{EAE-1999-1009}. Figure~\ref{fig:tris}d shows the impulse
responses of oriented smoothing for several distinct locations in the
image space. As illustrated later in this paper, oriented smoothing
can be applied for generating geophysical Earth models that are
compliant with the local geological structure \cite[]{SEG-1993-0591,SEG-1993-0595,clapp}.

Appendix B describes general rules for combining elementary shaping operators.
%\end{comment}

\inputdir{int1}

\section{Examples}
Two simple examples in data regularization and seismic velocity
estimation illustrate the method of shaping.


\subsection{1-D inverse data interpolation}
I start with a simple 1-D example: a benchmark data regularization test used
previously by \cite{GEO68-02-05770588}.

\multiplot{2}{both2,data2}{width=0.45\columnwidth}{The input data (b) are irregular
  samples from a sinusoid (a).}

The input synthetic data are irregular samples from a sinusoidal
signal (Figure~\ref{fig:both2,data2}). The task of data regularization
is to reconstruct the data on a regular grid. The forward operator
$\mathbf{L}$ in this case is forward interpolation from a regular grid
using linear (two-point) interpolation.

Figure~\ref{fig:if} shows some of the first iterations and the final
results of inverse interpolation with Tikhonov's regularization
using equation~\ref{eqn:minv1} and with model preconditioning
using equation~\ref{eqn:dinv1}. The regularization operator
$\mathbf{D}$ in equation~\ref{eqn:minv1} is the first finite
difference, and the preconditioning operator $\mathbf{P}$
in~\ref{eqn:dinv1} is the inverse of $\mathbf{D}$ or causal
integration.  The preconditioned iteration converges faster but its
very first iterations produce unreasonable results. This type of
behavior can be dangerous in real large-scale problems, when only few
iterations are affordable.

\plot{if}{width=\columnwidth}{The first iterations and the final
  result of inverse interpolation with Tikhonov's regularization
  using equation~\ref{eqn:minv1} (left) and with model
  preconditioning using equation~\ref{eqn:dinv1} (right). The
  regularization operator $\mathbf{D}$ is the first finite
  difference. The preconditioning operator
  $\mathbf{P}=\mathbf{D}^{-1}$ is causal integration. The number
  of iterations is indicated in the plot labels.}

\plot{sz}{width=\columnwidth}{The first iterations and the final
  result of inverse interpolation with shaping regularization using
  equation~\ref{eqn:sym}. Left: the shaping operator $\mathbf{H}$ is
  lowpass filtering with a Gaussian smoother. Right: the shaping
  operator $\mathbf{H}$ is bandpass filtering with a shifted
  Gaussian. Shaping by bandpass filtering recovers the sinusoidal
  shape of the estimated model. The number of iterations is
  indicated in the plot labels.}

\plot{spec}{width=0.8\columnwidth}{Spectrum of the estimated model
  (solid curve) fitted to a shifted Gaussian (dashed curve). The
  Gaussian band-limited filter defines a shaping operator for
  recovering a band-limited signal.}

The left side of Figure~\ref{fig:sz} shows some of the first
iterations and the final result of inverse interpolation with shaping
regularization, where the shaping operator $\mathbf{S}$ was chosen to
be Gaussian smoothing with the impulse response width of about 10
samples. The final result is smoother, and the iteration is both
fast-converging and producing reasonable results at the very first
iterations. Thanks to the fact that the smoothing operation is
applied at each iteration, the estimated model is 
guaranteed to have the prescribed shape.

Examining the spectrum of the final result (Figure~\ref{fig:spec}),
one can immediately notice the peak at the dominant frequency of the
initial sinusoid.  Fitting a Gaussian shape to the peak defines a
data-adaptive shaping operator as a bandpass filter implemented in the
frequency domain (dashed curve in Figure~\ref{fig:spec}). Inverse
interpolation with the estimated shaping operator recovers the
original sinusoid (right side of Figure~\ref{fig:sz}). Analogous
ideas in the model preconditioning context were proposed by 
\cite{SEG-2001-19211924}.

%\plot{ss}{width=\columnwidth}{Inverse interpolation with the data-adaptive
%  shaper recovers the original sinusoid. The shown result is obtained after 20
%  iterations.}

\begin{comment}

\inputdir{rain}

\subsection{2-D data regularization}
The second benchmark was previously used in
\cite[]{Fomel.sepphd.107}. It comes from the test dataset that
contains rainfall measurements in Switzerland on the 8th of May
1986. The dataset appeared in the Spatial Interpolation Comparison
\cite[]{jgida}, which benchmarked several different spatial
interpolation methods. Figure~\ref{fig:elev} shows the data area: the
Digital Elevation Model of Switzerland and the country's
borders\footnote{I provide the elevation image only for reference. It
has not been used in the interpolation experiment.}. A total of 467
rainfall measurements were taken. A subset of randomly selected 100
measurements was used in the 1997 Spatial Interpolation Comparison in
order to compare the results with the known data.
Figure~\ref{fig:raindata} shows the spatial location of the selected
data samples.

\sideplot{elev}{width=\columnwidth}{Switzerland border overlaid on a
digital elevation map.}

\plot{raindata}{width=6.0in}{Left: data locations for all 467
  measurements. Right: data locations for selected 100 measurements.}

Rainfall level is generally a smoothly varying quantity. We cannot
expect it to be represented a priori by a simple function. Therefore,
it is reasonable to take the regularization operator $\mathbf{D}$ to be
a convolution with the Laplacian filter. The
interpolation result using the conventional regularization
scheme~\ref{eqn:minv1} is shown in
Figure~\ref{fig:lapinter}. The input irregular data were regularized
on a 376 by 253 grid, which corresponds to the digital elevation model
in Figure~\ref{fig:elev}. Similarly to what happens in the
one-dimensional synthetic examples, the solution converges steadily
but with a slow spread of information away from the known data points.
It takes about $10,000$ iterations to achieve full convergence.

\plot{lapinter}{width=\columnwidth}{Rainfall data after model-space
  regularization with 10, 100, 1000, and 10000 iterations.}

The result of shaping regularization is shown in
Figure~\ref{fig:gaussinter}. Two-dimensional triangle smoothing was
applied as a shaping operator. The method converges to the
machine-precision tolerance at about 32 iterations. Since triangle
smoothing has approximately the same cost as Laplacian filtering, the
method appears to be 300 times more efficient. Even more importantly,
the very early iterations (left plot in Figure~\ref{fig:gaussinter})
produce reasonable results. Thanks to the fact that smoothing is
applied at each iterations, we are guaranteed to end up with a smooth
model even when the iterative process stops early.
 
\plot{gaussinter}{width=\columnwidth}{Rainfall data after shaping
  regularization with 10 and 40 iterations.}

Figure~\ref{fig:stat} shows correlation plots of the observed and
interpolated data points for the 367 points that were not used in the
interpolation experiment.  If we take into account the fairly
unpredictable distribution of rainfall, the correlation is relatively
good in comparison with analogous results of other methods used in the
Spatial Interpolation Contest \cite{jgida}.

\plot{stat}{width=\columnwidth}{Correlation between observed and
  predicted rainfall data values. Left: Tikhonov's regularization
  result, right: shaping regularization result.}

\end{comment}

\inputdir{beivc}

\subsection{Velocity estimation}
The second example is an application of shaping regularization to seismic
velocity estimation. Figure~\ref{fig:bei-fmg2} shows a time-migrated image
from a historic Gulf of Mexico dataset \cite[]{bei}. The image was obtained by
velocity continuation \cite[]{GEO68-05-16621672}. The corresponding migration
velocity is shown in the right plot of Figure~\ref{fig:bei-fmg2}. Shaping
regularization was used for picking a smooth velocity profile from semblance
gathers obtained in the process of velocity continuation.

\plot{bei-fmg2}{width=\columnwidth}{Left: time-migrated image. Right: The
  corresponding migration velocity from automatic picking.}

The task of this example is to convert the time migration velocity to
the interval velocity. I use the simple approach of Dix inversion
\cite[]{GEO20-01-00680086} formulated as a regularized inverse problem
\cite[]{alejandro}. In this case, the forward operator~$\mathbf{L}$ in
equation~\ref{eqn:shape} is a weighted time
integration. There is a choice in choosing the shaping
operator~$\mathbf{H}$.

Figure~\ref{fig:bei-dix} shows the result of inversion with shaping by
triangle smoothing. While the interval velocity model yields a good
prediction of the measured velocity, it may not appear geologically
plausible because the velocity structure does not follow the structure
of seismic reflectors as seen in the migrated image.

Following the ideas of steering filters \cite[]{SEG-1998-1851,clapp}
and plane-wave construction \cite[]{pwc}, I estimate local slopes in
the migration image using the method of plane-wave destruction
\cite[]{GEO67-06-19461960} and define a triangle plane-wave shaping
operator $\mathbf{H}$ using the method of the previous
section. The result of inversion, shown in Figures~\ref{fig:bei-shp}
and~\ref{fig:bei-shpw}, makes the estimated interval velocity follow
the geological structure and thus appear more plausible for
direct interpretation. Similar results were obtained by \cite{pwc}
using model parameterization by plane-wave construction but at a
higher computational cost. In the case of shaping regularization,
about 25 efficient iterations were sufficient to converge to the
machine precision accuracy.

\plot{bei-dix}{width=\columnwidth}{Left: estimated interval velocity. Right:
  predicted migration velocity. Shaping by triangle smoothing.}

\plot{bei-shp}{width=\columnwidth}{Left: estimated interval
  velocity. Right: predicted migration velocity. Shaping by triangle
  local plane-wave smoothing creates a velocity model consistent with
  the reflector structure.}

\plot{bei-shpw}{width=\columnwidth}{Seismic image from
Figure~\ref{fig:bei-fmg2} overlaid on top of the interval velocity
model estimated with triangle plane-wave shaping regularization.}

\section{Conclusions}
Shaping regularization is a new general method for imposing
regularization constraints in estimation problems. The main idea of
shaping regularization is to recognize shaping (mapping to the space
of acceptable functions) as a fundamental operation and to incorporate
it into iterative inversion.
\begin{comment}
There is a
resemblance between this idea and the method of POCS (projection onto
convex sets) developed by \cite{pocs} and extended to geophysical
applications by \cite{menke} and \cite{GEO58-07-09410948}. Unlike
POCS, shaping regularization embeds a linear shaping operator inside
an iterative conjugate gradient algorithm, which leads to an optimally
fast iterative convergence.
\end{comment}

There is an important difference between shaping regularization and
conventional (Ti\-kho\-nov's) regularization from the user prospective. Instead
of trying to find and specify an appropriate regularization operator, the user
of the shaping regularization algorithm specifies a shaping operator, which is
often easier to design. Shaping operators can be defined following a triangle
construction from local predictions or by combining elementary shapers.

I have shown two simple illustrations of shaping applications. The
examples demonstrate a typical behavior of the method: 
enforced model compliance to the specified shape at each
iteration and, in many cases, fast iterative convergence of the
conjugate gradient iteration. The model compliance
behavior follows from the fact that shaping enters directly into the
iterative process and provides an explicit control on the shape of the
estimated model.

% My initial knowledge about regularization and
% preconditioning comes from Jon Claerbout, Bill Harlan, Dave Nichols,
% and Gennady Ryzhikov. Their insight is much appreciated.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: 

\section{Acknowledgments}
I would like to thank Pierre Hardy and Mauricio Sacchi for inspiring
discussions, TOTAL for partially supporting this research, and three
  reviewers for helpful suggestions.

\bibliographystyle{seg}
\bibliography{SEG,SEP2,shape}

\append{Conjugate-gradient algorithm}

A complete algorithm for conjugate-gradient iterative inversion with
shaping regularization is given below. The algorithm follows directly
from combining equation~\ref{eqn:sym} with the classic
conjugate-gradient algorithm of \cite{hestenes}.

\begin{algorithm}{Conjugate gradients with shaping}{\mathbf{L},\mathbf{H},\mathbf{d},\lambda,tol,N}
  \mathbf{p \= 0} \\
  \mathbf{m \= 0} \\
  \mathbf{r \= - d} \\
  \begin{FOR}{n \= 1, 2, \ldots, N} \\
    \mathbf{g}_m \= \mathbf{L}^T\,\mathbf{r} - \lambda\,\mathbf{m} \\
    \mathbf{g}_p \= \mathbf{H}^T\,\mathbf{g}_m + \lambda\,\mathbf{p}  \\
    \mathbf{g}_m \= \mathbf{H}\,\mathbf{g}_p \\
    \mathbf{g}_r \= \mathbf{L}\,\mathbf{g}_m \\
    \rho \= \mathbf{g}_p^T\,\mathbf{g}_p \\
    \begin{IF}{n = 1} 
      \beta \= 0 \\
      \rho_0 \= \rho
      \ELSE 
      \beta \= \rho/\hat{\rho} \\
      \begin{IF}{\beta < tol \text{or} \rho/\rho_0 < tol}
        \RETURN \mathbf{m}
      \end{IF} 
    \end{IF} \\
    \left[\begin{array}{l}
        \mathbf{s}_p \\
        \mathbf{s}_m \\
        \mathbf{s}_r
      \end{array}\right] \= 
    \left[\begin{array}{l}
        \mathbf{g}_p \\
        \mathbf{g}_m \\
        \mathbf{g}_r
      \end{array}\right] + \beta\,
    \left[\begin{array}{l}
        \mathbf{s}_p \\
        \mathbf{s}_m \\
        \mathbf{s}_r
      \end{array}\right] \\
    \alpha \= \rho/\left[
    \mathbf{s}_r^T\,\mathbf{s}_r + 
      \lambda\,(\mathbf{s}_p^T\,\mathbf{s}_p -    
      \mathbf{s}_m^T\,\mathbf{s}_m) \right] \\
    \left[\begin{array}{l}
        \mathbf{p} \\
        \mathbf{m} \\
        \mathbf{r}
      \end{array}\right] \= 
    \left[\begin{array}{l}
        \mathbf{p} \\
        \mathbf{m} \\
        \mathbf{r}
      \end{array}\right] - \alpha\,
    \left[\begin{array}{l}
        \mathbf{s}_p \\
        \mathbf{s}_m \\
        \mathbf{s}_r
      \end{array}\right] \\
    \hat{\rho} \= \rho
  \end{FOR} \\        
  \RETURN \mathbf{m}
\end{algorithm}
  
The iteration terminates after $N$ iterations or upon reaching convergence
to the specified tolerance $tol$. It uses auxiliary vectors $\mathbf{p}$,
$\mathbf{r}$, $\mathbf{s}_p$, $\mathbf{s}_m$, $\mathbf{s}_r$, $\mathbf{g}_p$,
$\mathbf{g}_m$, $\mathbf{g}_r$ and applies operators $\mathbf{L}$,
$\mathbf{H}$ and their adjoints only once per each iteration.

%\begin{comment}

\append{Combining shaping operators}

\inputdir{smoo}

General rules can be developed to combine two or more shaping operators for
the cases when there are several features in the model that need to be
characterized simultaneously. A general rule for combining two different
shaping operators $\mathbf{S}_1$ and $\mathbf{S}_2$ can have the form
\begin{equation}
  \label{eq:combine}
  \mathbf{S}_{12} =
  \mathbf{S}_{1} + \mathbf{S}_{2} -
  \mathbf{S}_{1}\,\mathbf{S}_{2}\;,
\end{equation}
where one adds the responses of the two shapers and then subtracts
their overlap. An example is shown in Figure~\ref{fig:test12}, where
an impulse response for oriented smoothing in two different directions
is constructing from smoothing in each of the two directions
separately.

\sideplot{test12}{width=0.3\columnwidth}{Impulse response for a
combination of two shaping operators smoothing in two different
directions.}

Combining two operators that work in orthogonal directions can be
accomplished with a simple tensor product, as follows:
\begin{equation}
  \label{eq:xy}
  \mathbf{S}_{xy} = \mathbf{S}_{x}\,\mathbf{S}_{y}\;,
\end{equation}
where $\mathbf{S}_{x}$ and $\mathbf{S}_{y}$ are shaping operators that apply
in orthogonal $x$- and $y$-directions, and $\mathbf{S}_{xy}$ is a combined
operator that works in both directions. An example is shown in
Figure~\ref{fig:plane}, where two two-dimensional shapers working in
orthogonal directions are combined to produce an impulse response of 3-D
shaping operator that applies smoothing along a three-dimensional plane.

\sideplot{plane}{width=0.3\columnwidth}{3-D impulse response for a
combination of two 2-D shaping operators smoothing in in-line and
cross-line directions.}

Constructing multidimensional recursive filters for helical
preconditioning \cite[]{GEO68-02-05770588} is significantly more
difficult. It involves helical spectral factorization, which may
create long inefficient filters \cite[]{burg}.

