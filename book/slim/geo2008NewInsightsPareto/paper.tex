\author{Gilles Hennenfent\footnotemark[1], Ewout van den
  Berg\footnotemark[2], Michael P. Friedlander\footnotemark[2], and
  Felix J. Herrmann\footnotemark[1]}

\footnotetext[1]{Seismic Laboratory for Imaging and Modeling,
  Department of Earth and Ocean Sciences, The University of British
  Columbia, 6339 Stores Road, Vancouver, V6T 1Z4, BC, Canada}

\footnotetext[2]{Scientific Computing Laboratory, Department of
  Computer Science, The University of British Columbia, 2366 Main
  Mall, Vancouver, V6K 1Z4, BC, Canada}
\published{Geophysics, 73, no. 4, A23-A26, (2008)}
\title{New insights into one-norm solvers from the Pareto curve}

\footer{Accepted 2008 March 7. Received 2008 March 3. In original form
  2007 December 18.}

\lefthead{Hennenfent et al.}
\righthead{Pareto curve and one-norm solvers}
\maketitle

\begin{abstract}
  Geophysical inverse problems typically involve a trade off between
  data misfit and some prior. Pareto curves trace the optimal trade
  off between these two competing aims. These curves are commonly used
  in problems with two-norm priors where they are plotted on a log-log
  scale and are known as L-curves. For other priors, such as the
  sparsity-promoting one norm, Pareto curves remain relatively
  unexplored. We show how these curves lead to new insights into
  one-norm regularization. First, we confirm the theoretical
  properties of smoothness and convexity of these curves from a
  stylized and a geophysical example. Second, we exploit these crucial
  properties to approximate the Pareto curve for a large-scale
  problem. Third, we show how Pareto curves provide an objective
  criterion to gauge how different one-norm solvers advance towards
  the solution.
\end{abstract}

\section{Introduction}

\mbox{}\indent Many geophysical inverse problems are ill posed
\cite[][]{parker94}---their solutions are not unique or are acutely
sensitive to changes in the data. To solve this kind of problem
stably, additional information must be introduced. This technique is
called \textit{regularization} \cite[see,
e.g.,][]{phillips62,tikhonov63}.

Specifically, when the solution of an ill-posed problem is known to be
(almost) sparse, \cite{oldenburg83} and others have observed that a
good approximation to the solution can be obtained by using one-norm
regularization to promote sparsity.  More recently, results in
information theory have breathed new life into the idea of promoting
sparsity to regularize ill-posed inverse problems. These results
establish that, under certain conditions, the sparsest solution of a
(severely) underdetermined linear system can be \textit{exactly}
recovered by seeking the minimum one-norm solution
\cite[][]{candes06robust,donoho06cs,rauhut07}.  This has led to
tremendous activity in the newly established field of
\textit{compressed sensing}. Several new one-norm solvers have
appeared in response \cite[see, e.g.,][ and references
therein]{daub04it,vandenberg07}.  In the context of geophysical
applications, it is a challenge to evaluate and compare these solvers
against more standard approaches such as iteratively reweighted
least-squares \cite[IRLS -][]{gersztenkorn86irls}, which uses a
quadratic approximation to the one-norm regularization function.

In this letter, we propose an approach to understand the behavior of
algorithms for solving one-norm regularized problems. The approach
consists of tracking on a graph the data misfit versus the one norm of
successive iterates. The \emph{Pareto curve} traces the optimal
tradeoff in the space spanned by these two axes and gives a rigorous
yardstick for measuring the quality of the solution path generated by
an algorithm. In the context of the two-norm---i.e.,
Tikhonov---regularization, the Pareto curve is often plotted on a
log-log scale and is called the L-curve \cite[][]{lawson74}. We draw
on the work of \cite{vandenberg07} who examine the theoretical
properties of the one-norm Pareto curve. Our goal is to understand the
compromises implicitly accepted when an algorithm is given a limited
number of iterations.

\section{Problem statement}
%
\mbox{}\indent Consider the following underdetermined system of linear
equations
%
\begin{equation}
  \label{eq:yeqAx}
  \vector{y} = \tensor{A}\vector{x}_0+\vector{n},
\end{equation}
%
where the $n$-vectors $\vector{y}$ and $\vector{n}$ represent
observations and additive noise, respectively. The $n$-by-$N$ matrix
$\tensor{A}$ is the modeling operator that links the model
$\vector{x}_0$ to the noise-free data given by
$\vector{y}-\vector{n}$. We assume that $N\gg n$ and that
$\vector{x}_0$ has few nonzero or significant entries. We use the
terms ``model'' and ``observations'' in a broad sense, so that many
linear geophysical problems can be cast in the form shown in
equation~\ref{eq:yeqAx}. In the case of wavefield reconstruction, for
example, $\vector{y}$ is the acquired seismic data with missing
traces, $\tensor{A}$ can be the restriction operator combined with the
curvelet synthesis operator so that $\vector{x}_0$ is the curvelet
representation of the fully-sampled wavefield
\cite[][]{herrmann07crsi,hennenfent07jitter}.

Because $\vector{x}_0$ is assumed to be (almost) sparse, one can
promote sparsity as a prior via one-norm regularization to overcome
the singular nature of $\tensor{A}$ when estimating $\vector{x}_0$
from $\vector{y}$. A common approach is to solve the convex
optimization problem
%
\begin{equation*}
  \label{eq:qp}
  \text{QP}_\lambda:
  \quad\min_{\vector{x}}\ \half\|\vector{y}-\tensor{A}\vector{x}\|_2^2
  +\lambda\|\vector{x}\|_1,
\end{equation*}
%
which is closely related to quadratic programming (QP); the positive
parameter $\lambda$ is the Lagrange multiplier, which balances the
tradeoff between the two norm of the data misfit and the one norm of
the solution.  Many algorithms are available for solving QP$_\lambda$,
including IRLS, iterative soft thresholding (IST), introduced by
\cite{daub04it}, and the IST extension to include cooling \cite[ISTc
-][]{figueiredo03}, which was tailored to geophysical applications by
\cite{herrmann07crsi}.

It is generally not clear, however, how to choose the parameter
$\lambda$ such that the solution of QP$_\lambda$ is, in some sense,
optimal. A directly related optimization problem, the basis pursuit
(BP) denoise problem \cite[][]{chen98bp}, minimizes the one norm of
the solution given a maximum misfit, and is given by
%
\begin{equation*}
  \label{eq:bpdn}
  \text{BP}_\sigma:\quad\min_{\vector{x}}\ \|\vector{x}\|_1\quad\text{s.t.}\quad\|\vector{y}-\tensor{A}\vector{x}\|_2\leq\sigma.
\end{equation*}
%
This formulation is often preferred when an estimate of the noise
level $\sigma\geq 0$ in the data is available. BP$_\sigma$ can be
solved using ISTc or the spectral projected-gradient algorithm
(SPG$\ell_1$) introduced by \cite{vandenberg07}.

For interest, a third optimization problem, connected to QP$_\lambda$
and BP$_\sigma$, minimizes the misfit given a maximum one norm of the
solution, and is given by the LASSO (LS) problem
\cite[][] {tibshirani94ls}
%
\begin{equation*}
  \label{eq:lasso}
  \text{LS}_\tau:\quad\min_{\vector{x}}\ \half\|\vector{y}-\tensor{A}\vector{x}\|_2^2\quad\text{s.t.}\quad\|\vector{x}\|_1\leq\tau.
\end{equation*}
%
Because an estimate of the one norm of the solution $\tau\geq 0$ is
typically not available for geophysical problems, this formulation is
seldom used directly. It is, however, a key internal problem used by
SPG$\ell_1$ in order to solve BP$_\sigma$.

To understand the connection between these approaches and compare
their related solvers in different scenarios, we propose to follow
\cite{daub07acc} and \cite{vandenberg07} and look at the Pareto curve.
%
\section{Pareto curve}
\inputdir{.}

\mbox{}\indent Figure \ref{fig:pcurve} gives a schematic illustration
of a Pareto curve. The curve traces the optimal tradeoff between
$\|\vector{y}-\tensor{A}\vector{x}\|_2$ and $\|\vector{x}\|_1$ for a
specific pair of $\tensor{A}$ and $\vector{y}$ in
equation~\ref{eq:yeqAx}. Point \textcircled{\scriptsize{1}} clarifies
the connection between the three parameters of QP$_\lambda$,
BP$_\sigma$, and LS$_\tau$. The coordinates of a point on the Pareto
curve are $(\tau,\sigma)$ and the slope of the tangent at this point
is $-\lambda$. The end points of the curve---points
\textcircled{\scriptsize{2}} and \textcircled{\scriptsize{3}}---are
two special cases. When $\tau = 0$, the solution of LS$_\tau$ is
$\vector{x}=0$ (point \textcircled{\scriptsize{2}}). It coincides with
the solutions of BP$_\sigma$ with $\sigma=\|\vector{y}\|_2$ and
QP$_\lambda$ with
$\lambda=\|\tensor{A}^H\vector{y}\|_\infty/\|\vector{y}\|_2$. (The
infinity norm $\|\cdot\|_\infty$ is given by
$\max\left(|\cdot|\right)$.)  When $\sigma=0$, the solution of
BP$_\sigma$ (point \textcircled{\scriptsize{3}}) coincides with the
solutions of LS$_\tau$, where $\tau$ is the one norm of the solution,
and QP$_\lambda$, where $\lambda=0^+$---i.e., $\lambda$ infinitely
close to zero from above.  These relations are formalized as follows
in \cite{vandenberg07}:
%
\begin{res} The Pareto curve i) is convex and decreasing, ii) is
  continuously differentiable, and iii) has a negative slope
  $\lambda=\|\tensor{A}^H\vector{r}\|_\infty/\|\vector{r}\|_2$ with
  the residual $\vector{r}$ given by
  $\vector{y}-\tensor{A}\vector{x}$.
\end{res}
%
For large-scale geophysical applications, it is not practical (or even
feasible) to sample the entire Pareto curve. However, its regularity,
as implied by this result, means that it is possible to obtain a good
approximation to the curve with very few interpolating points, as
illustrated later in this letter.
%
\plot{pcurve}{width=.9\columnwidth}{Schematic illustration of a Pareto
  curve. Point \textcircled{\scriptsize{1}} exposes the connection
  between the three parameters of QP$_\lambda$, BP$_\sigma$, and
  LS$_\tau$. Point \textcircled{\scriptsize{3}} corresponds to a
  solution of BP$_\sigma$ with $\sigma=0$.}
%
\section{Comparison of one-norm solvers}
%
\mbox{}\indent To illustrate the usefulness of the Pareto curve, we
compare IST, ISTc, SPG$\ell_1$, and IRLS on a noise-free problem and
compute a solution of BP$_\sigma$ for $\sigma=0$, i.e., BP$_0$. This
case is especially challenging for solvers that attack
QP$_\lambda$---e.g., IST, ISTc and IRLS---because the corresponding
solution can only be attained in the limit as $\lambda\to0$.

We construct a benchmark problem that is typically used in the
compressed sensing literature \cite[][]{donoho06stomp}. The matrix
$\tensor{A}$ is taken to have Gaussian independent and
identically-distributed entries; a sparse solution $\vector{x}_0$ is
randomly generated, and the ``observations'' $\vector{y}$ are computed
according to equation~\ref{eq:yeqAx}.

\subsection{Solution paths}
\inputdir{pareto}
%
\plot{plot}{width=.9\columnwidth}{Pareto curve and solution paths
  (large enough number of iterations) of four solvers for a BP$_0$
  problem. The symbols + represent a sampling of the Pareto curve. The
  solid (---) line, obscured by the Pareto curve, is the solution path
  of ISTc, the chain (-- $\cdot$ --) line the path of SPGL$\ell_1$,
  the dashed (-- --) line the path of IST, and the dotted ($\cdots$)
  line the path of IRLS.}
%
\mbox{}\indent Figure \ref{fig:plot} shows the solution paths of the
four solvers as they converge to the BP$_0$ solution. The starting
vector provided to each solver is the zero vector, and hence the paths
start at $(0,\|\vector{y}\|_2)$---point \textcircled{\scriptsize{2}}
in Figure \ref{fig:pcurve}. The number of iterations is large enough
for each solver to converge, and therefore the solution paths end at
$(\tau_{_{BP_0}},0)$---point \textcircled{\scriptsize{3}} in Figure
\ref{fig:pcurve}.

The two solvers SPG$\ell_1$ and ISTc approach the BP$_0$ solution from
the left and remain close to the Pareto curve. In contrast, IST and
IRLS aim at a least-squares solution before turning back towards the
BP$_0$ solution. ISTc solves QP$_\lambda$ for a decreasing sequence
$\lambda_i\to0$.  The starting vector for QP$_{\lambda_{i}}$ is the
solution of QP$_{\lambda_{i-1}}$, which is by definition on the Pareto
curve. This explains why ISTc so closely follows the curve.
SPG$\ell_1$ solves a sequence of LS$_\tau$ problems for an increasing
sequence of $\tau_i\to\tau_{_{BP_0}}$, hence the vertical segments
along the SPG$\ell_1$ solution path. IST solves QP$_{0^+}$.  Because
there is hardly any regularization, IST first works towards minimizing
the data misfit. When the data misfit is sufficiently small, the
effect of the one-norm penalization starts, yielding a change of
direction towards the BP$_0$ solution. IRLS solves a sequence of
weighted, damped, least-squares problems. Because the weights are
initialized to ones, IRLS first reaches the standard least-squares
solution. The estimates obtained from the subsequent reweightings have
a smaller one norm while maintaining the residual (close) to zero.
Eventually, IRLS gets to the BP$_0$ solution.
% 
\subsection{Practical considerations}
%
\mbox{}\indent In geophysical applications, problem sizes are large
and there is a severe computational constraint. We can use the
technique outlined above to understand the robustness of a given
solver that is limited by a maximum number of iterations or
matrix-vector products that can be performed.

Figure \ref{fig:plotLim} shows the Pareto curve and the solution paths
of the various solvers where the maximum number of iterations is
fixed. This roughly equates to using the same number of matrix-vector
products for each solver. Whereas SPG$\ell_1$ continues to provide a
fairly accurate approximation to the BP$_0$ solution, those computed
by IST, ISTc, and IRLS suffer from larger errors.  IST stops before
the effect of the one-norm regularization kicks in; hence the data
misfit at the candidate solution is small but the one norm is
completely incorrect. ISTc and IRLS accumulate small errors along
their paths because there are not enough iterations to solve each
subproblem to sufficient accuracy. Note that both solvers accumulate
errors along both axes.
%
\plot{plotLim}{width=.9\columnwidth}{Pareto curve and optimization
  paths (same, limited number of iterations) of four solvers for a
  BP$_0$ problem (see Figure \ref{fig:plot} for legend).}
%
\section{Geophysical example}
\inputdir{geop}

\mbox{}\indent As a concrete example of the use of the Pareto curve in
the geophysical context, we study the problem of wavefield
reconstruction with sparsity-promoting inversion in the curvelet
domain \cite[CRSI -][]{herrmann07crsi}. The simulated acquired data,
shown in Figure \ref{fig:data}, corresponds to a shot record with 35\%
of the traces missing. The interpolated result, shown in Figure
\ref{fig:interp}, is obtained by solving BP$_0$ using SPG$\ell_1$.
This problem has more than half a million unknowns and forty-two
thousand data points.

The points in Figure \ref{fig:res} are samples of the corresponding
Pareto curve.  The regularity of these points strongly indicates that
the underlying curve---which we know to be convex---is smooth and well
behaved, and empirically supports our earlier claim.  However problems
of practical interest are often significantly larger, and it may be
prohibitively expensive to compute a similarly fine sampling of the
curve.

Because the curve is well behaved, we can leverage its smoothness and
use a small set of samples to obtain a good interpolation. The solid
line in Figure \ref{fig:res} shows an interpolation based only on
information from the circled samples. The interpolated curve closely
matches the samples that were not included in the interpolation. The
figure also plots the iterates taken by SPG$\ell_1$ in order to obtain
the reconstruction shown in Figure \ref{fig:interp}. The plot shows
that the iterates remain to the Pareto curve and that they convergence
towards the BP$_0$ solution.

\multiplot{2}{data,interp}{width=.45\columnwidth}{CRSI on synthetic
  data. (a) Input and (b) interpolated data using CRSI with
  SPG$\ell_1$.}

\plot{res}{width=.9\columnwidth}{Pareto curve and SPG$\ell_1$ solution
  path for a CRSI problem. The symbols + represent a fine, accurate
  sampling of the Pareto curve. The solid (---) line is an
  approximation to the Pareto curve using the few, circled points, the
  chain (-- $\cdot$ --) line the solution path of SPG$\ell_1$.}

\section{Conclusions}
%
\mbox{}\indent The sheer size of seismic problems makes it a certainty
that there will be significant constraints on the amount of
computation that can be done when solving an inverse problem.  Hence
it is especially important to explore the nature of a solver's
iterations in order to make an informed decision on how to best
truncate the solution process. The Pareto curve serves as the optimal
reference, which makes an unbiased comparison between different
one-norm solvers possible.

Of course, in practice it is prohibitively expensive to compute the
entire Pareto curve exactly. We observe, however, that the Pareto
curves for many of the one-norm regularized problems are regular, as
confirmed by the theoretical Result~1. This suggests that it is
possible to approximate the Pareto curve by fitting a curve to a small
set of sample points, taking into account derivative information at
these points. As such, the insights from the Pareto curve can be
leveraged to large-scale one-norm regularized problems, as we
illustrate on a geophysical example.  This prospect is particularly
exciting given the current resurgence of this type of regularization
in many different areas of research.

\section{Acknowledgments}
%
\mbox{}\indent The authors are grateful to Sergey Fomel and Tamas
Nemeth for their valuable input, and to Eric Verschuur for the
synthetic data. The authors also thank the anonymous reviewers and
associate editor for their comments that certainly helped improve this
letter. This publication was prepared using Madagascar
(\url{rsf.sf.net}), a package for reproducible computational
experiments, SPG$\ell_1$ (\url{cs.ubc.ca/labs/scl/spgl1}), and Sparco
(\url{cs.ubc.ca/labs/scl/sparco}), a suite of linear operators and
problems for testing algorithms for sparse signal reconstruction.
This research was in part financially supported by
\href{http://www.nserc.gc.ca/index.htm}{NSERC} Discovery Grant
22R81254 of F.J.H. and by CRD Grant
\href{http://slim.eos.ubc.ca/?module=articles&func=display&catid=43&aid=100}{DNOISE}
334810-05 of F.J.H. and M.P.F., and was carried out as part of the
\href{http://slim.eos.ubc.ca/?module=articles&func=display&catid=22&aid=11}{SINBAD}
project with support, secured through
\href{http://www.oil-itf.com/}{ITF}, from the following organizations:
\href{http://www.bg-group.com/}{BG Group},
\href{http://www.bp.com/}{BP},
\href{http://www.chevron.com/}{Chevron},
\href{http://www.exxonmobil.com/}{ExxonMobil}, and
\href{http://www.shell.com/}{Shell}.

\bibliographystyle{seg}
\bibliography{bib}

% $Id: paper.tex 91 2008-03-11 03:25:18Z hegilles $

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
